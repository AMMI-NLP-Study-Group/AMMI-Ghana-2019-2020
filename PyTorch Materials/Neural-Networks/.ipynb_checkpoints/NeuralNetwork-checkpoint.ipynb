{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Why Neural Networks?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Before NN\n",
    "Unless the specific steps that computer needs to follow are known the computer cannot solve a problem\n",
    "this restricts the problem solving capability of conventional computers to problems that we already understand and know how to solve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## After NN\n",
    "-With Neural Networks computers can do things that we don't exactly know how to do.\n",
    "-Neural Networks learn by example. They cannot be programmed to perform a specific task.\n",
    "\n",
    "The term Neural Network exist since 1943 but at that time we don't have enough of data to train."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Motivation behind NN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The building block of neural network is the neuron. An artificial neuron works much the same way the biological one does\n",
    "<img src=\"assets/nn2.jpg\" width=400px>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NN are basecally computing systems inspired by the biological neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Descrption of NN\n",
    "\n",
    " A NN is constituted of Layers and each layer contains units.\n",
    " \n",
    " The First Layer is the inputs layer and the last one is the output layer.\n",
    " \n",
    " #### What is an unit?\n",
    " \n",
    " Unit, also called node or neuron is the basic building of Neural Networks. It has the following properties:\n",
    " \n",
    " * A unit is a function that takes as inputs a vectors and returns a scalar.\n",
    " \n",
    " * Each unit is characterized by its Weights $W$ and bias term $b$.\n",
    " \n",
    " * The output of a unit can be described by:\n",
    " \n",
    " \\begin{equation}\n",
    " f\\left( \\sum_{i=1}^{n}x_{i}w_{i}+b\\right),\n",
    " \\end{equation}\n",
    " \n",
    " Where $f$ corresponds to the activation function.\n",
    " \n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example\n",
    "\n",
    "An example of NN is given by the following figure:\n",
    " \n",
    " <img src=\"assets/nn1.png\" width=400px>\n",
    " \n",
    "We can notice from the figure above that the Network contains three layers denoted $L_{1}$ and $L_{2}$ and $L_{3}$, where $L_{1}$ and $L_{3}$ are respectively the inputs and  outputs Layer. $L_{2}$ is the hidden Layer.\n",
    "\n",
    "\n",
    "* For $L_{2}$, we have five neurons, the inputs of those neurons are $x_{i}, i=1,\\cdots,4$ and its activation function are given as follows:\n",
    "\n",
    "  $a^{(2)}_{1}    =f\\left(W^{(1)}_{11}x_{1}+W^{(1)}_{12}x_{2}+W^{(1)}_{13}x_{3}+W^{(1)}_{14}x_{4}+b^{(1)}_{1}\\right).$\n",
    "\n",
    "     $a^{(2)}_{2} =f\\left(W^{(1)}_{21}x_{1}+W^{(1)}_{22}x_{2}+W^{(1)}_{23}x_{3}+W^{(1)}_{24}x_{4}+b^{(1)}_{2}\\right).$\n",
    "\n",
    " $\\vdots$\n",
    "\n",
    "  $a^{(2)}_{5} =f\\left(W^{(1)}_{51}x_{1}+W^{(1)}_{52}x_{2}+W^{(1)}_{53}x_{3}+W^{(1)}_{54}x_{4}+b^{(1)}_{5}\\right).$\n",
    "  \n",
    "* For the output Layer, one has the following activation function:\n",
    "\n",
    "$a^{(3)}_{1} = f\\left(W^{(2)}_{11}a^{(2)}_{1}+ W^{(2)}_{12}a^{(2)}_{2}+\\cdots +W^{(2)}_{15}a^{(2)}_{5}+b^{(2)}_{1}\\right).$\n",
    "\n",
    "\n",
    "For a more general case, I mean a NN with many hiddens layers the general formular for the activation function is given by:\n",
    "\n",
    "$a_{i}^{(\\ell+1)}  = f\\left(\\sum_{j=1}^{{s_{\\ell}}}W_{ij}^{(\\ell)} a_{j}^{(\\ell)}+b_{i}^{(\\ell)}\\right).$\n",
    "\n",
    "\n",
    "Let us set $z^{(\\ell)}_{i}$, the total weighted sum of inputs to units $i$ of the layer $\\ell$\n",
    "such that:\n",
    "\n",
    "$a^{(\\ell)}_{i} = f\\left(z^{(\\ell)}_{i} \\right).$\n",
    "\n",
    "\n",
    "Note: $W^{(\\ell)}_{ij}$ represents the connection between the $j-th$ unit of the layer $\\ell$ and the $i-th$ unit of the layer $\\ell+1$.\n",
    "\n",
    "$b^{\\ell}_{i}$ denotes the bias term of the unit $i$ of the layer $\\ell+1$.\n",
    "\n",
    "$a^{(\\ell)}_{i}$, the activation function of unit $i$ of the layer $\\ell.$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Back Propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Error Terms \n",
    "\n",
    "In the NN, the errors terms is given as follows:\n",
    "\n",
    "\\begin{equation}\n",
    "J(W,b) = \\frac{1}{2}\\sum_{i=1}^{N}J(W,b;x^{i},y^{i})+ \\frac{\\lambda}{2}\\sum_{\\ell=1}^{n_{\\ell}}\\sum_{i}^{s_{\\ell}}\\sum_{j}^{s_{\\ell+1}} \\left(W^{(\\ell)}_{ji}\\right)^{2},\n",
    "\\end{equation}\n",
    "\n",
    "Where:\n",
    "\n",
    "$J(W,b;x,y)=\\frac{1}{N}\\sum_{i=1}^{N}||y_{i}-h_{W,b}(x^{(i)})||^{2} $\n",
    "\n",
    "and $\\left(W^{(\\ell)}_{ji}\\right)^{2}$ is the regularization term called the weight decay. It helps to decrease the magnitude of the weight function, hence to avoid Overfitting.\n",
    "\n",
    "$\\lambda $ is the weight decay parameter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Intuition behind the Backpropagation\n",
    "\n",
    "* we  have a training dataset $D=\\{x_{i},y_{i}\\}$\n",
    "\n",
    "* we perform the forward pass, that means we compute all the activation function through all the networks including the one of the output layer.\n",
    "\n",
    "* Then, we compute the errors term $\\delta_{i}^{(\\ell)}$ that measures how much each node $i$ of layer $\\ell$ was responsible for any errors in the output.\n",
    "\n",
    "\n",
    "The backpropagation is an algorithm used in order to take in account the error brought by every node during the forward pass. it helps us to penalized the weight hence to avoid Overfitting as we mentioned before."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How NN work?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perceptron (artificial neuron)\n",
    "we will have multiples inputs $ x_{1}, x_{2}, \\dots x_{n} $ and their corresponding weights $ w_{1},w_{2}, \\dots w_{n}$ and we calculate the sum of the inputs $ s = \\sum x_{i} w_{i}$ , then we pass it throught an activation function $Y$ and get the outputs\n",
    "<img src=\"assets/nn2.png\" width=400px>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two modes in perceptron that is Training Mode and Using Mode\n",
    "<img src=\"assets/nn3.png\" width=400px>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activation function\n",
    "the activation function of a node defines the output of that node given an input or set of inputs. Most popular types of Activation functions \n",
    "##### 1- Sigmoid or Logistic\n",
    "  It is a activation function of form $ f(x) = 1 / 1 + \\exp(-x)$ Its Range is between 0 and 1. It is a S — shaped curve. It is easy to understand and apply but it has major reasons which have made it fall out of popularity:\n",
    "  - Vanishing gradient problem\n",
    "  - Secondly , its output isn’t zero centered. It makes the gradient updates go too far in different directions. 0 < output < 1, and it makes optimization harder.\n",
    "  - Sigmoids saturate and kill gradients.\n",
    "  -Sigmoids have slow convergence.\n",
    "  \n",
    "##### 2- Tanh — Hyperbolic tangent\n",
    "  It’s mathamatical formula is $ f(x) = 1 - \\exp(-2x) / 1 + \\exp(-2x)$ Now it’s output is zero centered because its range in between -1 to 1 i.e -1 < output < 1 . Hence optimization is easier in this method hence in practice it is always preferred over Sigmoid function . But still it suffers from Vanishing gradient problem. \n",
    " #### 3-ReLu -Rectified linear units\n",
    "   It has become very popular in the past couple of years. It was recently proved that it had 6 times improvement in convergence from Tanh function. It’s just $ R(x) = max(0,x) i.e if x<0, R(x) = 0 and if x>0, R(x) = 0 so R(x) = x$ Hence as seeing the mathamatical form of this function we can see that it is very simple and efficinent . A lot of times in Machine learning and computer science we notice that most simple and consistent techniques and methods are only preferred and are best. Hence it avoids and rectifies vanishing gradient problem . Almost all deep learning Models use ReLu nowadays. But its limitation is that it should only be used within Hidden layers of a Neural Network Model. Hence for output layers we should use a Softmax function for a Classification problem to compute the probabilites for the classes , and for a regression problem it should simply use a linear function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multilayer perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you know our brain is made up of millions of neurons, so a NN is really just a composition of perceptrons, connected in different ways and operating on different activation functions\n",
    "<img src=\"assets/nn5.png\" width=400px>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Types of NN\n",
    "\n",
    "#### 1-  Feedforward Neural Network – Artificial Neuron:\n",
    "This neural network is one of the simplest form of ANN, where the data or the input travels in one direction. The data passes through the input nodes and exit on the output nodes. This neural network may or may not have the hidden layers. In simple words, it has a front propagated wave and no back propagation by using a classifying activation function usually.\n",
    "Below is a Single layer feed forward network. Here, the sum of the products of inputs and weights are calculated and fed to the output. The output is considered if it is above a certain value i.e threshold(usually 0) and the neuron fires with an activated output (usually 1) and if it does not fire, the deactivated value is emitted (usually -1).\n",
    "<img src=\"assets/nn6.png\" width=200px>\n",
    "Application of Feed forward neural networks are found in computer vision and speech recognition where classifying the target classes are complicated. These kind of Neural Networks are responsive to noisy data and easy to maintain.\n",
    "#### 2-  Radial basis function Neural Network:\n",
    "Radial basic functions consider the distance of a point with respect to the center. RBF functions have two layers, first where the features are combined with the Radial Basis Function in the inner layer and then the output of these features are taken into consideration while computing the same output in the next time-step which is basically a memory.\n",
    "Below is a diagram which represents the distance calculating from the center to a point in the plane similar to a radius of the circle. Here, the distance measure used in euclidean, other distance measures can also be used. The model depends on the maximum reach or the radius of the circle in classifying the points into different categories. If the point is in or around the radius, the likelihood of the new point begin classified into that class is high. There can be a transition while changing from one region to another and this can be controlled by the beta function.\n",
    "<img src=\"assets/nn7.png\" width=200px>\n",
    "This neural network has been applied in Power Restoration Systems. Power systems have increased in size and complexity. Both factors increase the risk of major power outages. After a blackout, power needs to be restored as quickly and reliably as possible. This paper how RBFnn has been implemented in this domain.\n",
    "#### 3-Kohonen Self Organizing Neural Network\n",
    "The objective of a Kohonen map is to input vectors of arbitrary dimension to discrete map comprised of neurons. The map needs to me trained to create its own organization of the training data. It comprises of either one or two dimensions. When training the map the location of the neuron remains constant but the weights differ depending on the value. This self organization process has different parts, in the first phase every neuron value is initialized with a small weight and the input vector. In the second phase, the neuron closest to the point is the ‘winning neuron’ and the neurons connected to the winning neuron will also move towards the point like in the graphic below. The distance between the point and the neurons is calculated by the euclidean distance, the neuron with the least distance wins. Through the iterations, all the points are clustered and each neuron represents each kind of cluster. This is the gist behind the organization of Kohonen Neural Network.\n",
    "<img src=\"assets/nn1.gif\" width=200px>\n",
    "<img src=\"assets/nn8.png\" width=200px>\n",
    "Kohonen Neural Network is used to recognize patterns in the data. Its application can be found in medical analysis to cluster data into different categories.\n",
    "#### 4- Recurrent Neural Network(RNN) – Long Short Term Memory:\n",
    "The Recurrent Neural Network works on the principle of saving the output of a layer and feeding this back to the input to help in predicting the outcome of the layer.\n",
    "Here, the first layer is formed similar to the feed forward neural network with the product of the sum of the weights and the features. The recurrent neural network process starts once this is computed, this means that from one time step to the next each neuron will remember some information it had in the previous time-step. This makes each neuron act like a memory cell in performing computations. In this process, we need to let the neural network to work on the front propagation and remember what information it needs for later use. Here, if the prediction is wrong we use the learning rate or error correction to make small changes so that it will gradually work towards making the right prediction during the back propagation. This is how a basic Recurrent Neural Network looks like\n",
    "<img src=\"assets/nn2.gif\" width=200px>\n",
    "The application of Recurrent Neural Networks can be found in text to speech(TTS) conversion models.\n",
    "#### 5- Convolutional Neural Network:\n",
    "Convolutional neural networks are similar to feed forward neural networks , where the neurons have learn-able weights and biases. Its application have been in signal and image processing which takes over OpenCV in field of computer vision.\n",
    "Below is a representation of a ConvNet, in this neural network, the input features are taken in batch wise like a filter. This will help the network to remember the images in parts and can compute the operations. These computations involve conversion of the image from RGB or HSI scale to Gray-scale. Once we have this, the changes in the pixel value will help detecting the edges and images can be classified into different categories.\n",
    "<img src=\"assets/nn3.gif\" width=200px>\n",
    "ConvNet are applied in techniques like signal processing and image classification techniques. Computer vision techniques are dominated by convolutional neural networks because of their accuracy in image classification.\n",
    "#### 6- Modular Neural Network:\n",
    "Modular Neural Networks have a collection of different networks working independently and contributing towards the output. Each neural network has a set of inputs which are unique compared to other networks constructing and performing sub-tasks. These networks do not interact or signal each other in accomplishing the tasks. The advantage of a modular neural network is that it breakdowns a large computational process into smaller components decreasing the complexity. This breakdown will help in decreasing the number of connections and negates the interaction of these network with each other, which in turn will increase the computation speed. However, the processing time will depend on the number of neurons and their involvement in computing the results.\n",
    "Below is a visual representation,\n",
    "<img src=\"assets/nn4.gif\" width=200px>\n",
    "Modular Neural Networks (MNNs) is a rapidly growing field in artificial Neural Networks research. This paper surveys the different motivations for creating MNNs: biological, psychological, hardware, and computational. Then, the general stages of MNN design are outlined and surveyed as well, viz., task decomposition techniques, learning schemes and multi-module decision-making strategies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training a NN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most common deep learning algorithm for supervised training of the multi-layer perceptrons is known as backpropagation. In it, after the weighted sum of inputs and passing through the activation function we propagate backwards and update the weights to reduce the error (desired output-model output)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.datasets as dsets\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import torchvision.transforms as transforms\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(Model, self).__init__()\n",
    "        self.layer1 = nn.Linear(input_dim,50)\n",
    "        self.layer2 = nn.Linear(50, 20)\n",
    "        self.layer3 = nn.Linear(20, 3)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.layer1(x))\n",
    "        x = F.relu(self.layer2(x))\n",
    "        x = F.softmax(self.layer3(x)) # To check with the loss function\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "features, labels = load_iris(return_X_y=True)\n",
    "features_train,features_test, labels_train, labels_test = train_test_split(features, labels, random_state=42, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "model = Model(features_train.shape[1])\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "epochs = 10\n",
    "\n",
    "def print_(loss):\n",
    "    print (\"The loss calculated: \", loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 1\n",
      "The loss calculated:  1.084611177444458\n",
      "Epoch # 2\n",
      "The loss calculated:  1.0589287281036377\n",
      "Epoch # 3\n",
      "The loss calculated:  1.042871117591858\n",
      "Epoch # 4\n",
      "The loss calculated:  1.01982843875885\n",
      "Epoch # 5\n",
      "The loss calculated:  0.9865216016769409\n",
      "Epoch # 6\n",
      "The loss calculated:  0.9583816528320312\n",
      "Epoch # 7\n",
      "The loss calculated:  0.9344419836997986\n",
      "Epoch # 8\n",
      "The loss calculated:  0.9024511575698853\n",
      "Epoch # 9\n",
      "The loss calculated:  0.8794012665748596\n",
      "Epoch # 10\n",
      "The loss calculated:  0.8613719940185547\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/ipykernel_launcher.py:11: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    }
   ],
   "source": [
    "# Not using dataloader\n",
    "x_train, y_train = Variable(torch.from_numpy(features_train)).float(), Variable(torch.from_numpy(labels_train)).long()\n",
    "for epoch in range(1, epochs+1):\n",
    "    print (\"Epoch #\",epoch)\n",
    "    y_pred = model(x_train)\n",
    "    loss = loss_fn(y_pred, y_train)\n",
    "    print_(loss.item())\n",
    "    \n",
    "    # Zero gradients\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward() # Gradients\n",
    "    optimizer.step() # Update\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/ipykernel_launcher.py:11: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    }
   ],
   "source": [
    "# Prediction\n",
    "x_test = Variable(torch.from_numpy(features_test)).float()\n",
    "pred = model(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.05697161, 0.29412127, 0.6489071 ],\n",
       "       [0.91191626, 0.04918151, 0.03890222],\n",
       "       [0.00211426, 0.14895074, 0.84893495],\n",
       "       [0.05718697, 0.28539914, 0.6574139 ],\n",
       "       [0.05055362, 0.28836423, 0.6610822 ],\n",
       "       [0.8904781 , 0.06150251, 0.04801933],\n",
       "       [0.17277575, 0.30878782, 0.51843643],\n",
       "       [0.01842327, 0.22602433, 0.7555524 ],\n",
       "       [0.03538903, 0.2949286 , 0.6696824 ],\n",
       "       [0.12442167, 0.31731695, 0.5582614 ],\n",
       "       [0.0239523 , 0.2356728 , 0.7403749 ],\n",
       "       [0.87590134, 0.07037628, 0.05372234],\n",
       "       [0.9233247 , 0.04335823, 0.03331704],\n",
       "       [0.8766615 , 0.06977676, 0.05356177],\n",
       "       [0.91224456, 0.04895632, 0.0387991 ],\n",
       "       [0.06004994, 0.2714223 , 0.6685277 ],\n",
       "       [0.00716032, 0.18816221, 0.8046775 ],\n",
       "       [0.11222101, 0.32513776, 0.5626412 ],\n",
       "       [0.05889581, 0.29232612, 0.648778  ],\n",
       "       [0.00784537, 0.19586195, 0.79629266],\n",
       "       [0.86162716, 0.07776302, 0.0606099 ],\n",
       "       [0.0286808 , 0.25125998, 0.7200592 ],\n",
       "       [0.87028795, 0.07244075, 0.05727138],\n",
       "       [0.00852415, 0.20163555, 0.7898403 ],\n",
       "       [0.01088909, 0.19102159, 0.79808927],\n",
       "       [0.01404195, 0.21811026, 0.7678478 ],\n",
       "       [0.00779451, 0.21146584, 0.78073967],\n",
       "       [0.00705563, 0.18368259, 0.8092618 ],\n",
       "       [0.8622539 , 0.07794169, 0.05980441],\n",
       "       [0.8564604 , 0.08090509, 0.06263453],\n",
       "       [0.91614676, 0.04696282, 0.03689051],\n",
       "       [0.9427834 , 0.03169057, 0.02552599],\n",
       "       [0.09865347, 0.29608843, 0.6052581 ],\n",
       "       [0.8798515 , 0.06731425, 0.05283425],\n",
       "       [0.88046765, 0.06737225, 0.05216017],\n",
       "       [0.01643629, 0.24491987, 0.7386438 ],\n",
       "       [0.0803118 , 0.2861273 , 0.63356084],\n",
       "       [0.9047775 , 0.05353561, 0.04168688]], dtype=float32)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = pred.detach().numpy()\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy is 0.7105263157894737\n"
     ]
    }
   ],
   "source": [
    "print (\"The accuracy is\", accuracy_score(labels_test, np.argmax(pred, axis=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/ipykernel_launcher.py:11: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking for first value\n",
    "np.argmax(model(x_test[0]).detach().numpy(), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_test[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aims/.local/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Model. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/aims/.local/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    }
   ],
   "source": [
    "torch.save(model, \"iris-pytorch.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_model = torch.load(\"iris-pytorch.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/ipykernel_launcher.py:11: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(saved_model(x_test[0]).detach().numpy(), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
