{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ipdb in /usr/local/lib/python3.6/dist-packages (0.13.3)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from ipdb) (47.3.1)\n",
      "Requirement already satisfied: ipython>=5.1.0; python_version >= \"3.4\" in /usr/local/lib/python3.6/dist-packages (from ipdb) (5.5.0)\n",
      "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.6/dist-packages (from ipython>=5.1.0; python_version >= \"3.4\"->ipdb) (1.0.18)\n",
      "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.6/dist-packages (from ipython>=5.1.0; python_version >= \"3.4\"->ipdb) (0.8.1)\n",
      "Requirement already satisfied: pygments in /usr/local/lib/python3.6/dist-packages (from ipython>=5.1.0; python_version >= \"3.4\"->ipdb) (2.1.3)\n",
      "Requirement already satisfied: pexpect; sys_platform != \"win32\" in /usr/local/lib/python3.6/dist-packages (from ipython>=5.1.0; python_version >= \"3.4\"->ipdb) (4.8.0)\n",
      "Requirement already satisfied: decorator in /usr/local/lib/python3.6/dist-packages (from ipython>=5.1.0; python_version >= \"3.4\"->ipdb) (4.4.2)\n",
      "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.6/dist-packages (from ipython>=5.1.0; python_version >= \"3.4\"->ipdb) (4.3.3)\n",
      "Requirement already satisfied: pickleshare in /usr/local/lib/python3.6/dist-packages (from ipython>=5.1.0; python_version >= \"3.4\"->ipdb) (0.7.5)\n",
      "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython>=5.1.0; python_version >= \"3.4\"->ipdb) (1.12.0)\n",
      "Requirement already satisfied: wcwidth in /usr/local/lib/python3.6/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython>=5.1.0; python_version >= \"3.4\"->ipdb) (0.2.5)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.6/dist-packages (from pexpect; sys_platform != \"win32\"->ipython>=5.1.0; python_version >= \"3.4\"->ipdb) (0.6.0)\n",
      "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.6/dist-packages (from traitlets>=4.2->ipython>=5.1.0; python_version >= \"3.4\"->ipdb) (0.2.0)\n",
      "Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (3.0.1)\n",
      "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.5)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
      "Requirement already satisfied: sentencepiece!=0.1.92 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.1.91)\n",
      "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.43)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
      "Requirement already satisfied: tokenizers==0.8.0-rc4 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.8.0rc4)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.15.1)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.12.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.9)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.6.20)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n"
     ]
    }
   ],
   "source": [
    "!pip install ipdb\n",
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Package loading "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
      "  import pandas.util.testing as tm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sb\n",
    "import numpy as np\n",
    "\n",
    "import ipdb\n",
    "import spacy\n",
    "spacy.load(\"en_core_web_sm\")\n",
    "import torch\n",
    "from torchtext import data\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchsummary import summary\n",
    "\n",
    "from sklearn.model_selection import KFold, StratifiedKFold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"./data/Train_process.csv\")\n",
    "test = pd.read_csv(\"./data/Test_process.csv\")\n",
    "\n",
    "# train = pd.read_csv(\"./data/Train.csv\")\n",
    "# test = pd.read_csv(\"./data/Test.csv\")\n",
    "\n",
    "sample = pd.read_csv(\"./data/SampleSubmission.csv\")\n",
    "\n",
    "y_kfold = train.label.values # this is used down in the the kfold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>611</th>\n",
       "      <td>BOHSNXCN</td>\n",
       "      <td>what should i do to stop alcoholism ?</td>\n",
       "      <td>Alcohol</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>612</th>\n",
       "      <td>GVDXRQPY</td>\n",
       "      <td>how to become my oneself again</td>\n",
       "      <td>Suicide</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>613</th>\n",
       "      <td>IO4JHIQS</td>\n",
       "      <td>how can someone stop it ?</td>\n",
       "      <td>Alcohol</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>614</th>\n",
       "      <td>1DS3P1XO</td>\n",
       "      <td>i feel unworthy</td>\n",
       "      <td>Depression</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>615</th>\n",
       "      <td>ORF71PVQ</td>\n",
       "      <td>i feel so discouraged with life</td>\n",
       "      <td>Depression</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           ID                                   text       label\n",
       "611  BOHSNXCN  what should i do to stop alcoholism ?     Alcohol\n",
       "612  GVDXRQPY         how to become my oneself again     Suicide\n",
       "613  IO4JHIQS              how can someone stop it ?     Alcohol\n",
       "614  1DS3P1XO                        i feel unworthy  Depression\n",
       "615  ORF71PVQ        i feel so discouraged with life  Depression"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let check the distribution of the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>304</th>\n",
       "      <td>Z9A6ACLK</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>305</th>\n",
       "      <td>ZDUOIGKN</td>\n",
       "      <td>my girlfriend dumped me</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>306</th>\n",
       "      <td>ZHQ60CCH</td>\n",
       "      <td>how can i go back to being my old self ?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>307</th>\n",
       "      <td>ZVIJMA4O</td>\n",
       "      <td>is it true hang is medicinal ?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>308</th>\n",
       "      <td>ZYIFAY98</td>\n",
       "      <td>how can i overcome the problem ?</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           ID                                      text\n",
       "304  Z9A6ACLK                                       yes\n",
       "305  ZDUOIGKN                   my girlfriend dumped me\n",
       "306  ZHQ60CCH  how can i go back to being my old self ?\n",
       "307  ZVIJMA4O            is it true hang is medicinal ?\n",
       "308  ZYIFAY98          how can i overcome the problem ?"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Depression</th>\n",
       "      <th>Alcohol</th>\n",
       "      <th>Suicide</th>\n",
       "      <th>Drugs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>304</th>\n",
       "      <td>Z9A6ACLK</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>305</th>\n",
       "      <td>ZDUOIGKN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>306</th>\n",
       "      <td>ZHQ60CCH</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>307</th>\n",
       "      <td>ZVIJMA4O</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>308</th>\n",
       "      <td>ZYIFAY98</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           ID  Depression  Alcohol  Suicide  Drugs\n",
       "304  Z9A6ACLK           0        0        0      0\n",
       "305  ZDUOIGKN           0        0        0      0\n",
       "306  ZHQ60CCH           0        0        0      0\n",
       "307  ZVIJMA4O           0        0        0      0\n",
       "308  ZYIFAY98           0        0        0      0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Depression    352\n",
       "Alcohol       140\n",
       "Suicide        66\n",
       "Drugs          58\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Alcohol', 'Depression', 'Drugs', 'Suicide']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(np.unique(train.label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # This plot need to be checked \n",
    "# plt.pie(list(train.len.value_counts()), labels=list(np.unique(train.len)))\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing Data\n",
    "\n",
    "One of the main concepts of TorchText is the `Field`. These define how your data should be processed. In our sentiment classification task the data consists of both the raw string of the review and the sentiment, either \"pos\" or \"neg\".\n",
    "\n",
    "The parameters of a `Field` specify how the data should be processed. \n",
    "\n",
    "We use the `TEXT` field to define how the review should be processed, and the `LABEL` field to process the sentiment. \n",
    "\n",
    "Our `TEXT` field has `tokenize='spacy'` as an argument. This defines that the \"tokenization\" (the act of splitting the string into discrete \"tokens\") should be done using the [spaCy](https://spacy.io) tokenizer. If no `tokenize` argument is passed, the default is simply splitting the string on spaces.\n",
    "\n",
    "`LABEL` is defined by a `LabelField`, a special subset of the `Field` class specifically used for handling labels. We will explain the `dtype` argument later.\n",
    "\n",
    "For more on `Fields`, go [here](https://github.com/pytorch/text/blob/master/torchtext/data/field.py).\n",
    "\n",
    "We also set the random seeds for reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 1234\n",
    "\n",
    "# torch.manual_seed(SEED)\n",
    "# torch.backends.cudnn.deterministic = True\n",
    "\n",
    "# # TEXT = data.Field(tokenize = 'spacy')\n",
    "# LABELS = data.LabelField(dtype = torch.float)\n",
    "\n",
    "# TEXT = data.Field(batch_first = True,\n",
    "#                   use_vocab = False,\n",
    "# #                   tokenize = tokenize_and_cut,\n",
    "#                   preprocessing = tokenizer.convert_tokens_to_ids,\n",
    "#                   init_token = init_token_idx,\n",
    "#                   eos_token = eos_token_idx,\n",
    "#                   pad_token = pad_token_idx,\n",
    "#                   unk_token = unk_token_idx)\n",
    "\n",
    "import torch\n",
    "from torchtext import data\n",
    "from torchtext import datasets\n",
    "import random\n",
    "\n",
    "SEED = 1234\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "TEXT = data.Field(tokenize = 'spacy')\n",
    "LABELS = data.LabelField(dtype = torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = data.TabularDataset.splits(\n",
    "    path='./data', train='Train_process.csv',\n",
    "    test='Test_process.csv', format='csv', skip_header=True,\n",
    "    fields=[('ID', None),\n",
    "            ('text', TEXT),\n",
    "            ('label', LABELS)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 616\n",
      "Number of testing examples: 309\n"
     ]
    }
   ],
   "source": [
    "print(f'Number of training examples: {len(train_data)}')\n",
    "print(f'Number of testing examples: {len(test_data)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let check one example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': ['i', 'feel', 'that', 'it', 'was', 'better', 'i', 'die', 'am', 'happy'], 'label': 'Depression'}\n"
     ]
    }
   ],
   "source": [
    "print(vars(train_data.examples[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'label': 'Depression',\n",
       " 'text': ['i', 'feel', 'so', 'discouraged', 'with', 'life']}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vars(train_data[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAX_VOCAB_SIZE = 25_000\n",
    "\n",
    "# glove.6B.300d\": 862 M\n",
    "\n",
    "TEXT.build_vocab(train_data, \n",
    "#                  max_size = MAX_VOCAB_SIZE, \n",
    "                 vectors = \"glove.6B.300d\",  #glove.twitter.27B.200d\n",
    "                 unk_init = torch.Tensor.normal_)\n",
    "\n",
    "LABELS.build_vocab(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "train_data, valid_data = train_data.split(random_state = random.seed(SEED), \n",
    "                                          stratified=True, split_ratio=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 493\n",
      "Number of testing examples: 309\n"
     ]
    }
   ],
   "source": [
    "print(f'Number of training examples: {len(train_data)}')\n",
    "# print(f'Number of validation examples: {len(valid_data)}')\n",
    "print(f'Number of testing examples: {len(test_data)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique tokens in LABEL vocabulary: 4\n"
     ]
    }
   ],
   "source": [
    "# print(f\"Unique tokens in TEXT vocabulary: {len(TEXT.vocab)}\")\n",
    "print(f\"Unique tokens in LABEL vocabulary: {len(LABELS.vocab)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<function _default_unk_index at 0x7f9a5cf4e2f0>, {'Depression': 0, 'Alcohol': 1, 'Suicide': 2, 'Drugs': 3})\n"
     ]
    }
   ],
   "source": [
    "print(LABELS.vocab.stoi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': ['what', 'are', 'the', 'effects', 'of', 'depression', '?'], 'label': 'Depression'}\n"
     ]
    }
   ],
   "source": [
    "print(vars(valid_data.examples[-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final step of preparing the data is creating the iterators. We iterate over these in the training/evaluation loop, and they return a batch of examples (indexed and converted into tensors) at each iteration.\n",
    "\n",
    "We'll use a BucketIterator which is a special type of iterator that will return a batch of examples where each example is of a similar length, minimizing the amount of padding per example.\n",
    "\n",
    "We also want to place the tensors returned by the iterator on the GPU (if you're using one). PyTorch handles this using torch.device, we then pass this device to the iterator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(\n",
    "    (train_data, valid_data, test_data), \n",
    "    batch_size = BATCH_SIZE,\n",
    "    sort=False,\n",
    "    device = device)\n",
    "\n",
    "# train_iterator, test_iterator = data.BucketIterator.splits(\n",
    "#     (train_data, test_data), \n",
    "#     batch_size = BATCH_SIZE,\n",
    "#     sort=False,\n",
    "#     device = device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# next(test_iterator.__iter__())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, n_filters, filter_sizes, output_dim, \n",
    "                 dropout, pad_idx):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        \n",
    "        self.convs = nn.ModuleList([\n",
    "                                    nn.Conv2d(in_channels = 1, \n",
    "                                              out_channels = n_filters, \n",
    "                                              kernel_size = (fs, embedding_dim)) \n",
    "                                    for fs in filter_sizes\n",
    "                                    ])\n",
    "        \n",
    "        self.fc = nn.Linear(len(filter_sizes) * n_filters, output_dim)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, text):\n",
    "        \n",
    "        #text = [sent len, batch size]\n",
    "        \n",
    "        text = text.permute(1, 0)\n",
    "                \n",
    "        #text = [batch size, sent len]\n",
    "        \n",
    "        embedded = self.embedding(text)\n",
    "                \n",
    "        #embedded = [batch size, sent len, emb dim]\n",
    "        \n",
    "        embedded = embedded.unsqueeze(1)\n",
    "        \n",
    "        #embedded = [batch size, 1, sent len, emb dim]\n",
    "        \n",
    "        conved = [F.relu(conv(embedded)).squeeze(3) for conv in self.convs]\n",
    "            \n",
    "        #conv_n = [batch size, n_filters, sent len - filter_sizes[n]]\n",
    "        \n",
    "        pooled = [F.max_pool1d(conv, conv.shape[2]).squeeze(2) for conv in conved]\n",
    "        \n",
    "        #pooled_n = [batch size, n_filters]\n",
    "        \n",
    "        cat = self.dropout(torch.cat(pooled, dim = 1))\n",
    "\n",
    "        #cat = [batch size, n_filters * len(filter_sizes)]\n",
    "        \n",
    "        out = self.fc(cat)\n",
    "        \n",
    "#         out = self.sigmoid(out)\n",
    "            \n",
    "        return out #self.fc(cat) #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIM = len(TEXT.vocab)\n",
    "EMBEDDING_DIM = 300 #100\n",
    "N_FILTERS = 100\n",
    "FILTER_SIZES = [2,3,4]\n",
    "OUTPUT_DIM = len(LABELS.vocab)\n",
    "DROPOUT = 0.3\n",
    "PAD_IDX = TEXT.vocab.stoi[TEXT.pad_token]\n",
    "\n",
    "model = CNN(INPUT_DIM, EMBEDDING_DIM, N_FILTERS, FILTER_SIZES, OUTPUT_DIM, DROPOUT, PAD_IDX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # can view the summary if you like\n",
    "# model.to(device)\n",
    "# summary(model, (16, 32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAX_VOCAB_SIZE = 25_000\n",
    "\n",
    "# TEXT.build_vocab(train_data, \n",
    "#                  max_size = MAX_VOCAB_SIZE, \n",
    "#                  vectors = \"glove.6B.100d\", \n",
    "#                  unk_init = torch.Tensor.normal_)\n",
    "\n",
    "# LABEL.build_vocab(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also create a function that will tell us how many trainable parameters our model has so we can compare the number of parameters across different models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 512,104 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for name, param in model.named_parameters():                \n",
    "#     if name.startswith('bert'):\n",
    "#         param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 512,104 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedding.weight\n",
      "convs.0.weight\n",
      "convs.0.bias\n",
      "convs.1.weight\n",
      "convs.1.bias\n",
      "convs.2.weight\n",
      "convs.2.bias\n",
      "fc.weight\n",
      "fc.bias\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():                \n",
    "    if param.requires_grad:\n",
    "        print(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll load our pre-trained embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1117, -0.4966,  0.1631,  ..., -1.4447,  0.8402, -0.8668],\n",
       "        [ 0.1032, -1.6268,  0.5729,  ...,  0.3180, -0.1626, -0.0417],\n",
       "        [-0.1329,  0.1699, -0.1436,  ..., -0.2378,  0.1477,  0.6290],\n",
       "        ...,\n",
       "        [ 0.5390,  0.0090,  0.2787,  ..., -0.1150, -0.4234,  0.0318],\n",
       "        [-0.3364, -0.1851, -0.1022,  ..., -0.1311, -0.0346,  0.3446],\n",
       "        [-0.2117,  0.1462,  0.0605,  ..., -0.4895, -0.0707, -0.0814]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pretrained_embeddings = TEXT.vocab.vectors\n",
    "\n",
    "model.embedding.weight.data.copy_(pretrained_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then zero the initial weights of the unknown and padding tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "UNK_IDX = TEXT.vocab.stoi[TEXT.unk_token]\n",
    "\n",
    "model.embedding.weight.data[UNK_IDX] = torch.zeros(EMBEDDING_DIM)\n",
    "model.embedding.weight.data[PAD_IDX] = torch.zeros(EMBEDDING_DIM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another different to the previous notebooks is our loss function (aka criterion). Before we used `BCEWithLogitsLoss`, however now we use `CrossEntropyLoss`. Without going into too much detail, `CrossEntropyLoss` performs a *softmax* function over our model outputs and the loss is given by the *cross entropy* between that and the label.\n",
    "\n",
    "Generally:\n",
    "- `CrossEntropyLoss` is used when our examples exclusively belong to one of $C$ classes\n",
    "- `BCEWithLogitsLoss` is used when our examples exclusively belong to only 2 classes (0 and 1) and is also used in the case where our examples belong to between 0 and $C$ classes (aka multilabel classification)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Depression', 352), ('Alcohol', 140), ('Suicide', 66), ('Drugs', 58)]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "class_distrbution = Counter()\n",
    "for text in train[\"label\"].values:\n",
    "    class_distrbution[text] += 1\n",
    "        \n",
    "class_distrbution.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "616"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(list(class_distrbution.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'Alcohol': 140, 'Depression': 352, 'Drugs': 58, 'Suicide': 66})"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_distrbution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['Depression', 'Alcohol', 'Suicide', 'Drugs'])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LABELS.vocab.stoi.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([1.0000, 2.5143, 5.3333, 6.0690], device='cuda:0'),\n",
       " Counter({'Alcohol': 140, 'Depression': 352, 'Drugs': 58, 'Suicide': 66}))"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# criterion = nn.CrossEntropyLoss(weight = x)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# train_loader_origin.dataset.classes,\n",
    "class_weights = [class_distrbution[i] for i in LABELS.vocab.stoi.keys()]\n",
    "class_weights_normalized = [max(class_weights)/i for i in class_weights]\n",
    "\n",
    "class_weights_normalized ,torch.Tensor(class_weights_normalized)\n",
    "\n",
    "w = torch.Tensor(class_weights_normalized)\n",
    "w = w.to(device)\n",
    "# x = x\n",
    "w,class_distrbution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([352, 140, 66, 58], tensor([1.0000, 2.5143, 5.3333, 6.0690], device='cuda:0'))"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_weights, w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# criterion = nn.CrossEntropyLoss(weight=w)\n",
    "\n",
    "\n",
    "\n",
    "model = model.to(device)\n",
    "criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before, we had a function that calculated accuracy in the binary label case, where we said if the value was over 0.5 then we would assume it is positive. In the case where we have more than 2 classes, our model outputs a $C$ dimensional vector, where the value of each element is the beleief that the example belongs to that class. \n",
    "\n",
    "For example, in our labels we have: 'HUM' = 0, 'ENTY' = 1, 'DESC' = 2, 'NUM' = 3, 'LOC' = 4 and 'ABBR' = 5. If the output of our model was something like: **[5.1, 0.3, 0.1, 2.1, 0.2, 0.6]** this means that the model strongly believes the example belongs to class 0, a question about a human, and slightly believes the example belongs to class 3, a numerical question.\n",
    "\n",
    "We calculate the accuracy by performing an `argmax` to get the index of the maximum value in the prediction for each element in the batch, and then counting how many times this equals the actual label. We then average this across the batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score\n",
    "\n",
    "def categorical_accuracy(preds, y):\n",
    "    \"\"\"\n",
    "    Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8, NOT 8\n",
    "    \"\"\"\n",
    "    max_preds = preds.argmax(dim = 1, keepdim = True) # get the index of the max probability\n",
    "    \n",
    "    correct = max_preds.squeeze(1).eq(y)\n",
    "#     ipdb.set_trace()\n",
    "    pr = correct.sum().item() / torch.FloatTensor([y.shape[0]])\n",
    "    \n",
    "#     pr = precision_score(y.detach().numpy(), \n",
    "#                            max_preds.argmax(dim = 1, \n",
    "#                                         keepdim = True).detach().numpy(), average='macro')\n",
    "#     ipdb.set_trace()\n",
    "    return pr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training loop is similar to before, without the need to `squeeze` the model predictions as `CrossEntropyLoss` expects the input to be **[batch size, n classes]** and the label to be **[batch size]**.\n",
    "\n",
    "The label needs to be a `LongTensor`, which it is by default as we did not set the `dtype` to a `FloatTensor` as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for batch in iterator:\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        \n",
    "        predictions = model(batch.text)\n",
    "#         ipdb.set_trace()\n",
    "        \n",
    "        loss = criterion(predictions, batch.label.long())\n",
    "        \n",
    "        acc = categorical_accuracy(predictions, batch.label.long())\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The evaluation loop is, again, similar to before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, criterion):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for batch in iterator:\n",
    "\n",
    "            predictions = model(batch.text)\n",
    "            \n",
    "            loss = criterion(predictions, batch.label.long())\n",
    "            \n",
    "            acc = categorical_accuracy(predictions, batch.label.long())\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we train our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.904 | Train Acc: 67.11%\n",
      "\t Val. Loss: 0.643 |  Val. Acc: 77.98%\n",
      "Epoch: 02 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.468 | Train Acc: 84.69%\n",
      "\t Val. Loss: 0.466 |  Val. Acc: 83.45%\n",
      "Epoch: 03 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.289 | Train Acc: 91.32%\n",
      "\t Val. Loss: 0.409 |  Val. Acc: 85.01%\n",
      "Epoch: 04 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.202 | Train Acc: 94.46%\n",
      "\t Val. Loss: 0.399 |  Val. Acc: 84.23%\n",
      "Epoch: 05 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.138 | Train Acc: 96.78%\n",
      "\t Val. Loss: 0.399 |  Val. Acc: 84.23%\n",
      "Epoch: 06 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train Acc: 99.02%\n",
      "\t Val. Loss: 0.395 |  Val. Acc: 85.79%\n",
      "Epoch: 07 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.062 | Train Acc: 99.41%\n",
      "\t Val. Loss: 0.446 |  Val. Acc: 85.01%\n",
      "Epoch: 08 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.048 | Train Acc: 99.41%\n",
      "\t Val. Loss: 0.413 |  Val. Acc: 84.87%\n",
      "Epoch: 09 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.034 | Train Acc: 99.61%\n",
      "\t Val. Loss: 0.424 |  Val. Acc: 84.09%\n",
      "Epoch: 10 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.022 | Train Acc: 100.00%\n",
      "\t Val. Loss: 0.474 |  Val. Acc: 85.01%\n"
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 10\n",
    "\n",
    "best_valid_loss = 0.28 #float('inf')\n",
    "best_valid_acc = 0.88\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "\n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_loss, train_acc = train(model, train_iterator, optimizer, criterion)\n",
    "    valid_loss, valid_acc = evaluate(model, valid_iterator, criterion)\n",
    "    \n",
    "    end_time = time.time()\n",
    "\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')\n",
    "    \n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        print('Saving Model ...')\n",
    "        torch.save(model.state_dict(), 'Best_Model_Bert_'+str(best_valid_loss)[:4]+'.pt')\n",
    "        print('*****************************************************')\n",
    "        print('best record: [epoch %d], [val loss %.5f], [val acc %.5f]' % (epoch, valid_loss, valid_acc))\n",
    "        print('*****************************************************')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score\n",
    "\n",
    "def categorical_accuracy(preds, y):\n",
    "    \"\"\"\n",
    "    Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8, NOT 8\n",
    "    \"\"\"\n",
    "    max_preds = preds.argmax(dim = 1, keepdim = True) # get the index of the max probability\n",
    "    \n",
    "    correct = max_preds.squeeze(1).eq(y)\n",
    "#     ipdb.set_trace()\n",
    "    pr = correct.sum().item() / torch.FloatTensor([y.shape[0]])\n",
    "    \n",
    "#     pr = precision_score(y.detach().numpy(), \n",
    "#                            max_preds.argmax(dim = 1, \n",
    "#                                         keepdim = True).detach().numpy(), average='macro')\n",
    "#     ipdb.set_trace()\n",
    "    return pr\n",
    "\n",
    "def train(model, iterator, optimizer, criterion):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for batch in iterator:\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        \n",
    "        predictions = model(batch.text)\n",
    "#         ipdb.set_trace()\n",
    "        \n",
    "        loss = criterion(predictions, batch.label.long())\n",
    "        \n",
    "        acc = categorical_accuracy(predictions, batch.label.long())\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n",
    "\n",
    "\n",
    "def evaluate(model, iterator, criterion):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for batch in iterator:\n",
    "\n",
    "            predictions = model(batch.text)\n",
    "            \n",
    "            loss = criterion(predictions, batch.label.long())\n",
    "            \n",
    "            acc = categorical_accuracy(predictions, batch.label.long())\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n",
    "\n",
    "\n",
    "import time\n",
    "\n",
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, n_filters, filter_sizes, output_dim, \n",
    "                 dropout, pad_idx):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        \n",
    "        self.convs = nn.ModuleList([\n",
    "                                    nn.Conv2d(in_channels = 1, \n",
    "                                              out_channels = n_filters, \n",
    "                                              kernel_size = (fs, embedding_dim)) \n",
    "                                    for fs in filter_sizes\n",
    "                                    ])\n",
    "        \n",
    "        self.fc = nn.Linear(len(filter_sizes) * n_filters, output_dim)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, text):\n",
    "        \n",
    "        #text = [sent len, batch size]\n",
    "        \n",
    "        text = text.permute(1, 0)\n",
    "                \n",
    "        #text = [batch size, sent len]\n",
    "        \n",
    "        embedded = self.embedding(text)\n",
    "                \n",
    "        #embedded = [batch size, sent len, emb dim]\n",
    "        \n",
    "        embedded = embedded.unsqueeze(1)\n",
    "        \n",
    "        #embedded = [batch size, 1, sent len, emb dim]\n",
    "        \n",
    "        conved = [F.relu(conv(embedded)).squeeze(3) for conv in self.convs]\n",
    "            \n",
    "        #conv_n = [batch size, n_filters, sent len - filter_sizes[n]]\n",
    "        \n",
    "        pooled = [F.max_pool1d(conv, conv.shape[2]).squeeze(2) for conv in conved]\n",
    "        \n",
    "        #pooled_n = [batch size, n_filters]\n",
    "        \n",
    "        cat = self.dropout(torch.cat(pooled, dim = 1))\n",
    "\n",
    "        #cat = [batch size, n_filters * len(filter_sizes)]\n",
    "        \n",
    "        out = self.fc(cat)\n",
    "        \n",
    "#         out = self.sigmoid(out)\n",
    "            \n",
    "        return out #self.fc(cat) #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class load_data(object):\n",
    "    def __init__(self, SEED=1234):\n",
    "        torch.manual_seed(SEED)\n",
    "        torch.cuda.manual_seed(SEED)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "\n",
    "        TEXT = data.Field(tokenize = 'spacy')\n",
    "        LABELS = data.LabelField(dtype = torch.float)\n",
    "\n",
    "\n",
    "#         self.train_data, self.test_data = datasets.IMDB.splits(TEXT, LABEL)\n",
    "        \n",
    "        self.train_data, self.test_data = data.TabularDataset.splits(\n",
    "            path='./data', train='Train_process.csv',\n",
    "            test='Test_process.csv', format='csv', skip_header=True,\n",
    "            fields=[('ID', None),\n",
    "                    ('text', TEXT),\n",
    "                    ('label', LABELS)])\n",
    "\n",
    "        self.SEED = SEED\n",
    "\n",
    "\n",
    "    def get_fold_data(self, num_folds=10):\n",
    "        \"\"\"\n",
    "        More details about 'fields' are available at \n",
    "        https://github.com/pytorch/text/blob/master/torchtext/datasets/imdb.py\n",
    "        \"\"\"\n",
    "\n",
    "        TEXT = data.Field(tokenize='spacy')\n",
    "        LABELS = data.LabelField(dtype=torch.float)\n",
    "        fields = [('text', TEXT), ('label', LABELS)]\n",
    "        \n",
    "#         kf = KFold(n_splits=num_folds, shuffle=True, random_state=self.SEED)\n",
    "        kf = StratifiedKFold(n_splits=num_folds, random_state=self.SEED)\n",
    "        \n",
    "        train_data_arr = np.array(self.train_data.examples)\n",
    "\n",
    "#         ipdb.set_trace()\n",
    "        for train_index, val_index in kf.split(train_data_arr, y_kfold):\n",
    "            yield(\n",
    "                TEXT,\n",
    "                LABELS,\n",
    "                data.Dataset(train_data_arr[train_index], fields=fields),\n",
    "                data.Dataset(train_data_arr[val_index], fields=fields),\n",
    "            )\n",
    "    \n",
    "    def get_test_data(self):\n",
    "        return self.test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KFold?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py:296: FutureWarning: Setting a random_state has no effect since shuffle is False. This will raise an error in 0.24. You should leave random_state to its default (None), or set shuffle=True.\n",
      "  FutureWarning\n",
      ".vector_cache/glove.6B.zip: 0.00B [00:00, ?B/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** Running Training *****\n",
      "Now fold: 1 / 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ".vector_cache/glove.6B.zip: 862MB [06:26, 2.23MB/s]                              \n",
      "100%|| 399555/400000 [00:37<00:00, 10831.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding size: torch.Size([717, 300]).\n",
      "| Epoch: 01 | Train Loss: 0.924 | Train Acc: 64.13%\n",
      "| Epoch: 01 | Valid Loss: 0.616 | Valid Acc: 80.58%\n",
      "################################################################################\n",
      "| Epoch: 02 | Train Loss: 0.485 | Train Acc: 83.98%\n",
      "| Epoch: 02 | Valid Loss: 0.416 | Valid Acc: 85.94%\n",
      "################################################################################\n",
      "| Epoch: 03 | Train Loss: 0.320 | Train Acc: 89.13%\n",
      "| Epoch: 03 | Valid Loss: 0.372 | Valid Acc: 86.94%\n",
      "################################################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 399555/400000 [00:50<00:00, 10831.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Epoch: 04 | Train Loss: 0.210 | Train Acc: 94.34%\n",
      "| Epoch: 04 | Valid Loss: 0.325 | Valid Acc: 89.51%\n",
      "################################################################################\n",
      "| Epoch: 05 | Train Loss: 0.139 | Train Acc: 97.46%\n",
      "| Epoch: 05 | Valid Loss: 0.305 | Valid Acc: 89.51%\n",
      "################################################################################\n",
      "Val. Loss: 0.298 | Val. Acc: 89.84% |\n",
      "***** Running Training *****\n",
      "Now fold: 2 / 5\n",
      "Embedding size: torch.Size([708, 300]).\n",
      "| Epoch: 01 | Train Loss: 0.908 | Train Acc: 67.79%\n",
      "| Epoch: 01 | Valid Loss: 0.560 | Valid Acc: 84.14%\n",
      "################################################################################\n",
      "| Epoch: 02 | Train Loss: 0.463 | Train Acc: 84.98%\n",
      "| Epoch: 02 | Valid Loss: 0.396 | Valid Acc: 86.92%\n",
      "################################################################################\n",
      "| Epoch: 03 | Train Loss: 0.306 | Train Acc: 90.34%\n",
      "| Epoch: 03 | Valid Loss: 0.356 | Valid Acc: 86.00%\n",
      "################################################################################\n",
      "| Epoch: 04 | Train Loss: 0.201 | Train Acc: 94.53%\n",
      "| Epoch: 04 | Valid Loss: 0.337 | Valid Acc: 90.34%\n",
      "################################################################################\n",
      "| Epoch: 05 | Train Loss: 0.143 | Train Acc: 96.39%\n",
      "| Epoch: 05 | Valid Loss: 0.328 | Valid Acc: 90.62%\n",
      "################################################################################\n",
      "Val. Loss: 0.333 | Val. Acc: 90.34% |\n",
      "***** Running Training *****\n",
      "Now fold: 3 / 5\n",
      "Embedding size: torch.Size([716, 300]).\n",
      "| Epoch: 01 | Train Loss: 0.861 | Train Acc: 69.64%\n",
      "| Epoch: 01 | Valid Loss: 0.602 | Valid Acc: 78.82%\n",
      "################################################################################\n",
      "| Epoch: 02 | Train Loss: 0.425 | Train Acc: 86.13%\n",
      "| Epoch: 02 | Valid Loss: 0.467 | Valid Acc: 84.87%\n",
      "################################################################################\n",
      "| Epoch: 03 | Train Loss: 0.291 | Train Acc: 91.12%\n",
      "| Epoch: 03 | Valid Loss: 0.396 | Valid Acc: 85.65%\n",
      "################################################################################\n",
      "| Epoch: 04 | Train Loss: 0.193 | Train Acc: 94.14%\n",
      "| Epoch: 04 | Valid Loss: 0.368 | Valid Acc: 85.85%\n",
      "################################################################################\n",
      "| Epoch: 05 | Train Loss: 0.137 | Train Acc: 96.88%\n",
      "| Epoch: 05 | Valid Loss: 0.336 | Valid Acc: 84.87%\n",
      "################################################################################\n",
      "Val. Loss: 0.333 | Val. Acc: 85.50% |\n",
      "***** Running Training *****\n",
      "Now fold: 4 / 5\n",
      "Embedding size: torch.Size([707, 300]).\n",
      "| Epoch: 01 | Train Loss: 0.890 | Train Acc: 67.20%\n",
      "| Epoch: 01 | Valid Loss: 0.639 | Valid Acc: 80.38%\n",
      "################################################################################\n",
      "| Epoch: 02 | Train Loss: 0.446 | Train Acc: 85.55%\n",
      "| Epoch: 02 | Valid Loss: 0.465 | Valid Acc: 83.65%\n",
      "################################################################################\n",
      "| Epoch: 03 | Train Loss: 0.286 | Train Acc: 90.93%\n",
      "| Epoch: 03 | Valid Loss: 0.452 | Valid Acc: 84.29%\n",
      "################################################################################\n",
      "| Epoch: 04 | Train Loss: 0.191 | Train Acc: 94.53%\n",
      "| Epoch: 04 | Valid Loss: 0.393 | Valid Acc: 84.87%\n",
      "################################################################################\n",
      "| Epoch: 05 | Train Loss: 0.135 | Train Acc: 97.27%\n",
      "| Epoch: 05 | Valid Loss: 0.415 | Valid Acc: 84.09%\n",
      "################################################################################\n",
      "Val. Loss: 0.417 | Val. Acc: 84.09% |\n",
      "***** Running Training *****\n",
      "Now fold: 5 / 5\n",
      "Embedding size: torch.Size([708, 300]).\n",
      "| Epoch: 01 | Train Loss: 0.894 | Train Acc: 68.18%\n",
      "| Epoch: 01 | Valid Loss: 0.648 | Valid Acc: 78.82%\n",
      "################################################################################\n",
      "| Epoch: 02 | Train Loss: 0.451 | Train Acc: 85.74%\n",
      "| Epoch: 02 | Valid Loss: 0.472 | Valid Acc: 83.80%\n",
      "################################################################################\n",
      "| Epoch: 03 | Train Loss: 0.308 | Train Acc: 89.86%\n",
      "| Epoch: 03 | Valid Loss: 0.411 | Valid Acc: 84.09%\n",
      "################################################################################\n",
      "| Epoch: 04 | Train Loss: 0.211 | Train Acc: 92.68%\n",
      "| Epoch: 04 | Valid Loss: 0.395 | Valid Acc: 85.65%\n",
      "################################################################################\n",
      "| Epoch: 05 | Train Loss: 0.151 | Train Acc: 96.68%\n",
      "| Epoch: 05 | Valid Loss: 0.406 | Valid Acc: 87.21%\n",
      "################################################################################\n",
      "Val. Loss: 0.411 | Val. Acc: 87.07% |\n",
      "***** Cross Validation Result *****\n",
      "LOSS: 0.35822638086974623, ACC: 0.8736689805984497\n"
     ]
    }
   ],
   "source": [
    "data_generator = load_data()\n",
    "_history = []\n",
    "device = None\n",
    "model = None\n",
    "criterion = None\n",
    "fold_index = 0\n",
    "num_folds = 5\n",
    "batch_size = 32\n",
    "epochs = 5\n",
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "# optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "best_valid_acc = 0.9\n",
    "best_val_loss = 0.25\n",
    "    \n",
    "for TEXT, LABELS, train_data, val_data in data_generator.get_fold_data(num_folds=num_folds):\n",
    "    print(\"***** Running Training *****\")\n",
    "    print(f\"Now fold: {fold_index + 1} / {num_folds}\")\n",
    "\n",
    "    TEXT.build_vocab(train_data,\n",
    "                     vectors = \"glove.6B.300d\",\n",
    "                     unk_init = torch.Tensor.normal_)\n",
    "    \n",
    "    print(f'Embedding size: {TEXT.vocab.vectors.size()}.')\n",
    "    LABELS.build_vocab(train_data) # For converting str into float labels.\n",
    "    \n",
    "#     Model(len(TEXT.vocab), embedding_dim, hidden_dim,\n",
    "#         output_dim, num_layers, dropout, TEXT.vocab.vectors, embedding_trainable)\n",
    "    \n",
    "    INPUT_DIM = len(TEXT.vocab)\n",
    "    EMBEDDING_DIM = 300\n",
    "    N_FILTERS = 100\n",
    "    FILTER_SIZES = [2,3,4]\n",
    "    OUTPUT_DIM = len(LABELS.vocab)\n",
    "    DROPOUT = 0.308\n",
    "    PAD_IDX = TEXT.vocab.stoi[TEXT.pad_token]\n",
    "    \n",
    "    model = CNN(INPUT_DIM, EMBEDDING_DIM, N_FILTERS, FILTER_SIZES, OUTPUT_DIM, DROPOUT, PAD_IDX)\n",
    "\n",
    "    pretrained_embeddings = TEXT.vocab.vectors\n",
    "\n",
    "    model.embedding.weight.data.copy_(pretrained_embeddings)\n",
    "    \n",
    "    UNK_IDX = TEXT.vocab.stoi[TEXT.unk_token]\n",
    "\n",
    "    model.embedding.weight.data[UNK_IDX] = torch.zeros(EMBEDDING_DIM)\n",
    "    model.embedding.weight.data[PAD_IDX] = torch.zeros(EMBEDDING_DIM)\n",
    "    \n",
    "    \n",
    "    # Get w\n",
    "    \n",
    "    from collections import Counter\n",
    "    class_distrbution = Counter()\n",
    "    for text in y_kfold:\n",
    "        class_distrbution[text] += 1\n",
    "\n",
    "\n",
    "    class_weights = [class_distrbution[i] for i in LABELS.vocab.stoi.keys()]\n",
    "    class_weights_normalized = [max(class_weights)/i for i in class_weights]\n",
    "\n",
    "\n",
    "    w = torch.Tensor(class_weights_normalized)\n",
    "    w = w.to(device)\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters())\n",
    "#     criterion = nn.CrossEntropyLoss(weight=w)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "        \n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = model.to(device)\n",
    "    criterion = criterion.to(device)\n",
    "\n",
    "    train_iterator = data.Iterator(train_data, batch_size=batch_size, sort_key=lambda x: len(x.text), device=device)\n",
    "    val_iterator = data.Iterator(val_data, batch_size=batch_size, sort_key=lambda x: len(x.text), device=device)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        train_loss, train_acc = train(model, train_iterator, optimizer, criterion)\n",
    "        val_loss, val_acc = evaluate(model, val_iterator, criterion)\n",
    "        print(f'| Epoch: {epoch+1:02} | Train Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
    "        print(f'| Epoch: {epoch+1:02} | Valid Loss: {val_loss:.3f} | Valid Acc: {val_acc*100:.2f}%')\n",
    "        print('################################################################################')\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            print('Saving Model ...')\n",
    "            torch.save(model.state_dict(), 'Best_Model_'+str(best_val_loss)[:5]+'.pt')\n",
    "            print('*****************************************************')\n",
    "            print('best record: [epoch %d], [val loss %.5f], [val acc %.5f]' % (epoch, val_loss, val_acc))\n",
    "            print('*****************************************************')\n",
    "    \n",
    "    val_loss, val_acc = evaluate(model, val_iterator, criterion) \n",
    "    \n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        print('Saving Model ...')\n",
    "        torch.save(model.state_dict(), 'Best_Model_'+str(best_val_loss)[:5]+'.pt')\n",
    "        print('*****************************************************')\n",
    "        print('best record: [epoch %d], [val loss %.5f], [val acc %.5f]' % (epoch, val_loss, val_acc))\n",
    "        print('*****************************************************')\n",
    "    \n",
    "#     if val_acc > best_valid_acc:\n",
    "#         best_valid_acc = val_acc\n",
    "#         print('Saving Model ...')\n",
    "#         torch.save(model.state_dict(), 'Best_Model_'+str(val_acc)+'.pt')\n",
    "#         print('*****************************************************')\n",
    "#         print('best record: [epoch %d], [val loss %.5f], [val acc %.5f]' % (epoch, val_loss, val_acc))\n",
    "#         print('*****************************************************')\n",
    "        \n",
    "    print(f'Val. Loss: {val_loss:.3f} | Val. Acc: {val_acc*100:.2f}% |')\n",
    "\n",
    "    _history.append([val_loss, val_acc])\n",
    "    fold_index += 1\n",
    "\n",
    "_history = np.asarray(_history)\n",
    "loss = np.mean(_history[:, 0])\n",
    "acc = np.mean(_history[:, 1])\n",
    "\n",
    "print('***** Cross Validation Result *****')\n",
    "print(f'LOSS: {loss}, ACC: {acc}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOSS: 0.5449422478675843, ACC: 0.842294979095459\n",
    "        \n",
    "LOSS: 0.4586620181798935, ACC: 0.861218586564064\n",
    "\n",
    "LOSS: 0.3850980430841446, ACC: 0.8524718940258026\n",
    "        \n",
    "LOSS: 0.3720459468662739, ACC: 0.8570932596921921\n",
    "\n",
    "LOSS: 0.3683295398950577, ACC: 0.8664682626724243 # lower\n",
    "        \n",
    "LOSS: 0.3577434681355953, ACC: 0.8683201134204864 # spel checker contraction\n",
    "\n",
    "LOSS: 0.35579548478126527, ACC: 0.8727182626724244\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for CNN:\n\tsize mismatch for embedding.weight: copying a param with shape torch.Size([720, 300]) from checkpoint, the shape in current model is torch.Size([711, 300]).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-134-84a7b5f2e906>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Best_Model_0.305.pt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mval_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_iterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mval_loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mload_state_dict\u001b[0;34m(self, state_dict, strict)\u001b[0m\n\u001b[1;32m    845\u001b[0m             \u001b[0;34m>>\u001b[0m\u001b[0;34m>\u001b[0m \u001b[0ml\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    846\u001b[0m             \u001b[0;34m>>\u001b[0m\u001b[0;34m>\u001b[0m \u001b[0mnet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSequential\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 847\u001b[0;31m             \u001b[0;34m>>\u001b[0m\u001b[0;34m>\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mm\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnamed_modules\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    848\u001b[0m                     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'->'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for CNN:\n\tsize mismatch for embedding.weight: copying a param with shape torch.Size([720, 300]) from checkpoint, the shape in current model is torch.Size([711, 300])."
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('Best_Model_0.305.pt'))\n",
    "\n",
    "val_loss, val_acc = evaluate(model, valid_iterator, criterion) \n",
    "val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best record: [epoch 4], [val loss 0.60001], [val acc 0.83239]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's run our model on the test set!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.load_state_dict(torch.load('tut5-model.pt'))\n",
    "\n",
    "# test_loss, test_acc = evaluate(model, test_iterator, criterion)\n",
    "\n",
    "# print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 480,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def predict_sentiment(model, tokenizer, sentence):\n",
    "#     model.eval()\n",
    "#     tokens = tokenizer.tokenize(sentence)\n",
    "#     tokens = tokens#[:max_input_length-2]\n",
    "#     indexed = [init_token_idx] + tokenizer.convert_tokens_to_ids(tokens) + [eos_token_idx]\n",
    "#     tensor = torch.LongTensor(indexed).to(device)\n",
    "#     tensor = tensor.unsqueeze(0)\n",
    "#     prediction = torch.sigmoid(model(tensor))\n",
    "#     return prediction\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load('en')\n",
    "\n",
    "def predict_sentiment(model, sentence, min_len = 4):\n",
    "    model.eval()\n",
    "    tokenized = [tok.text for tok in nlp.tokenizer(sentence)]\n",
    "    if len(tokenized) < min_len:\n",
    "        tokenized += ['<pad>'] * (min_len - len(tokenized))\n",
    "    indexed = [TEXT.vocab.stoi[t] for t in tokenized]\n",
    "    tensor = torch.LongTensor(indexed).to(device)\n",
    "    tensor = tensor.unsqueeze(1)\n",
    "    preds = torch.softmax(model(tensor), dim=1)\n",
    "#     preds = torch.sigmoid(model(tensor))\n",
    "#     preds = model(tensor)\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 481,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<function _default_unk_index at 0x7f354edce2f0>, {'Depression': 0, 'Alcohol': 1, 'Suicide': 2, 'Drugs': 3})\n"
     ]
    }
   ],
   "source": [
    "print(LABELS.vocab.stoi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 482,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict_class(model, \"Who is Keyser Sze?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 483,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0117, 0.2914, 0.0116, 0.6853]], device='cuda:0',\n",
       "       grad_fn=<SoftmaxBackward>)"
      ]
     },
     "execution_count": 483,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_sentiment(model, \"weed addiction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 484,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict(model, test_iterator, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 485,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>02V56KMO</td>\n",
       "      <td>how to overcome bad feelings and emotions</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>03BMGTOK</td>\n",
       "      <td>feel like giving up in life</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>03LZVFM6</td>\n",
       "      <td>was so depressed feel like got no strength to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0EPULUM5</td>\n",
       "      <td>feel so low especially since had no one to tal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0GM4C5GD</td>\n",
       "      <td>can be successful when am a drug addict</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         ID                                               text\n",
       "0  02V56KMO          how to overcome bad feelings and emotions\n",
       "1  03BMGTOK                        feel like giving up in life\n",
       "2  03LZVFM6  was so depressed feel like got no strength to ...\n",
       "3  0EPULUM5  feel so low especially since had no one to tal...\n",
       "4  0GM4C5GD            can be successful when am a drug addict"
      ]
     },
     "execution_count": 485,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data = pd.read_csv(\"./data/Test_process.csv\")\n",
    "# test_data = pd.read_csv(\"./data/Test.csv\")\n",
    "\n",
    "test_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 486,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'how to overcome bad feelings and emotions'"
      ]
     },
     "execution_count": 486,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.iloc[0]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 487,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "309"
      ]
     },
     "execution_count": 487,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference\n",
    "\n",
    "# Load Best Model\n",
    "# model.load_state_dict(torch.load('Best_Model_Bert_0.23.pt'))\n",
    "\n",
    "y_pred = np.zeros((309, 4))\n",
    "for i in range(len(test_data)):\n",
    "    y_pred[i] = predict_sentiment(model, test_data.iloc[i]['text']).data.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 489,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(309, 4)"
      ]
     },
     "execution_count": 489,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: optuna in /usr/local/lib/python3.6/dist-packages (1.5.0)\n",
      "Requirement already satisfied: sqlalchemy>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from optuna) (1.3.17)\n",
      "Requirement already satisfied: cliff in /usr/local/lib/python3.6/dist-packages (from optuna) (3.3.0)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from optuna) (4.41.1)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from optuna) (0.15.1)\n",
      "Requirement already satisfied: alembic in /usr/local/lib/python3.6/dist-packages (from optuna) (1.4.2)\n",
      "Requirement already satisfied: cmaes>=0.5.0 in /usr/local/lib/python3.6/dist-packages (from optuna) (0.5.1)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from optuna) (1.19.0)\n",
      "Requirement already satisfied: colorlog in /usr/local/lib/python3.6/dist-packages (from optuna) (4.1.0)\n",
      "Requirement already satisfied: scipy!=1.4.0 in /usr/local/lib/python3.6/dist-packages (from optuna) (1.4.1)\n",
      "Requirement already satisfied: pyparsing>=2.1.0 in /usr/local/lib/python3.6/dist-packages (from cliff->optuna) (2.4.7)\n",
      "Requirement already satisfied: cmd2!=0.8.3,>=0.8.0 in /usr/local/lib/python3.6/dist-packages (from cliff->optuna) (1.1.0)\n",
      "Requirement already satisfied: pbr!=2.1.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from cliff->optuna) (5.4.5)\n",
      "Requirement already satisfied: stevedore>=1.20.0 in /usr/local/lib/python3.6/dist-packages (from cliff->optuna) (2.0.1)\n",
      "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from cliff->optuna) (1.15.0)\n",
      "Requirement already satisfied: PyYAML>=3.12 in /usr/local/lib/python3.6/dist-packages (from cliff->optuna) (3.13)\n",
      "Requirement already satisfied: PrettyTable<0.8,>=0.7.2 in /usr/local/lib/python3.6/dist-packages (from cliff->optuna) (0.7.2)\n",
      "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.6/dist-packages (from alembic->optuna) (2.8.1)\n",
      "Requirement already satisfied: python-editor>=0.3 in /usr/local/lib/python3.6/dist-packages (from alembic->optuna) (1.0.4)\n",
      "Requirement already satisfied: Mako in /usr/local/lib/python3.6/dist-packages (from alembic->optuna) (1.1.3)\n",
      "Requirement already satisfied: setuptools>=34.4 in /usr/local/lib/python3.6/dist-packages (from cmd2!=0.8.3,>=0.8.0->cliff->optuna) (47.3.1)\n",
      "Requirement already satisfied: pyperclip>=1.6 in /usr/local/lib/python3.6/dist-packages (from cmd2!=0.8.3,>=0.8.0->cliff->optuna) (1.8.0)\n",
      "Requirement already satisfied: wcwidth>=0.1.7 in /usr/local/lib/python3.6/dist-packages (from cmd2!=0.8.3,>=0.8.0->cliff->optuna) (0.2.5)\n",
      "Requirement already satisfied: attrs>=16.3.0 in /usr/local/lib/python3.6/dist-packages (from cmd2!=0.8.3,>=0.8.0->cliff->optuna) (19.3.0)\n",
      "Requirement already satisfied: colorama>=0.3.7 in /usr/local/lib/python3.6/dist-packages (from cmd2!=0.8.3,>=0.8.0->cliff->optuna) (0.4.3)\n",
      "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.6/dist-packages (from Mako->alembic->optuna) (1.1.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "class load_data(object):\n",
    "    def __init__(self, SEED=1234):\n",
    "        torch.manual_seed(SEED)\n",
    "        torch.cuda.manual_seed(SEED)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "\n",
    "        TEXT = data.Field(tokenize = 'spacy')\n",
    "        LABELS = data.LabelField(dtype = torch.float)\n",
    "\n",
    "\n",
    "#         self.train_data, self.test_data = datasets.IMDB.splits(TEXT, LABEL)\n",
    "        \n",
    "        self.train_data, self.test_data = data.TabularDataset.splits(\n",
    "            path='./data', train='Train_process.csv',\n",
    "            test='Test_process.csv', format='csv', skip_header=True,\n",
    "            fields=[('ID', None),\n",
    "                    ('text', TEXT),\n",
    "                    ('label', LABELS)])\n",
    "\n",
    "        self.SEED = SEED\n",
    "\n",
    "\n",
    "    def get_fold_data(self, num_folds=10):\n",
    "        \"\"\"\n",
    "        More details about 'fields' are available at \n",
    "        https://github.com/pytorch/text/blob/master/torchtext/datasets/imdb.py\n",
    "        \"\"\"\n",
    "\n",
    "        TEXT = data.Field(tokenize='spacy')\n",
    "        LABELS = data.LabelField(dtype=torch.float)\n",
    "        fields = [('text', TEXT), ('label', LABELS)]\n",
    "        \n",
    "#         kf = KFold(n_splits=num_folds, shuffle=True, random_state=self.SEED)\n",
    "        kf = StratifiedKFold(n_splits=num_folds, random_state=self.SEED)\n",
    "        \n",
    "        train_data_arr = np.array(self.train_data.examples)\n",
    "\n",
    "#         ipdb.set_trace()\n",
    "        for train_index, val_index in kf.split(train_data_arr, y_kfold):\n",
    "            yield(\n",
    "                TEXT,\n",
    "                LABELS,\n",
    "                data.Dataset(train_data_arr[train_index], fields=fields),\n",
    "                data.Dataset(train_data_arr[val_index], fields=fields),\n",
    "            )\n",
    "    \n",
    "    def get_test_data(self):\n",
    "        return self.test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best_loss_val = 0.8\n",
    "\n",
    "def objective(trial):\n",
    "    \n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')    \n",
    "    \n",
    "    \n",
    "    lr  = trial.suggest_loguniform('lr', 1e-4, 1e-2)\n",
    "#     optim_ = trial.suggest_categorical('optim_',[optim.SGD, optim.RMSprop,optim.Adam])\n",
    "    \n",
    "    \n",
    "    momentum = trial.suggest_uniform('momentum', 0.5, 0.9)\n",
    "    \n",
    "    \n",
    "    \n",
    "    data_generator = load_data()\n",
    "    _history = []\n",
    "    device = None\n",
    "    model = None\n",
    "    criterion = None\n",
    "    fold_index = 0\n",
    "    num_folds = 5\n",
    "    batch_size = 32\n",
    "    epochs = 20\n",
    "\n",
    "    import torch.optim as optim\n",
    "\n",
    "    # optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "    best_valid_acc = 0.9\n",
    "    best_val_loss = 0.2\n",
    "\n",
    "    for TEXT, LABELS, train_data, val_data in data_generator.get_fold_data(num_folds=num_folds):\n",
    "#         print(\"***** Running Training *****\")\n",
    "#         print(f\"Now fold: {fold_index + 1} / {num_folds}\")\n",
    "\n",
    "        TEXT.build_vocab(train_data,\n",
    "                         vectors = \"glove.6B.300d\",\n",
    "                         unk_init = torch.Tensor.normal_)\n",
    "\n",
    "#         print(f'Embedding size: {TEXT.vocab.vectors.size()}.')\n",
    "        LABELS.build_vocab(train_data) # For converting str into float labels.\n",
    "\n",
    "    #     Model(len(TEXT.vocab), embedding_dim, hidden_dim,\n",
    "    #         output_dim, num_layers, dropout, TEXT.vocab.vectors, embedding_trainable)\n",
    "\n",
    "        INPUT_DIM = len(TEXT.vocab)\n",
    "        EMBEDDING_DIM = 300\n",
    "        N_FILTERS = 100\n",
    "        FILTER_SIZES = [2,3,4]\n",
    "        OUTPUT_DIM = len(LABELS.vocab)\n",
    "        DROPOUT = 0.5\n",
    "        PAD_IDX = TEXT.vocab.stoi[TEXT.pad_token]\n",
    "\n",
    "        model = CNN(INPUT_DIM, EMBEDDING_DIM, N_FILTERS, FILTER_SIZES, OUTPUT_DIM, DROPOUT, PAD_IDX)\n",
    "\n",
    "        pretrained_embeddings = TEXT.vocab.vectors\n",
    "\n",
    "        model.embedding.weight.data.copy_(pretrained_embeddings)\n",
    "\n",
    "        UNK_IDX = TEXT.vocab.stoi[TEXT.unk_token]\n",
    "\n",
    "        model.embedding.weight.data[UNK_IDX] = torch.zeros(EMBEDDING_DIM)\n",
    "        model.embedding.weight.data[PAD_IDX] = torch.zeros(EMBEDDING_DIM)\n",
    "\n",
    "\n",
    "        # Get w\n",
    "        from collections import Counter\n",
    "        class_distrbution = Counter()\n",
    "        for text in y_kfold:\n",
    "            class_distrbution[text] += 1\n",
    "\n",
    "\n",
    "        class_weights = [class_distrbution[i] for i in LABELS.vocab.stoi.keys()]\n",
    "        class_weights_normalized = [max(class_weights)/i for i in class_weights]\n",
    "\n",
    "\n",
    "        w = torch.Tensor(class_weights_normalized)\n",
    "        w = w.to(device)\n",
    "\n",
    "#         optimizer = optim.Adam(model.parameters())\n",
    "#         criterion = nn.CrossEntropyLoss(weight=w)\n",
    "        \n",
    "        optimizer = optim.SGD(model.parameters(), lr = lr, momentum=momentum, \n",
    "                      weight_decay=0, dampening=0, nesterov=False)\n",
    "        \n",
    "#         optimizer = optim.Adam(model.parameters(), lr = 1e-4, momentum=0.9, \n",
    "#                       weight_decay=0, dampening=0, nesterov=True)\n",
    "    \n",
    "        criterion = nn.CrossEntropyLoss(weight = w)\n",
    "        \n",
    "        \n",
    "\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        model = model.to(device)\n",
    "        criterion = criterion.to(device)\n",
    "\n",
    "        train_iterator = data.Iterator(train_data, batch_size=batch_size, sort_key=lambda x: len(x.text), device=device)\n",
    "        val_iterator = data.Iterator(val_data, batch_size=batch_size, sort_key=lambda x: len(x.text), device=device)\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            train_loss, train_acc = train(model, train_iterator, optimizer, criterion)\n",
    "            val_loss, val_acc = evaluate(model, val_iterator, criterion)\n",
    "#             print(f'| Epoch: {epoch+1:02} | Train Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
    "#             print(f'| Epoch: {epoch+1:02} | Valid Loss: {val_loss:.3f} | Valid Acc: {val_acc*100:.2f}%')\n",
    "#             print('################################################################################')\n",
    "#             if val_loss < best_val_loss:\n",
    "#                 best_val_loss = val_loss\n",
    "#                 print('Saving Model ...')\n",
    "#                 torch.save(model.state_dict(), 'Best_Model_'+str(best_val_loss)+'.pt')\n",
    "#                 print('*****************************************************')\n",
    "#                 print('best record: [epoch %d], [val loss %.5f], [val acc %.5f]' % (epoch, val_loss, val_acc))\n",
    "#                 print('*****************************************************')\n",
    "\n",
    "        val_loss, val_acc = evaluate(model, val_iterator, criterion) \n",
    "\n",
    "#         if val_loss < best_val_loss:\n",
    "#                 best_val_loss = val_loss\n",
    "#                 print('Saving Model ...')\n",
    "#                 torch.save(model.state_dict(), 'Best_Model_'+str(best_val_loss)+'.pt')\n",
    "#                 print('*****************************************************')\n",
    "#                 print('best record: [epoch %d], [val loss %.5f], [val acc %.5f]' % (epoch, val_loss, val_acc))\n",
    "#                 print('*****************************************************')\n",
    "\n",
    "    #     if val_acc > best_valid_acc:\n",
    "    #         best_valid_acc = val_acc\n",
    "    #         print('Saving Model ...')\n",
    "    #         torch.save(model.state_dict(), 'Best_Model_'+str(val_acc)+'.pt')\n",
    "    #         print('*****************************************************')\n",
    "    #         print('best record: [epoch %d], [val loss %.5f], [val acc %.5f]' % (epoch, val_loss, val_acc))\n",
    "    #         print('*****************************************************')\n",
    "\n",
    "#         print(f'Val. Loss: {val_loss:.3f} | Val. Acc: {val_acc*100:.2f}% |')\n",
    "\n",
    "        _history.append([val_loss, val_acc])\n",
    "        fold_index += 1\n",
    "\n",
    "    _history = np.asarray(_history)\n",
    "    loss = np.mean(_history[:, 0])\n",
    "    acc = np.mean(_history[:, 1])\n",
    "\n",
    "    print('***** Cross Validation Result *****')\n",
    "    print(f'LOSS: {loss}, ACC: {acc}')\n",
    "    \n",
    "    # Handle pruning based on the intermediate value.\n",
    "    if trial.should_prune():\n",
    "        raise optuna.exceptions.TrialPruned()\n",
    "            \n",
    "            \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py:296: FutureWarning:\n",
      "\n",
      "Setting a random_state has no effect since shuffle is False. This will raise an error in 0.24. You should leave random_state to its default (None), or set shuffle=True.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** Cross Validation Result *****\n",
      "LOSS: 1.327958458662033, ACC: 0.6552496731281281\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2020-07-03 20:37:05,156] Finished trial#0 with value: 1.327958458662033 with parameters: {'lr': 0.0003017745339013328, 'momentum': 0.6526010976015626}. Best is trial#0 with value: 1.327958458662033.\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py:296: FutureWarning:\n",
      "\n",
      "Setting a random_state has no effect since shuffle is False. This will raise an error in 0.24. You should leave random_state to its default (None), or set shuffle=True.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** Cross Validation Result *****\n",
      "LOSS: 0.9759525686502457, ACC: 0.8164930582046509\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2020-07-03 20:37:57,548] Finished trial#1 with value: 0.9759525686502457 with parameters: {'lr': 0.0010456463513600518, 'momentum': 0.822185665013464}. Best is trial#1 with value: 0.9759525686502457.\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py:296: FutureWarning:\n",
      "\n",
      "Setting a random_state has no effect since shuffle is False. This will raise an error in 0.24. You should leave random_state to its default (None), or set shuffle=True.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** Cross Validation Result *****\n",
      "LOSS: 1.2384560048580169, ACC: 0.7838376343250275\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2020-07-03 20:38:29,012] Finished trial#2 with value: 1.2384560048580169 with parameters: {'lr': 0.0010636778719478645, 'momentum': 0.5259356573424152}. Best is trial#1 with value: 0.9759525686502457.\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py:296: FutureWarning:\n",
      "\n",
      "Setting a random_state has no effect since shuffle is False. This will raise an error in 0.24. You should leave random_state to its default (None), or set shuffle=True.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** Cross Validation Result *****\n",
      "LOSS: 0.9574449002742768, ACC: 0.8234788358211518\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2020-07-03 20:39:03,721] Finished trial#3 with value: 0.9574449002742768 with parameters: {'lr': 0.002299344228500365, 'momentum': 0.6203303100320009}. Best is trial#3 with value: 0.9574449002742768.\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py:296: FutureWarning:\n",
      "\n",
      "Setting a random_state has no effect since shuffle is False. This will raise an error in 0.24. You should leave random_state to its default (None), or set shuffle=True.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** Cross Validation Result *****\n",
      "LOSS: 1.343417513370514, ACC: 0.5591435208916664\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2020-07-03 20:39:46,386] Finished trial#4 with value: 1.343417513370514 with parameters: {'lr': 0.0003074002482770984, 'momentum': 0.5102302083946814}. Best is trial#3 with value: 0.9574449002742768.\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py:296: FutureWarning:\n",
      "\n",
      "Setting a random_state has no effect since shuffle is False. This will raise an error in 0.24. You should leave random_state to its default (None), or set shuffle=True.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** Cross Validation Result *****\n",
      "LOSS: 0.5771822988986969, ACC: 0.8426752686500549\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2020-07-03 20:40:26,041] Finished trial#5 with value: 0.5771822988986969 with parameters: {'lr': 0.006813470144576557, 'momentum': 0.6136871553855281}. Best is trial#5 with value: 0.5771822988986969.\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py:296: FutureWarning:\n",
      "\n",
      "Setting a random_state has no effect since shuffle is False. This will raise an error in 0.24. You should leave random_state to its default (None), or set shuffle=True.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** Cross Validation Result *****\n",
      "LOSS: 1.254415547847748, ACC: 0.7763144910335541\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2020-07-03 20:41:09,892] Finished trial#6 with value: 1.254415547847748 with parameters: {'lr': 0.0008173743804225808, 'momentum': 0.5931760006045274}. Best is trial#5 with value: 0.5771822988986969.\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py:296: FutureWarning:\n",
      "\n",
      "Setting a random_state has no effect since shuffle is False. This will raise an error in 0.24. You should leave random_state to its default (None), or set shuffle=True.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** Cross Validation Result *****\n",
      "LOSS: 0.8541928082704544, ACC: 0.8344824761152267\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2020-07-03 20:42:01,228] Finished trial#7 with value: 0.8541928082704544 with parameters: {'lr': 0.0027036542125089534, 'momentum': 0.6481203743133906}. Best is trial#5 with value: 0.5771822988986969.\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py:296: FutureWarning:\n",
      "\n",
      "Setting a random_state has no effect since shuffle is False. This will raise an error in 0.24. You should leave random_state to its default (None), or set shuffle=True.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** Cross Validation Result *****\n",
      "LOSS: 0.7429178118705749, ACC: 0.8310681253671646\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2020-07-03 20:42:51,933] Finished trial#8 with value: 0.7429178118705749 with parameters: {'lr': 0.0031461854417959936, 'momentum': 0.6878953743839797}. Best is trial#5 with value: 0.5771822988986969.\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py:296: FutureWarning:\n",
      "\n",
      "Setting a random_state has no effect since shuffle is False. This will raise an error in 0.24. You should leave random_state to its default (None), or set shuffle=True.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** Cross Validation Result *****\n",
      "LOSS: 1.3194498419761658, ACC: 0.6813409417867661\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2020-07-03 20:43:40,976] Finished trial#9 with value: 1.3194498419761658 with parameters: {'lr': 0.00026471941458783835, 'momentum': 0.7366998336728974}. Best is trial#5 with value: 0.5771822988986969.\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py:296: FutureWarning:\n",
      "\n",
      "Setting a random_state has no effect since shuffle is False. This will raise an error in 0.24. You should leave random_state to its default (None), or set shuffle=True.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** Cross Validation Result *****\n",
      "LOSS: 0.5657602921128273, ACC: 0.8691220283508301\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2020-07-03 20:44:32,934] Finished trial#10 with value: 0.5657602921128273 with parameters: {'lr': 0.009556410480067438, 'momentum': 0.898475684579351}. Best is trial#10 with value: 0.5657602921128273.\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py:296: FutureWarning:\n",
      "\n",
      "Setting a random_state has no effect since shuffle is False. This will raise an error in 0.24. You should leave random_state to its default (None), or set shuffle=True.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** Cross Validation Result *****\n",
      "LOSS: 0.5728199563920497, ACC: 0.864145177602768\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2020-07-03 20:45:22,624] Finished trial#11 with value: 0.5728199563920497 with parameters: {'lr': 0.009862062666237929, 'momentum': 0.8993882852502081}. Best is trial#10 with value: 0.5657602921128273.\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py:296: FutureWarning:\n",
      "\n",
      "Setting a random_state has no effect since shuffle is False. This will raise an error in 0.24. You should leave random_state to its default (None), or set shuffle=True.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** Cross Validation Result *****\n",
      "LOSS: 0.5629334472119808, ACC: 0.864145177602768\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2020-07-03 20:46:14,359] Finished trial#12 with value: 0.5629334472119808 with parameters: {'lr': 0.009441720893355788, 'momentum': 0.8981196270204903}. Best is trial#12 with value: 0.5629334472119808.\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py:296: FutureWarning:\n",
      "\n",
      "Setting a random_state has no effect since shuffle is False. This will raise an error in 0.24. You should leave random_state to its default (None), or set shuffle=True.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** Cross Validation Result *****\n",
      "LOSS: 0.5075690373778343, ACC: 0.8529844641685486\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2020-07-03 20:47:03,633] Finished trial#13 with value: 0.5075690373778343 with parameters: {'lr': 0.006284692590279114, 'momentum': 0.8893101866289932}. Best is trial#13 with value: 0.5075690373778343.\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py:296: FutureWarning:\n",
      "\n",
      "Setting a random_state has no effect since shuffle is False. This will raise an error in 0.24. You should leave random_state to its default (None), or set shuffle=True.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** Cross Validation Result *****\n",
      "LOSS: 0.5104975253343582, ACC: 0.8448164731264114\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2020-07-03 20:47:49,734] Finished trial#14 with value: 0.5104975253343582 with parameters: {'lr': 0.005063145350314804, 'momentum': 0.8197161960858727}. Best is trial#13 with value: 0.5075690373778343.\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py:296: FutureWarning:\n",
      "\n",
      "Setting a random_state has no effect since shuffle is False. This will raise an error in 0.24. You should leave random_state to its default (None), or set shuffle=True.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** Cross Validation Result *****\n",
      "LOSS: 0.517751906812191, ACC: 0.8432539731264115\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2020-07-03 20:48:19,402] Finished trial#15 with value: 0.517751906812191 with parameters: {'lr': 0.004754352005186824, 'momentum': 0.8166970100187024}. Best is trial#13 with value: 0.5075690373778343.\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py:296: FutureWarning:\n",
      "\n",
      "Setting a random_state has no effect since shuffle is False. This will raise an error in 0.24. You should leave random_state to its default (None), or set shuffle=True.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** Cross Validation Result *****\n",
      "LOSS: 0.5097382679581642, ACC: 0.8432539731264115\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2020-07-03 20:48:49,102] Finished trial#16 with value: 0.5097382679581642 with parameters: {'lr': 0.004610301102350988, 'momentum': 0.8360673015569071}. Best is trial#13 with value: 0.5075690373778343.\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py:296: FutureWarning:\n",
      "\n",
      "Setting a random_state has no effect since shuffle is False. This will raise an error in 0.24. You should leave random_state to its default (None), or set shuffle=True.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** Cross Validation Result *****\n",
      "LOSS: 0.8418743252754212, ACC: 0.8313574761152267\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2020-07-03 20:49:18,860] Finished trial#17 with value: 0.8418743252754212 with parameters: {'lr': 0.0018537581543728256, 'momentum': 0.7670527672049179}. Best is trial#13 with value: 0.5075690373778343.\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py:296: FutureWarning:\n",
      "\n",
      "Setting a random_state has no effect since shuffle is False. This will raise an error in 0.24. You should leave random_state to its default (None), or set shuffle=True.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** Cross Validation Result *****\n",
      "LOSS: 0.498178668320179, ACC: 0.8529183268547058\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2020-07-03 20:49:48,570] Finished trial#18 with value: 0.498178668320179 with parameters: {'lr': 0.004812406405444112, 'momentum': 0.8583275347264456}. Best is trial#18 with value: 0.498178668320179.\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py:296: FutureWarning:\n",
      "\n",
      "Setting a random_state has no effect since shuffle is False. This will raise an error in 0.24. You should leave random_state to its default (None), or set shuffle=True.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** Cross Validation Result *****\n",
      "LOSS: 1.331704217195511, ACC: 0.6256779134273529\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2020-07-03 20:50:18,324] Finished trial#19 with value: 1.331704217195511 with parameters: {'lr': 0.00010663868987473341, 'momentum': 0.8704580913416442}. Best is trial#18 with value: 0.498178668320179.\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py:296: FutureWarning:\n",
      "\n",
      "Setting a random_state has no effect since shuffle is False. This will raise an error in 0.24. You should leave random_state to its default (None), or set shuffle=True.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** Cross Validation Result *****\n",
      "LOSS: 0.8373440325260162, ACC: 0.8313574761152267\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2020-07-03 20:50:47,979] Finished trial#20 with value: 0.8373440325260162 with parameters: {'lr': 0.0018793991785664612, 'momentum': 0.7663002701292841}. Best is trial#18 with value: 0.498178668320179.\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py:296: FutureWarning:\n",
      "\n",
      "Setting a random_state has no effect since shuffle is False. This will raise an error in 0.24. You should leave random_state to its default (None), or set shuffle=True.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** Cross Validation Result *****\n",
      "LOSS: 0.5073713183403015, ACC: 0.8463789731264114\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2020-07-03 20:51:17,758] Finished trial#21 with value: 0.5073713183403015 with parameters: {'lr': 0.004427093288335344, 'momentum': 0.8455298795520827}. Best is trial#18 with value: 0.498178668320179.\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py:296: FutureWarning:\n",
      "\n",
      "Setting a random_state has no effect since shuffle is False. This will raise an error in 0.24. You should leave random_state to its default (None), or set shuffle=True.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** Cross Validation Result *****\n",
      "LOSS: 0.5028461828827858, ACC: 0.8578951776027679\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2020-07-03 20:51:47,429] Finished trial#22 with value: 0.5028461828827858 with parameters: {'lr': 0.006704858222478878, 'momentum': 0.8598288069408165}. Best is trial#18 with value: 0.498178668320179.\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py:296: FutureWarning:\n",
      "\n",
      "Setting a random_state has no effect since shuffle is False. This will raise an error in 0.24. You should leave random_state to its default (None), or set shuffle=True.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** Cross Validation Result *****\n",
      "LOSS: 0.5023185387253761, ACC: 0.8479414731264114\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2020-07-03 20:52:17,086] Finished trial#23 with value: 0.5023185387253761 with parameters: {'lr': 0.00427960425242461, 'momentum': 0.8590210374247593}. Best is trial#18 with value: 0.498178668320179.\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py:296: FutureWarning:\n",
      "\n",
      "Setting a random_state has no effect since shuffle is False. This will raise an error in 0.24. You should leave random_state to its default (None), or set shuffle=True.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** Cross Validation Result *****\n",
      "LOSS: 0.6010732769966125, ACC: 0.8349289059638977\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2020-07-03 20:52:46,841] Finished trial#24 with value: 0.6010732769966125 with parameters: {'lr': 0.003489408070340888, 'momentum': 0.7795236638473473}. Best is trial#18 with value: 0.498178668320179.\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py:296: FutureWarning:\n",
      "\n",
      "Setting a random_state has no effect since shuffle is False. This will raise an error in 0.24. You should leave random_state to its default (None), or set shuffle=True.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** Cross Validation Result *****\n",
      "LOSS: 0.5051060721278191, ACC: 0.85818452835083\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2020-07-03 20:53:16,596] Finished trial#25 with value: 0.5051060721278191 with parameters: {'lr': 0.007012997459985982, 'momentum': 0.8586563719296875}. Best is trial#18 with value: 0.498178668320179.\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py:296: FutureWarning:\n",
      "\n",
      "Setting a random_state has no effect since shuffle is False. This will raise an error in 0.24. You should leave random_state to its default (None), or set shuffle=True.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** Cross Validation Result *****\n",
      "LOSS: 0.9403471916913986, ACC: 0.8214037716388702\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2020-07-03 20:53:46,334] Finished trial#26 with value: 0.9403471916913986 with parameters: {'lr': 0.0013035748335349932, 'momentum': 0.7954272348274283}. Best is trial#18 with value: 0.498178668320179.\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py:296: FutureWarning:\n",
      "\n",
      "Setting a random_state has no effect since shuffle is False. This will raise an error in 0.24. You should leave random_state to its default (None), or set shuffle=True.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** Cross Validation Result *****\n",
      "LOSS: 0.5110106810927391, ACC: 0.8458002716302871\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2020-07-03 20:54:16,017] Finished trial#27 with value: 0.5110106810927391 with parameters: {'lr': 0.0077133022346861525, 'momentum': 0.7326134020687094}. Best is trial#18 with value: 0.498178668320179.\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py:296: FutureWarning:\n",
      "\n",
      "Setting a random_state has no effect since shuffle is False. This will raise an error in 0.24. You should leave random_state to its default (None), or set shuffle=True.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** Cross Validation Result *****\n",
      "LOSS: 1.1124606639146806, ACC: 0.8027199119329452\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2020-07-03 20:54:46,044] Finished trial#28 with value: 1.1124606639146806 with parameters: {'lr': 0.0005250800672394833, 'momentum': 0.8713776467968388}. Best is trial#18 with value: 0.498178668320179.\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py:296: FutureWarning:\n",
      "\n",
      "Setting a random_state has no effect since shuffle is False. This will raise an error in 0.24. You should leave random_state to its default (None), or set shuffle=True.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** Cross Validation Result *****\n",
      "LOSS: 0.5659467592835427, ACC: 0.8364914059638977\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2020-07-03 20:55:15,881] Finished trial#29 with value: 0.5659467592835427 with parameters: {'lr': 0.00372528128488455, 'momentum': 0.7996418417067669}. Best is trial#18 with value: 0.498178668320179.\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py:296: FutureWarning:\n",
      "\n",
      "Setting a random_state has no effect since shuffle is False. This will raise an error in 0.24. You should leave random_state to its default (None), or set shuffle=True.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** Cross Validation Result *****\n",
      "LOSS: 0.9158313989639282, ACC: 0.8245287716388703\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2020-07-03 20:55:46,089] Finished trial#30 with value: 0.9158313989639282 with parameters: {'lr': 0.0018748628447404743, 'momentum': 0.7200422405594902}. Best is trial#18 with value: 0.498178668320179.\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py:296: FutureWarning:\n",
      "\n",
      "Setting a random_state has no effect since shuffle is False. This will raise an error in 0.24. You should leave random_state to its default (None), or set shuffle=True.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** Cross Validation Result *****\n",
      "LOSS: 0.5006795361638069, ACC: 0.856332677602768\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2020-07-03 20:56:15,855] Finished trial#31 with value: 0.5006795361638069 with parameters: {'lr': 0.006338951549525465, 'momentum': 0.8605992823796142}. Best is trial#18 with value: 0.498178668320179.\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py:296: FutureWarning:\n",
      "\n",
      "Setting a random_state has no effect since shuffle is False. This will raise an error in 0.24. You should leave random_state to its default (None), or set shuffle=True.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** Cross Validation Result *****\n",
      "LOSS: 0.4973932966589928, ACC: 0.8532076776027679\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2020-07-03 20:56:46,126] Finished trial#32 with value: 0.4973932966589928 with parameters: {'lr': 0.005638888881777268, 'momentum': 0.8497150508132949}. Best is trial#32 with value: 0.4973932966589928.\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py:296: FutureWarning:\n",
      "\n",
      "Setting a random_state has no effect since shuffle is False. This will raise an error in 0.24. You should leave random_state to its default (None), or set shuffle=True.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** Cross Validation Result *****\n",
      "LOSS: 0.5013158261775971, ACC: 0.8476521223783493\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2020-07-03 20:57:16,373] Finished trial#33 with value: 0.5013158261775971 with parameters: {'lr': 0.005449740712645151, 'momentum': 0.8344842118093602}. Best is trial#32 with value: 0.4973932966589928.\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py:296: FutureWarning:\n",
      "\n",
      "Setting a random_state has no effect since shuffle is False. This will raise an error in 0.24. You should leave random_state to its default (None), or set shuffle=True.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** Cross Validation Result *****\n",
      "LOSS: 0.6036181226372719, ACC: 0.8349950432777404\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2020-07-03 20:57:47,109] Finished trial#34 with value: 0.6036181226372719 with parameters: {'lr': 0.0026452946118709093, 'momentum': 0.8315738301457176}. Best is trial#32 with value: 0.4973932966589928.\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py:296: FutureWarning:\n",
      "\n",
      "Setting a random_state has no effect since shuffle is False. This will raise an error in 0.24. You should leave random_state to its default (None), or set shuffle=True.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** Cross Validation Result *****\n",
      "LOSS: 0.5009583294391632, ACC: 0.8561094641685486\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2020-07-03 20:58:17,888] Finished trial#35 with value: 0.5009583294391632 with parameters: {'lr': 0.00587480554121094, 'momentum': 0.8828883535034651}. Best is trial#32 with value: 0.4973932966589928.\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py:296: FutureWarning:\n",
      "\n",
      "Setting a random_state has no effect since shuffle is False. This will raise an error in 0.24. You should leave random_state to its default (None), or set shuffle=True.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** Cross Validation Result *****\n",
      "LOSS: 0.5377031370997429, ACC: 0.8607308268547058\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2020-07-03 20:58:48,722] Finished trial#36 with value: 0.5377031370997429 with parameters: {'lr': 0.008926924490083754, 'momentum': 0.8809399494092747}. Best is trial#32 with value: 0.4973932966589928.\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py:296: FutureWarning:\n",
      "\n",
      "Setting a random_state has no effect since shuffle is False. This will raise an error in 0.24. You should leave random_state to its default (None), or set shuffle=True.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** Cross Validation Result *****\n",
      "LOSS: 0.5071277871727944, ACC: 0.8460896223783493\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2020-07-03 20:59:18,906] Finished trial#37 with value: 0.5071277871727944 with parameters: {'lr': 0.00612479457389616, 'momentum': 0.7988145819652185}. Best is trial#32 with value: 0.4973932966589928.\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py:296: FutureWarning:\n",
      "\n",
      "Setting a random_state has no effect since shuffle is False. This will raise an error in 0.24. You should leave random_state to its default (None), or set shuffle=True.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** Cross Validation Result *****\n",
      "LOSS: 0.8984489351511001, ACC: 0.8328538358211517\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2020-07-03 20:59:48,709] Finished trial#38 with value: 0.8984489351511001 with parameters: {'lr': 0.003075573880291357, 'momentum': 0.5558547824370474}. Best is trial#32 with value: 0.4973932966589928.\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py:296: FutureWarning:\n",
      "\n",
      "Setting a random_state has no effect since shuffle is False. This will raise an error in 0.24. You should leave random_state to its default (None), or set shuffle=True.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** Cross Validation Result *****\n",
      "LOSS: 0.6978113040328026, ACC: 0.8282324761152268\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2020-07-03 21:00:18,415] Finished trial#39 with value: 0.6978113040328026 with parameters: {'lr': 0.0013713014341580375, 'momentum': 0.8821073846862513}. Best is trial#32 with value: 0.4973932966589928.\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py:296: FutureWarning:\n",
      "\n",
      "Setting a random_state has no effect since shuffle is False. This will raise an error in 0.24. You should leave random_state to its default (None), or set shuffle=True.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** Cross Validation Result *****\n",
      "LOSS: 0.501986576616764, ACC: 0.856332677602768\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2020-07-03 21:00:48,280] Finished trial#40 with value: 0.501986576616764 with parameters: {'lr': 0.00806591542228223, 'momentum': 0.8141330145525059}. Best is trial#32 with value: 0.4973932966589928.\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py:296: FutureWarning:\n",
      "\n",
      "Setting a random_state has no effect since shuffle is False. This will raise an error in 0.24. You should leave random_state to its default (None), or set shuffle=True.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** Cross Validation Result *****\n",
      "LOSS: 0.49940393716096876, ACC: 0.8510664761066437\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2020-07-03 21:01:17,928] Finished trial#41 with value: 0.49940393716096876 with parameters: {'lr': 0.005389076195181052, 'momentum': 0.8468873018957996}. Best is trial#32 with value: 0.4973932966589928.\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py:296: FutureWarning:\n",
      "\n",
      "Setting a random_state has no effect since shuffle is False. This will raise an error in 0.24. You should leave random_state to its default (None), or set shuffle=True.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** Cross Validation Result *****\n",
      "LOSS: 0.49941366314888, ACC: 0.8529183268547058\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2020-07-03 21:01:47,623] Finished trial#42 with value: 0.49941366314888 with parameters: {'lr': 0.005556703602478259, 'momentum': 0.8475175336328599}. Best is trial#32 with value: 0.4973932966589928.\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py:296: FutureWarning:\n",
      "\n",
      "Setting a random_state has no effect since shuffle is False. This will raise an error in 0.24. You should leave random_state to its default (None), or set shuffle=True.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** Cross Validation Result *****\n",
      "LOSS: 0.5234861850738526, ACC: 0.8380539059638977\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2020-07-03 21:02:17,231] Finished trial#43 with value: 0.5234861850738526 with parameters: {'lr': 0.0036521502139770863, 'momentum': 0.8493354002445885}. Best is trial#32 with value: 0.4973932966589928.\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py:296: FutureWarning:\n",
      "\n",
      "Setting a random_state has no effect since shuffle is False. This will raise an error in 0.24. You should leave random_state to its default (None), or set shuffle=True.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** Cross Validation Result *****\n",
      "LOSS: 0.5031858935952187, ACC: 0.8578951776027679\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2020-07-03 21:02:46,877] Finished trial#44 with value: 0.5031858935952187 with parameters: {'lr': 0.00980640324782613, 'momentum': 0.7811570318721086}. Best is trial#32 with value: 0.4973932966589928.\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py:296: FutureWarning:\n",
      "\n",
      "Setting a random_state has no effect since shuffle is False. This will raise an error in 0.24. You should leave random_state to its default (None), or set shuffle=True.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** Cross Validation Result *****\n",
      "LOSS: 0.5857230976223946, ACC: 0.8349950432777404\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2020-07-03 21:03:16,594] Finished trial#45 with value: 0.5857230976223946 with parameters: {'lr': 0.0026184862847948927, 'momentum': 0.845233670313885}. Best is trial#32 with value: 0.4973932966589928.\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py:296: FutureWarning:\n",
      "\n",
      "Setting a random_state has no effect since shuffle is False. This will raise an error in 0.24. You should leave random_state to its default (None), or set shuffle=True.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** Cross Validation Result *****\n",
      "LOSS: 0.6648435816168785, ACC: 0.8330770552158355\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2020-07-03 21:03:46,290] Finished trial#46 with value: 0.6648435816168785 with parameters: {'lr': 0.00406716924320123, 'momentum': 0.6771108282689534}. Best is trial#32 with value: 0.4973932966589928.\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py:296: FutureWarning:\n",
      "\n",
      "Setting a random_state has no effect since shuffle is False. This will raise an error in 0.24. You should leave random_state to its default (None), or set shuffle=True.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** Cross Validation Result *****\n",
      "LOSS: 0.5096331372857094, ACC: 0.8479414731264114\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2020-07-03 21:04:24,018] Finished trial#47 with value: 0.5096331372857094 with parameters: {'lr': 0.005373769076267617, 'momentum': 0.8102574148667122}. Best is trial#32 with value: 0.4973932966589928.\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py:296: FutureWarning:\n",
      "\n",
      "Setting a random_state has no effect since shuffle is False. This will raise an error in 0.24. You should leave random_state to its default (None), or set shuffle=True.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** Cross Validation Result *****\n",
      "LOSS: 0.5055279046297073, ACC: 0.8510664731264115\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2020-07-03 21:05:06,164] Finished trial#48 with value: 0.5055279046297073 with parameters: {'lr': 0.007725464720398875, 'momentum': 0.757959353011282}. Best is trial#32 with value: 0.4973932966589928.\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py:296: FutureWarning:\n",
      "\n",
      "Setting a random_state has no effect since shuffle is False. This will raise an error in 0.24. You should leave random_state to its default (None), or set shuffle=True.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** Cross Validation Result *****\n",
      "LOSS: 0.5025134295225143, ACC: 0.8448826134204864\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2020-07-03 21:05:36,000] Finished trial#49 with value: 0.5025134295225143 with parameters: {'lr': 0.0029770168145560627, 'momentum': 0.8996217709986357}. Best is trial#32 with value: 0.4973932966589928.\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py:296: FutureWarning:\n",
      "\n",
      "Setting a random_state has no effect since shuffle is False. This will raise an error in 0.24. You should leave random_state to its default (None), or set shuffle=True.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** Cross Validation Result *****\n",
      "LOSS: 0.5133072577416897, ACC: 0.8613095283508301\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2020-07-03 21:06:05,737] Finished trial#50 with value: 0.5133072577416897 with parameters: {'lr': 0.009891185843061624, 'momentum': 0.8243339548178882}. Best is trial#32 with value: 0.4973932966589928.\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py:296: FutureWarning:\n",
      "\n",
      "Setting a random_state has no effect since shuffle is False. This will raise an error in 0.24. You should leave random_state to its default (None), or set shuffle=True.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** Cross Validation Result *****\n",
      "LOSS: 0.5001281499862671, ACC: 0.8578951776027679\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2020-07-03 21:06:35,449] Finished trial#51 with value: 0.5001281499862671 with parameters: {'lr': 0.005940307518252711, 'momentum': 0.8749812937600042}. Best is trial#32 with value: 0.4973932966589928.\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py:296: FutureWarning:\n",
      "\n",
      "Setting a random_state has no effect since shuffle is False. This will raise an error in 0.24. You should leave random_state to its default (None), or set shuffle=True.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** Cross Validation Result *****\n",
      "LOSS: 0.4962854325771332, ACC: 0.8497933268547058\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2020-07-03 21:07:05,317] Finished trial#52 with value: 0.4962854325771332 with parameters: {'lr': 0.004961516392725852, 'momentum': 0.8677498031158593}. Best is trial#52 with value: 0.4962854325771332.\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py:296: FutureWarning:\n",
      "\n",
      "Setting a random_state has no effect since shuffle is False. This will raise an error in 0.24. You should leave random_state to its default (None), or set shuffle=True.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** Cross Validation Result *****\n",
      "LOSS: 0.4956252224743366, ACC: 0.8561094641685486\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2020-07-03 21:07:34,984] Finished trial#53 with value: 0.4956252224743366 with parameters: {'lr': 0.0049311387000938005, 'momentum': 0.8949810015323556}. Best is trial#53 with value: 0.4956252224743366.\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py:296: FutureWarning:\n",
      "\n",
      "Setting a random_state has no effect since shuffle is False. This will raise an error in 0.24. You should leave random_state to its default (None), or set shuffle=True.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** Cross Validation Result *****\n",
      "LOSS: 0.4926346495747566, ACC: 0.8480076134204865\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2020-07-03 21:08:04,712] Finished trial#54 with value: 0.4926346495747566 with parameters: {'lr': 0.004144441719651159, 'momentum': 0.8985078355329079}. Best is trial#54 with value: 0.4926346495747566.\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py:296: FutureWarning:\n",
      "\n",
      "Setting a random_state has no effect since shuffle is False. This will raise an error in 0.24. You should leave random_state to its default (None), or set shuffle=True.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** Cross Validation Result *****\n",
      "LOSS: 0.494085419178009, ACC: 0.8563988149166107\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2020-07-03 21:08:34,402] Finished trial#55 with value: 0.494085419178009 with parameters: {'lr': 0.004385925588751196, 'momentum': 0.8999122337365262}. Best is trial#54 with value: 0.4926346495747566.\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py:296: FutureWarning:\n",
      "\n",
      "Setting a random_state has no effect since shuffle is False. This will raise an error in 0.24. You should leave random_state to its default (None), or set shuffle=True.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** Cross Validation Result *****\n",
      "LOSS: 0.5465383842587471, ACC: 0.8300181895494461\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2020-07-03 21:09:04,100] Finished trial#56 with value: 0.5465383842587471 with parameters: {'lr': 0.0020967496872158265, 'momentum': 0.8983698328229224}. Best is trial#54 with value: 0.4926346495747566.\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py:296: FutureWarning:\n",
      "\n",
      "Setting a random_state has no effect since shuffle is False. This will raise an error in 0.24. You should leave random_state to its default (None), or set shuffle=True.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** Cross Validation Result *****\n",
      "LOSS: 0.49381727874279024, ACC: 0.8576719641685486\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2020-07-03 21:09:33,828] Finished trial#57 with value: 0.49381727874279024 with parameters: {'lr': 0.00438624264644518, 'momentum': 0.8990023145864727}. Best is trial#54 with value: 0.4926346495747566.\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py:296: FutureWarning:\n",
      "\n",
      "Setting a random_state has no effect since shuffle is False. This will raise an error in 0.24. You should leave random_state to its default (None), or set shuffle=True.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** Cross Validation Result *****\n",
      "LOSS: 0.5428416922688484, ACC: 0.8284556895494462\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2020-07-03 21:10:03,516] Finished trial#58 with value: 0.5428416922688484 with parameters: {'lr': 0.00224767554914959, 'momentum': 0.8933020574803477}. Best is trial#54 with value: 0.4926346495747566.\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py:296: FutureWarning:\n",
      "\n",
      "Setting a random_state has no effect since shuffle is False. This will raise an error in 0.24. You should leave random_state to its default (None), or set shuffle=True.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** Cross Validation Result *****\n",
      "LOSS: 0.4931644096970558, ACC: 0.8495701134204865\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2020-07-03 21:10:33,192] Finished trial#59 with value: 0.4931644096970558 with parameters: {'lr': 0.003606950640278496, 'momentum': 0.8987150631418933}. Best is trial#54 with value: 0.4926346495747566.\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py:296: FutureWarning:\n",
      "\n",
      "Setting a random_state has no effect since shuffle is False. This will raise an error in 0.24. You should leave random_state to its default (None), or set shuffle=True.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** Cross Validation Result *****\n",
      "LOSS: 0.49742158204317094, ACC: 0.8464451134204865\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2020-07-03 21:11:02,940] Finished trial#60 with value: 0.49742158204317094 with parameters: {'lr': 0.0033238253680310612, 'momentum': 0.8962541508432517}. Best is trial#54 with value: 0.4926346495747566.\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py:296: FutureWarning:\n",
      "\n",
      "Setting a random_state has no effect since shuffle is False. This will raise an error in 0.24. You should leave random_state to its default (None), or set shuffle=True.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** Cross Validation Result *****\n",
      "LOSS: 0.5002163961529732, ACC: 0.8510664731264115\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2020-07-03 21:11:32,609] Finished trial#61 with value: 0.5002163961529732 with parameters: {'lr': 0.004091949155944398, 'momentum': 0.8702658044156668}. Best is trial#54 with value: 0.4926346495747566.\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py:296: FutureWarning:\n",
      "\n",
      "Setting a random_state has no effect since shuffle is False. This will raise an error in 0.24. You should leave random_state to its default (None), or set shuffle=True.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** Cross Validation Result *****\n",
      "LOSS: 0.49456333070993425, ACC: 0.8545469641685486\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2020-07-03 21:12:02,331] Finished trial#62 with value: 0.49456333070993425 with parameters: {'lr': 0.00474262609991237, 'momentum': 0.8863940479394523}. Best is trial#54 with value: 0.4926346495747566.\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py:296: FutureWarning:\n",
      "\n",
      "Setting a random_state has no effect since shuffle is False. This will raise an error in 0.24. You should leave random_state to its default (None), or set shuffle=True.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** Cross Validation Result *****\n",
      "LOSS: 0.49470569267868997, ACC: 0.8579613149166108\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2020-07-03 21:12:31,992] Finished trial#63 with value: 0.49470569267868997 with parameters: {'lr': 0.004486400306359615, 'momentum': 0.8998348902346536}. Best is trial#54 with value: 0.4926346495747566.\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py:296: FutureWarning:\n",
      "\n",
      "Setting a random_state has no effect since shuffle is False. This will raise an error in 0.24. You should leave random_state to its default (None), or set shuffle=True.\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-55-0a2733d33586>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0msampler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptuna\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msamplers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTPESampler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mstudy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptuna\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_study\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msampler\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msampler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdirection\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'minimize'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mstudy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobjective\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_trials\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/optuna/study.py\u001b[0m in \u001b[0;36moptimize\u001b[0;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m    337\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mn_jobs\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m                 self._optimize_sequential(\n\u001b[0;32m--> 339\u001b[0;31m                     \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_trials\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgc_after_trial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    340\u001b[0m                 )\n\u001b[1;32m    341\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/optuna/study.py\u001b[0m in \u001b[0;36m_optimize_sequential\u001b[0;34m(self, func, n_trials, timeout, catch, callbacks, gc_after_trial, time_start)\u001b[0m\n\u001b[1;32m    680\u001b[0m                     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    681\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 682\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_trial_and_callbacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgc_after_trial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    683\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    684\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_progress_bar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtime_start\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtotal_seconds\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/optuna/study.py\u001b[0m in \u001b[0;36m_run_trial_and_callbacks\u001b[0;34m(self, func, catch, callbacks, gc_after_trial)\u001b[0m\n\u001b[1;32m    711\u001b[0m         \u001b[0;31m# type: (...) -> None\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    712\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 713\u001b[0;31m         \u001b[0mtrial\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_trial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgc_after_trial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    714\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallbacks\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    715\u001b[0m             \u001b[0mfrozen_trial\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_storage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_trial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_trial_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/optuna/study.py\u001b[0m in \u001b[0;36m_run_trial\u001b[0;34m(self, func, catch, gc_after_trial)\u001b[0m\n\u001b[1;32m    732\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    733\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 734\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    735\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrialPruned\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    736\u001b[0m             message = \"Setting status of trial#{} as {}. {}\".format(\n",
      "\u001b[0;32m<ipython-input-54-69d8f670da9d>\u001b[0m in \u001b[0;36mobjective\u001b[0;34m(trial)\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m             \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_iterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m             \u001b[0mval_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_iterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[0;31m#             print(f'| Epoch: {epoch+1:02} | Train Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-46-0862dd344697>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, iterator, optimizer, criterion)\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0mepoch_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m         \u001b[0mepoch_acc\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0macc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "\n",
    "sampler = optuna.samplers.TPESampler()\n",
    "study = optuna.create_study(sampler=sampler, direction='minimize')\n",
    "study.optimize(func=objective, n_trials=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import optuna\n",
    "\n",
    "# sampler = optuna.samplers.TPESampler()\n",
    "# study = optuna.create_study(sampler=sampler, direction='maximize')\n",
    "# study.optimize(func=objective, n_trials=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 490,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 491,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Depression</th>\n",
       "      <th>Alcohol</th>\n",
       "      <th>Suicide</th>\n",
       "      <th>Drugs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>304</th>\n",
       "      <td>Z9A6ACLK</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>305</th>\n",
       "      <td>ZDUOIGKN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>306</th>\n",
       "      <td>ZHQ60CCH</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>307</th>\n",
       "      <td>ZVIJMA4O</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>308</th>\n",
       "      <td>ZYIFAY98</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           ID  Depression  Alcohol  Suicide  Drugs\n",
       "304  Z9A6ACLK           0        0        0      0\n",
       "305  ZDUOIGKN           0        0        0      0\n",
       "306  ZHQ60CCH           0        0        0      0\n",
       "307  ZVIJMA4O           0        0        0      0\n",
       "308  ZYIFAY98           0        0        0      0"
      ]
     },
     "execution_count": 491,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 492,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 492,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(test.ID == sample.ID).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 493,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['Depression', 'Alcohol', 'Suicide', 'Drugs'])"
      ]
     },
     "execution_count": 493,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LABELS.vocab.stoi.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 494,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Depression</th>\n",
       "      <th>Alcohol</th>\n",
       "      <th>Suicide</th>\n",
       "      <th>Drugs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.766105</td>\n",
       "      <td>0.024983</td>\n",
       "      <td>0.197407</td>\n",
       "      <td>0.011505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.970994</td>\n",
       "      <td>0.002207</td>\n",
       "      <td>0.023719</td>\n",
       "      <td>0.003081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.992472</td>\n",
       "      <td>0.000565</td>\n",
       "      <td>0.006251</td>\n",
       "      <td>0.000712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.982532</td>\n",
       "      <td>0.001679</td>\n",
       "      <td>0.013480</td>\n",
       "      <td>0.002308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.558778</td>\n",
       "      <td>0.141814</td>\n",
       "      <td>0.108502</td>\n",
       "      <td>0.190906</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Depression   Alcohol   Suicide     Drugs\n",
       "0    0.766105  0.024983  0.197407  0.011505\n",
       "1    0.970994  0.002207  0.023719  0.003081\n",
       "2    0.992472  0.000565  0.006251  0.000712\n",
       "3    0.982532  0.001679  0.013480  0.002308\n",
       "4    0.558778  0.141814  0.108502  0.190906"
      ]
     },
     "execution_count": 494,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = pd.DataFrame(y_pred, columns=LABELS.vocab.stoi.keys())\n",
    "predictions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 495,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pd.concat([sample[['ID']], predictions], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 496,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Depression</th>\n",
       "      <th>Alcohol</th>\n",
       "      <th>Suicide</th>\n",
       "      <th>Drugs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>02V56KMO</td>\n",
       "      <td>0.766105</td>\n",
       "      <td>0.024983</td>\n",
       "      <td>0.197407</td>\n",
       "      <td>0.011505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>03BMGTOK</td>\n",
       "      <td>0.970994</td>\n",
       "      <td>0.002207</td>\n",
       "      <td>0.023719</td>\n",
       "      <td>0.003081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>03LZVFM6</td>\n",
       "      <td>0.992472</td>\n",
       "      <td>0.000565</td>\n",
       "      <td>0.006251</td>\n",
       "      <td>0.000712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0EPULUM5</td>\n",
       "      <td>0.982532</td>\n",
       "      <td>0.001679</td>\n",
       "      <td>0.013480</td>\n",
       "      <td>0.002308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0GM4C5GD</td>\n",
       "      <td>0.558778</td>\n",
       "      <td>0.141814</td>\n",
       "      <td>0.108502</td>\n",
       "      <td>0.190906</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         ID  Depression   Alcohol   Suicide     Drugs\n",
       "0  02V56KMO    0.766105  0.024983  0.197407  0.011505\n",
       "1  03BMGTOK    0.970994  0.002207  0.023719  0.003081\n",
       "2  03LZVFM6    0.992472  0.000565  0.006251  0.000712\n",
       "3  0EPULUM5    0.982532  0.001679  0.013480  0.002308\n",
       "4  0GM4C5GD    0.558778  0.141814  0.108502  0.190906"
      ]
     },
     "execution_count": 496,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 497,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.to_csv('./Pretrained_glove_300_text_contraction_all_0.28_softmax.csv', \n",
    "              index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# np.sum([0.921355,0.010272,0.058423,0.009950]) # import spacy\n",
    "nlp = spacy.load('en')\n",
    "\n",
    "def predict_sentiment(model, sentence, min_len = 5):\n",
    "    model.eval()\n",
    "    tokenized = [tok.text for tok in nlp.tokenizer(sentence)]\n",
    "    if len(tokenized) < min_len:\n",
    "        tokenized += ['<pad>'] * (min_len - len(tokenized))\n",
    "    indexed = [TEXT.vocab.stoi[t] for t in tokenized]\n",
    "    tensor = torch.LongTensor(indexed).to(device)\n",
    "    tensor = tensor.unsqueeze(0)\n",
    "    prediction = torch.sigmoid(model(tensor))\n",
    "    return prediction.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class load_data(object):\n",
    "    def __init__(self, SEED=1234):\n",
    "        torch.manual_seed(SEED)\n",
    "        torch.cuda.manual_seed(SEED)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "\n",
    "        tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "        \n",
    "        init_token = tokenizer.cls_token\n",
    "        eos_token = tokenizer.sep_token\n",
    "        pad_token = tokenizer.pad_token\n",
    "        unk_token = tokenizer.unk_token\n",
    "        \n",
    "        init_token_idx = tokenizer.convert_tokens_to_ids(init_token)\n",
    "        eos_token_idx = tokenizer.convert_tokens_to_ids(eos_token)\n",
    "        pad_token_idx = tokenizer.convert_tokens_to_ids(pad_token)\n",
    "        unk_token_idx = tokenizer.convert_tokens_to_ids(unk_token)\n",
    "        \n",
    "        LABELS = data.LabelField(dtype = torch.float)\n",
    "\n",
    "        TEXT = data.Field(batch_first = True,\n",
    "                          use_vocab = False,\n",
    "        #                   tokenize = tokenize_and_cut,\n",
    "                          preprocessing = tokenizer.convert_tokens_to_ids,\n",
    "                          init_token = init_token_idx,\n",
    "                          eos_token = eos_token_idx,\n",
    "                          pad_token = pad_token_idx,\n",
    "                          unk_token = unk_token_idx)\n",
    "\n",
    "\n",
    "#         self.train_data, self.test_data = datasets.IMDB.splits(TEXT, LABEL)\n",
    "        \n",
    "        self.train_data, self.test_data = data.TabularDataset.splits(\n",
    "            path='./data', train='Train_process.csv',\n",
    "            test='Test_process.csv', format='csv', skip_header=True,\n",
    "            fields=[('ID', None),\n",
    "                    ('text', TEXT),\n",
    "                    ('label', LABELS)])\n",
    "\n",
    "        self.SEED = SEED\n",
    "\n",
    "\n",
    "    def get_fold_data(self, num_folds=10):\n",
    "        \"\"\"\n",
    "        More details about 'fields' are available at \n",
    "        https://github.com/pytorch/text/blob/master/torchtext/datasets/imdb.py\n",
    "        \"\"\"\n",
    "\n",
    "        tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "        \n",
    "        init_token = tokenizer.cls_token\n",
    "        eos_token = tokenizer.sep_token\n",
    "        pad_token = tokenizer.pad_token\n",
    "        unk_token = tokenizer.unk_token\n",
    "        \n",
    "        init_token_idx = tokenizer.convert_tokens_to_ids(init_token)\n",
    "        eos_token_idx = tokenizer.convert_tokens_to_ids(eos_token)\n",
    "        pad_token_idx = tokenizer.convert_tokens_to_ids(pad_token)\n",
    "        unk_token_idx = tokenizer.convert_tokens_to_ids(unk_token)\n",
    "        \n",
    "        LABELS = data.LabelField(dtype = torch.float)\n",
    "\n",
    "        TEXT = data.Field(batch_first = True,\n",
    "                          use_vocab = False,\n",
    "        #                   tokenize = tokenize_and_cut,\n",
    "                          preprocessing = tokenizer.convert_tokens_to_ids,\n",
    "                          init_token = init_token_idx,\n",
    "                          eos_token = eos_token_idx,\n",
    "                          pad_token = pad_token_idx,\n",
    "                          unk_token = unk_token_idx)\n",
    "\n",
    "        fields = [('text', TEXT), ('label', LABELS)]\n",
    "        \n",
    "#         kf = KFold(n_splits=num_folds, shuffle=True, random_state=self.SEED)\n",
    "        kf = StratifiedKFold(n_splits=num_folds, random_state=self.SEED)\n",
    "        \n",
    "        train_data_arr = np.array(self.train_data.examples)\n",
    "\n",
    "#         ipdb.set_trace()\n",
    "        for train_index, val_index in kf.split(train_data_arr, y_kfold):\n",
    "            yield(\n",
    "                TEXT,\n",
    "                LABELS,\n",
    "                data.Dataset(train_data_arr[train_index], fields=fields),\n",
    "                data.Dataset(train_data_arr[val_index], fields=fields),\n",
    "            )\n",
    "    \n",
    "    def get_test_data(self):\n",
    "        return self.test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best_loss_val = 0.8\n",
    "\n",
    "def objective(trial):\n",
    "    \n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')    \n",
    "    \n",
    "    \n",
    "    lr  = trial.suggest_loguniform('lr', 1e-5, 1e-1)\n",
    "#     optim_ = trial.suggest_categorical('optim_',[optim.SGD, optim.RMSprop,optim.Adam])\n",
    "    \n",
    "    \n",
    "    momentum = trial.suggest_uniform('momentum', 0, 0.9)\n",
    "    \n",
    "    \n",
    "    \n",
    "#     data_generator = load_data()\n",
    "#     _history = []\n",
    "#     device = None\n",
    "#     model = None\n",
    "#     criterion = None\n",
    "#     fold_index = 0\n",
    "#     num_folds = 5\n",
    "#     batch_size = 32\n",
    "#     epochs = 20\n",
    "    \n",
    "    HIDDEN_DIM = 500\n",
    "    OUTPUT_DIM = len(LABELS.vocab)\n",
    "    N_LAYERS = 2\n",
    "    BIDIRECTIONAL = True\n",
    "    DROPOUT = 0.5\n",
    "\n",
    "    import torch.optim as optim\n",
    "\n",
    "    # optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "    best_valid_acc = 0.9\n",
    "    best_val_loss = 0.2\n",
    "\n",
    "    for TEXT, LABELS, train_data, val_data in data_generator.get_fold_data(num_folds=num_folds):\n",
    "#         print(\"***** Running Training *****\")\n",
    "#         print(f\"Now fold: {fold_index + 1} / {num_folds}\")\n",
    "\n",
    "        TEXT.build_vocab(train_data,\n",
    "                         vectors = \"glove.6B.300d\",\n",
    "                         unk_init = torch.Tensor.normal_)\n",
    "\n",
    "#         print(f'Embedding size: {TEXT.vocab.vectors.size()}.')\n",
    "        LABELS.build_vocab(train_data) # For converting str into float labels.\n",
    "\n",
    "    #     Model(len(TEXT.vocab), embedding_dim, hidden_dim,\n",
    "    #         output_dim, num_layers, dropout, TEXT.vocab.vectors, embedding_trainable)\n",
    "\n",
    "        HIDDEN_DIM = 500\n",
    "        OUTPUT_DIM = len(LABELS.vocab)\n",
    "        N_LAYERS = 2\n",
    "        BIDIRECTIONAL = True\n",
    "        DROPOUT = 0.5\n",
    "\n",
    "        model = BERTGRUSentiment(model,\n",
    "                         HIDDEN_DIM,\n",
    "                         OUTPUT_DIM,\n",
    "                         N_LAYERS,\n",
    "                         BIDIRECTIONAL,\n",
    "                         DROPOUT)\n",
    "\n",
    "\n",
    "        # Get w\n",
    "        from collections import Counter\n",
    "        class_distrbution = Counter()\n",
    "        for text in y_kfold:\n",
    "            class_distrbution[text] += 1\n",
    "\n",
    "\n",
    "        class_weights = [class_distrbution[i] for i in LABELS.vocab.stoi.keys()]\n",
    "        class_weights_normalized = [max(class_weights)/i for i in class_weights]\n",
    "\n",
    "\n",
    "        w = torch.Tensor(class_weights_normalized)\n",
    "        w = w.to(device)\n",
    "\n",
    "#         optimizer = optim.Adam(model.parameters())\n",
    "#         criterion = nn.CrossEntropyLoss(weight=w)\n",
    "        \n",
    "        optimizer = optim.SGD(model.parameters(), lr = lr, momentum=momentum, \n",
    "                      weight_decay=0, dampening=0, nesterov=False)\n",
    "        \n",
    "#         optimizer = optim.Adam(model.parameters(), lr = 1e-4, betas=(0.9, 0.999), \n",
    "#                       weight_decay=0.0, amsgrad=False)\n",
    "    \n",
    "        criterion = nn.CrossEntropyLoss(weight = w)\n",
    "        \n",
    "        \n",
    "\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        model = model.to(device)\n",
    "        criterion = criterion.to(device)\n",
    "\n",
    "        train_iterator = data.Iterator(train_data, batch_size=batch_size, sort_key=lambda x: len(x.text), device=device)\n",
    "        val_iterator = data.Iterator(val_data, batch_size=batch_size, sort_key=lambda x: len(x.text), device=device)\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            train_loss, train_acc = train(model, train_iterator, optimizer, criterion)\n",
    "            val_loss, val_acc = evaluate(model, val_iterator, criterion)\n",
    "#             print(f'| Epoch: {epoch+1:02} | Train Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
    "#             print(f'| Epoch: {epoch+1:02} | Valid Loss: {val_loss:.3f} | Valid Acc: {val_acc*100:.2f}%')\n",
    "#             print('################################################################################')\n",
    "#             if val_loss < best_val_loss:\n",
    "#                 best_val_loss = val_loss\n",
    "#                 print('Saving Model ...')\n",
    "#                 torch.save(model.state_dict(), 'Best_Model_'+str(best_val_loss)+'.pt')\n",
    "#                 print('*****************************************************')\n",
    "#                 print('best record: [epoch %d], [val loss %.5f], [val acc %.5f]' % (epoch, val_loss, val_acc))\n",
    "#                 print('*****************************************************')\n",
    "\n",
    "        val_loss, val_acc = evaluate(model, val_iterator, criterion) \n",
    "\n",
    "#         if val_loss < best_val_loss:\n",
    "#                 best_val_loss = val_loss\n",
    "#                 print('Saving Model ...')\n",
    "#                 torch.save(model.state_dict(), 'Best_Model_'+str(best_val_loss)+'.pt')\n",
    "#                 print('*****************************************************')\n",
    "#                 print('best record: [epoch %d], [val loss %.5f], [val acc %.5f]' % (epoch, val_loss, val_acc))\n",
    "#                 print('*****************************************************')\n",
    "\n",
    "    #     if val_acc > best_valid_acc:\n",
    "    #         best_valid_acc = val_acc\n",
    "    #         print('Saving Model ...')\n",
    "    #         torch.save(model.state_dict(), 'Best_Model_'+str(val_acc)+'.pt')\n",
    "    #         print('*****************************************************')\n",
    "    #         print('best record: [epoch %d], [val loss %.5f], [val acc %.5f]' % (epoch, val_loss, val_acc))\n",
    "    #         print('*****************************************************')\n",
    "\n",
    "#         print(f'Val. Loss: {val_loss:.3f} | Val. Acc: {val_acc*100:.2f}% |')\n",
    "\n",
    "        _history.append([val_loss, val_acc])\n",
    "        fold_index += 1\n",
    "\n",
    "    _history = np.asarray(_history)\n",
    "    loss = np.mean(_history[:, 0])\n",
    "    acc = np.mean(_history[:, 1])\n",
    "\n",
    "    print('***** Cross Validation Result *****')\n",
    "    print(f'LOSS: {loss}, ACC: {acc}')\n",
    "    \n",
    "    # Handle pruning based on the intermediate value.\n",
    "    if trial.should_prune():\n",
    "        raise optuna.exceptions.TrialPruned()\n",
    "            \n",
    "            \n",
    "    return loss"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
