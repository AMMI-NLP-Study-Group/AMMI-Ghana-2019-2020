{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ipdb in /home/aims/anaconda3/envs/aims/lib/python3.7/site-packages (0.12.2)\n",
      "Requirement already satisfied: ipython>=5.1.0; python_version >= \"3.4\" in /home/aims/anaconda3/envs/aims/lib/python3.7/site-packages (from ipdb) (7.9.0)\n",
      "Requirement already satisfied: setuptools in /home/aims/anaconda3/envs/aims/lib/python3.7/site-packages (from ipdb) (46.4.0.post20200518)\n",
      "Requirement already satisfied: prompt-toolkit<2.1.0,>=2.0.0 in /home/aims/anaconda3/envs/aims/lib/python3.7/site-packages (from ipython>=5.1.0; python_version >= \"3.4\"->ipdb) (2.0.10)\n",
      "Requirement already satisfied: pygments in /home/aims/anaconda3/envs/aims/lib/python3.7/site-packages (from ipython>=5.1.0; python_version >= \"3.4\"->ipdb) (2.6.1)\n",
      "Requirement already satisfied: traitlets>=4.2 in /home/aims/anaconda3/envs/aims/lib/python3.7/site-packages (from ipython>=5.1.0; python_version >= \"3.4\"->ipdb) (4.3.3)\n",
      "Requirement already satisfied: backcall in /home/aims/anaconda3/envs/aims/lib/python3.7/site-packages (from ipython>=5.1.0; python_version >= \"3.4\"->ipdb) (0.1.0)\n",
      "Requirement already satisfied: pexpect; sys_platform != \"win32\" in /home/aims/anaconda3/envs/aims/lib/python3.7/site-packages (from ipython>=5.1.0; python_version >= \"3.4\"->ipdb) (4.7.0)\n",
      "Requirement already satisfied: jedi>=0.10 in /home/aims/anaconda3/envs/aims/lib/python3.7/site-packages (from ipython>=5.1.0; python_version >= \"3.4\"->ipdb) (0.15.2)\n",
      "Requirement already satisfied: pickleshare in /home/aims/anaconda3/envs/aims/lib/python3.7/site-packages (from ipython>=5.1.0; python_version >= \"3.4\"->ipdb) (0.7.5)\n",
      "Requirement already satisfied: decorator in /home/aims/anaconda3/envs/aims/lib/python3.7/site-packages (from ipython>=5.1.0; python_version >= \"3.4\"->ipdb) (4.4.2)\n",
      "Requirement already satisfied: wcwidth in /home/aims/anaconda3/envs/aims/lib/python3.7/site-packages (from prompt-toolkit<2.1.0,>=2.0.0->ipython>=5.1.0; python_version >= \"3.4\"->ipdb) (0.1.7)\n",
      "Requirement already satisfied: six>=1.9.0 in /home/aims/anaconda3/envs/aims/lib/python3.7/site-packages (from prompt-toolkit<2.1.0,>=2.0.0->ipython>=5.1.0; python_version >= \"3.4\"->ipdb) (1.15.0)\n",
      "Requirement already satisfied: ipython-genutils in /home/aims/anaconda3/envs/aims/lib/python3.7/site-packages (from traitlets>=4.2->ipython>=5.1.0; python_version >= \"3.4\"->ipdb) (0.2.0)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /home/aims/anaconda3/envs/aims/lib/python3.7/site-packages (from pexpect; sys_platform != \"win32\"->ipython>=5.1.0; python_version >= \"3.4\"->ipdb) (0.6.0)\n",
      "Requirement already satisfied: parso>=0.5.2 in /home/aims/anaconda3/envs/aims/lib/python3.7/site-packages (from jedi>=0.10->ipython>=5.1.0; python_version >= \"3.4\"->ipdb) (0.7.0)\n",
      "Requirement already satisfied: transformers in /home/aims/anaconda3/envs/aims/lib/python3.7/site-packages (2.3.0)\n",
      "Requirement already satisfied: tqdm in /home/aims/anaconda3/envs/aims/lib/python3.7/site-packages (from transformers) (4.46.0)\n",
      "Requirement already satisfied: sacremoses in /home/aims/anaconda3/envs/aims/lib/python3.7/site-packages (from transformers) (0.0.35)\n",
      "Requirement already satisfied: numpy in /home/aims/anaconda3/envs/aims/lib/python3.7/site-packages (from transformers) (1.18.5)\n",
      "Requirement already satisfied: requests in /home/aims/anaconda3/envs/aims/lib/python3.7/site-packages (from transformers) (2.22.0)\n",
      "Requirement already satisfied: sentencepiece in /home/aims/anaconda3/envs/aims/lib/python3.7/site-packages (from transformers) (0.1.85)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/aims/anaconda3/envs/aims/lib/python3.7/site-packages (from transformers) (2019.12.20)\n",
      "Requirement already satisfied: boto3 in /home/aims/anaconda3/envs/aims/lib/python3.7/site-packages (from transformers) (1.10.5)\n",
      "Requirement already satisfied: joblib in /home/aims/anaconda3/envs/aims/lib/python3.7/site-packages (from sacremoses->transformers) (0.15.1)\n",
      "Requirement already satisfied: click in /home/aims/anaconda3/envs/aims/lib/python3.7/site-packages (from sacremoses->transformers) (7.0)\n",
      "Requirement already satisfied: six in /home/aims/anaconda3/envs/aims/lib/python3.7/site-packages (from sacremoses->transformers) (1.15.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/aims/anaconda3/envs/aims/lib/python3.7/site-packages (from requests->transformers) (2020.6.20)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /home/aims/anaconda3/envs/aims/lib/python3.7/site-packages (from requests->transformers) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /home/aims/anaconda3/envs/aims/lib/python3.7/site-packages (from requests->transformers) (1.25.6)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /home/aims/anaconda3/envs/aims/lib/python3.7/site-packages (from requests->transformers) (2.8)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /home/aims/anaconda3/envs/aims/lib/python3.7/site-packages (from boto3->transformers) (0.9.4)\n",
      "Requirement already satisfied: s3transfer<0.3.0,>=0.2.0 in /home/aims/anaconda3/envs/aims/lib/python3.7/site-packages (from boto3->transformers) (0.2.1)\n",
      "Requirement already satisfied: botocore<1.14.0,>=1.13.5 in /home/aims/anaconda3/envs/aims/lib/python3.7/site-packages (from boto3->transformers) (1.13.5)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\" in /home/aims/anaconda3/envs/aims/lib/python3.7/site-packages (from botocore<1.14.0,>=1.13.5->boto3->transformers) (2.8.1)\n",
      "Requirement already satisfied: docutils<0.16,>=0.10 in /home/aims/anaconda3/envs/aims/lib/python3.7/site-packages (from botocore<1.14.0,>=1.13.5->boto3->transformers) (0.15.2)\n",
      "Requirement already satisfied: optuna in /home/aims/anaconda3/envs/aims/lib/python3.7/site-packages (1.4.0)\n",
      "Requirement already satisfied: cliff in /home/aims/anaconda3/envs/aims/lib/python3.7/site-packages (from optuna) (3.1.0)\n",
      "Requirement already satisfied: tqdm in /home/aims/anaconda3/envs/aims/lib/python3.7/site-packages (from optuna) (4.46.0)\n",
      "Requirement already satisfied: cmaes>=0.3.2 in /home/aims/anaconda3/envs/aims/lib/python3.7/site-packages (from optuna) (0.5.0)\n",
      "Requirement already satisfied: joblib in /home/aims/anaconda3/envs/aims/lib/python3.7/site-packages (from optuna) (0.15.1)\n",
      "Requirement already satisfied: alembic in /home/aims/anaconda3/envs/aims/lib/python3.7/site-packages (from optuna) (1.4.2)\n",
      "Requirement already satisfied: sqlalchemy>=1.1.0 in /home/aims/anaconda3/envs/aims/lib/python3.7/site-packages (from optuna) (1.3.17)\n",
      "Requirement already satisfied: scipy!=1.4.0 in /home/aims/anaconda3/envs/aims/lib/python3.7/site-packages (from optuna) (1.3.2)\n",
      "Requirement already satisfied: numpy in /home/aims/anaconda3/envs/aims/lib/python3.7/site-packages (from optuna) (1.18.5)\n",
      "Requirement already satisfied: colorlog in /home/aims/anaconda3/envs/aims/lib/python3.7/site-packages (from optuna) (4.1.0)\n",
      "Requirement already satisfied: pyparsing>=2.1.0 in /home/aims/anaconda3/envs/aims/lib/python3.7/site-packages (from cliff->optuna) (2.4.7)\n",
      "Requirement already satisfied: pbr!=2.1.0,>=2.0.0 in /home/aims/anaconda3/envs/aims/lib/python3.7/site-packages (from cliff->optuna) (5.4.5)\n",
      "Requirement already satisfied: PyYAML>=3.12 in /home/aims/anaconda3/envs/aims/lib/python3.7/site-packages (from cliff->optuna) (5.3.1)\n",
      "Requirement already satisfied: six>=1.10.0 in /home/aims/anaconda3/envs/aims/lib/python3.7/site-packages (from cliff->optuna) (1.15.0)\n",
      "Requirement already satisfied: cmd2!=0.8.3,<0.9.0,>=0.8.0 in /home/aims/anaconda3/envs/aims/lib/python3.7/site-packages (from cliff->optuna) (0.8.9)\n",
      "Requirement already satisfied: stevedore>=1.20.0 in /home/aims/anaconda3/envs/aims/lib/python3.7/site-packages (from cliff->optuna) (1.32.0)\n",
      "Requirement already satisfied: PrettyTable<0.8,>=0.7.2 in /home/aims/anaconda3/envs/aims/lib/python3.7/site-packages (from cliff->optuna) (0.7.2)\n",
      "Requirement already satisfied: Mako in /home/aims/anaconda3/envs/aims/lib/python3.7/site-packages (from alembic->optuna) (1.1.3)\n",
      "Requirement already satisfied: python-dateutil in /home/aims/anaconda3/envs/aims/lib/python3.7/site-packages (from alembic->optuna) (2.8.1)\n",
      "Requirement already satisfied: python-editor>=0.3 in /home/aims/anaconda3/envs/aims/lib/python3.7/site-packages (from alembic->optuna) (1.0.4)\n",
      "Requirement already satisfied: wcwidth; sys_platform != \"win32\" in /home/aims/anaconda3/envs/aims/lib/python3.7/site-packages (from cmd2!=0.8.3,<0.9.0,>=0.8.0->cliff->optuna) (0.1.7)\n",
      "Requirement already satisfied: pyperclip in /home/aims/anaconda3/envs/aims/lib/python3.7/site-packages (from cmd2!=0.8.3,<0.9.0,>=0.8.0->cliff->optuna) (1.8.0)\n",
      "Requirement already satisfied: MarkupSafe>=0.9.2 in /home/aims/anaconda3/envs/aims/lib/python3.7/site-packages (from Mako->alembic->optuna) (1.1.1)\n",
      "Collecting pyspellchecker\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/04/d1/ec4e830e9f9c1fd788e1459dd09279fdf807bc7a475579fd7192450b879c/pyspellchecker-0.5.4-py2.py3-none-any.whl (1.9MB)\n",
      "\u001b[K     |████████████████████████████████| 1.9MB 293kB/s eta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: pyspellchecker\n",
      "Successfully installed pyspellchecker-0.5.4\n"
     ]
    }
   ],
   "source": [
    "!pip install ipdb\n",
    "!pip install transformers\n",
    "\n",
    "!pip install optuna\n",
    "!pip install pyspellchecker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !git clone https://github.com/plasticityai/magnitude.git\n",
    "# %cd magnitude/\n",
    "# !python setup.py install"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Package loading "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/aims/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/aims/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/aims/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/aims/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sb\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "import string\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from pymagnitude import *\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"./data/Train.csv\")\n",
    "test = pd.read_csv(\"./data/Test.csv\")\n",
    "\n",
    "sample = pd.read_csv(\"./data/SampleSubmission.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>611</th>\n",
       "      <td>BOHSNXCN</td>\n",
       "      <td>What should I do to stop alcoholism?</td>\n",
       "      <td>Alcohol</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>612</th>\n",
       "      <td>GVDXRQPY</td>\n",
       "      <td>How to become my oldself again</td>\n",
       "      <td>Suicide</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>613</th>\n",
       "      <td>IO4JHIQS</td>\n",
       "      <td>How can someone stop it?</td>\n",
       "      <td>Alcohol</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>614</th>\n",
       "      <td>1DS3P1XO</td>\n",
       "      <td>I feel unworthy</td>\n",
       "      <td>Depression</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>615</th>\n",
       "      <td>ORF71PVQ</td>\n",
       "      <td>I feel so discouraged with life</td>\n",
       "      <td>Depression</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           ID                                  text       label\n",
       "611  BOHSNXCN  What should I do to stop alcoholism?     Alcohol\n",
       "612  GVDXRQPY        How to become my oldself again     Suicide\n",
       "613  IO4JHIQS              How can someone stop it?     Alcohol\n",
       "614  1DS3P1XO                      I feel unworthy   Depression\n",
       "615  ORF71PVQ       I feel so discouraged with life  Depression"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['I feel that it was better I dieAm happy',\n",
       "       'Why do I get hallucinations?',\n",
       "       'I am stresseed due to lack of financial support in school',\n",
       "       'Why is life important?',\n",
       "       'How could I be helped to go through the depression?',\n",
       "       'What are the effects of depression to ones health',\n",
       "       'Why is everything so hard to deal with in this life',\n",
       "       ' I feel emotionally overwhelmed ',\n",
       "       'How to manage taking alcohol?',\n",
       "       'Is heaven open for us who smoke bhang?',\n",
       "       'How to survive without money?',\n",
       "       'How do i stop this? What do I do when life becomes unbearable?',\n",
       "       'I was ever sad,alone and always on drugsNow I know how to cope with negative aspects in my life',\n",
       "       'I feel empty,absent minded',\n",
       "       'I experienced depression in my studies',\n",
       "       'I feel like am alone in this world',\n",
       "       'I feel low and out of place',\n",
       "       'What should i do to get out of depression?',\n",
       "       'How can I stop using alcohol?',\n",
       "       'I feel hopeless, unworthy and useless …How do I cope with stress and frorge the past?',\n",
       "       'Does mediataton help stop weed addiction?',\n",
       "       'I feel bad because life difficulties, I also feel bad that I have no one to control',\n",
       "       'What are the effects of smoking bhang?',\n",
       "       'Sadness,hatredNow feeling is happiness',\n",
       "       'Feelings then were like everything was falling apart,Now things are in control',\n",
       "       'how do I cope with ta difficult situation?what will I d to avoid it?',\n",
       "       'Deteriorating academic performance',\n",
       "       'Effect of alcohol both in the society and my body',\n",
       "       'How could i stop using bhang', 'How to avoid drug abuse?',\n",
       "       'How will I stop? What addiction means.', 'I feel rejected',\n",
       "       ' Is it right for depressed person to use drugs',\n",
       "       'I just stopped taking alcohol because i thought it was not good for me',\n",
       "       'How do I seek help from alcoholism', 'i am  very stressed',\n",
       "       'Main challenges of depression and their side effects to my health',\n",
       "       ' feel confused and helpless and was pushed to the cornerFeeling kind of better',\n",
       "       'How  does one avoid bad company?',\n",
       "       'How to avoiod depression in campus',\n",
       "       'Causes of suicide by youths', 'What are the effects of alcohol?',\n",
       "       'I feel very low,how do I overcome it?',\n",
       "       'I feel painful and unwanted',\n",
       "       'What can I do to change the situation?',\n",
       "       'What to do to resist the bad feeling?',\n",
       "       'Too much classwork, and how do I cope ith many responsibilities',\n",
       "       'How can I stop alcohol',\n",
       "       'I am facing a lot of challenges in life financially, emotionally, psycologically and with no solutions…How can I safely look for solutions about depression on google',\n",
       "       'I always feel down'], dtype=object)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.text.values[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let check the distribution of the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>304</th>\n",
       "      <td>Z9A6ACLK</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>305</th>\n",
       "      <td>ZDUOIGKN</td>\n",
       "      <td>My girlfriend dumped me</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>306</th>\n",
       "      <td>ZHQ60CCH</td>\n",
       "      <td>How can I go back to being my old self?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>307</th>\n",
       "      <td>ZVIJMA4O</td>\n",
       "      <td>Is it true bhang is  medicinal?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>308</th>\n",
       "      <td>ZYIFAY98</td>\n",
       "      <td>how can I overcome the problem?</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           ID                                     text\n",
       "304  Z9A6ACLK                                      Yes\n",
       "305  ZDUOIGKN                  My girlfriend dumped me\n",
       "306  ZHQ60CCH  How can I go back to being my old self?\n",
       "307  ZVIJMA4O          Is it true bhang is  medicinal?\n",
       "308  ZYIFAY98          how can I overcome the problem?"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"i, me, my, myself, we, our, ours, ourselves, you, you're, you've, you'll, you'd, your, yours, yourself, yourselves, he, him, his, himself, she, she's, her, hers, herself, it, it's, its, itself, they, them, their, theirs, themselves, what, which, who, whom, this, that, that'll, these, those, am, is, are, was, were, be, been, being, have, has, had, having, do, does, did, doing, a, an, the, and, but, if, or, because, as, until, while, of, at, by, for, with, about, against, between, into, through, during, before, after, above, below, to, from, up, down, in, out, on, off, over, under, again, further, then, once, here, there, when, where, why, how, all, any, both, each, few, more, most, other, some, such, no, nor, not, only, own, same, so, than, too, very, s, t, can, will, just, don, don't, should, should've, now, d, ll, m, o, re, ve, y, ain, aren, aren't, couldn, couldn't, didn, didn't, doesn, doesn't, hadn, hadn't, hasn, hasn't, haven, haven't, isn, isn't, ma, mightn, mightn't, mustn, mustn't, needn, needn't, shan, shan't, shouldn, shouldn't, wasn, wasn't, weren, weren't, won, won't, wouldn, wouldn't\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\", \".join(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "contraction_mapping = {\"ain't\": \"is not\", \"aren't\": \"are not\",\n",
    "                       \"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\", \n",
    "                       \"couldn't\": \"could not\", \"didn't\": \"did not\",  \"doesn't\": \"does not\", \n",
    "                       \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \n",
    "                       \"haven't\": \"have not\", \"he'd\": \"he would\",\"he'll\": \"he will\", \n",
    "                       \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \n",
    "                       \"how'll\": \"how will\", \"how's\": \"how is\",  \"I'd\": \"I would\", \n",
    "                       \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\n",
    "                       \"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\", \"i'd've\": \"i would have\", \n",
    "                       \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \n",
    "                       \"isn't\": \"is not\", \"it'd\": \"it would\", \"it'd've\": \"it would have\", \"it'll\": \n",
    "                       \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \n",
    "                       \"ma'am\": \"madam\", \"mayn't\": \"may not\", \"might've\": \"might have\",\n",
    "                       \"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \"must've\": \"must have\", \n",
    "                       \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \n",
    "                       \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\", \"oughtn't\": \"ought not\", \n",
    "                       \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \n",
    "                       \"shan't've\": \"shall not have\", \"she'd\": \"she would\", \"she'd've\": \"she would have\", \n",
    "                       \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\", \n",
    "                       \"should've\": \"should have\", \"shouldn't\": \"should not\", \n",
    "                       \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\", \n",
    "                       \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \n",
    "                       \"that's\": \"that is\", \"there'd\": \"there would\", \"there'd've\": \"there would have\", \n",
    "                       \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \n",
    "                       \"they'd've\": \"they would have\", \"they'll\": \"they will\", \"they'll've\": \"they will have\", \n",
    "                       \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\", \"wasn't\": \"was not\", \n",
    "                       \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \n",
    "                       \"we'll've\": \"we will have\", \"we're\": \"we are\", \"we've\": \"we have\", \n",
    "                       \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \n",
    "                       \"what're\": \"what are\",  \"what's\": \"what is\", \"what've\": \"what have\", \n",
    "                       \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \n",
    "                       \"where's\": \"where is\", \"where've\": \"where have\", \"who'll\": \"who will\", \n",
    "                       \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\", \n",
    "                       \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \n",
    "                       \"won't\": \"will not\", \"won't've\": \"will not have\", \"would've\": \"would have\", \n",
    "                       \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \n",
    "                       \"y'all\": \"you all\", \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\n",
    "                       \"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\"you'd\": \"you would\", \n",
    "                       \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\", \n",
    "                       \"you're\": \"you are\", \"you've\": \"you have\" }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_caract(sentence):\n",
    "    sentence = re.sub(r\"[^a-zA-Z0-9?,'’‘´`]+\", ' ', sentence).strip()\n",
    "    sentence = re.sub(r\"[,]+\", ' , ', sentence).strip()\n",
    "#     sentence = re.sub(r\"[,]+\", ' ', sentence).strip()\n",
    "\n",
    "    sentence = re.sub(\"([A-Z])\", \" \\\\1\", sentence).strip() # add space basedon capital later\n",
    "    sentence = re.sub(r\"[?]+\", ' ? ', sentence).strip()\n",
    "#     sentence = re.sub(r\"[?]+\", ' ', sentence).strip()\n",
    "\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I feel that it was better I dieAm happy'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.text[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I feel that it was better  I die Am happy'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_caract(train.text[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[\"text_processed\"] = train[\"text\"].apply(lambda x: clean_caract(x))\n",
    "test[\"text_processed\"] = test[\"text\"].apply(lambda x: clean_caract(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "393 of 616 (63.7987%) questions have been changed.\n",
      "207 of 309 (66.9903%) questions have been changed.\n"
     ]
    }
   ],
   "source": [
    "train['Changed'] = np.where(train['text']==train['text_processed'], 'no', 'yes')\n",
    "print(\"{} of {} ({:.4f}%) questions have been changed.\".format(len(train[train['Changed']=='yes']), \n",
    "                                    len(train), 100*len(train[train['Changed']=='yes'])/len(train)))\n",
    "\n",
    "test['Changed'] = np.where(test['text']==test['text_processed'], 'no', 'yes')\n",
    "print(\"{} of {} ({:.4f}%) questions have been changed.\".format(len(test[test['Changed']=='yes']), \n",
    "                                    len(test), 100*len(test[test['Changed']=='yes'])/len(test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[\"text_lower\"] = train[\"text_processed\"].str.lower()\n",
    "test[\"text_lower\"] = test[\"text_processed\"].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "584 of 616 (94.8052%) questions have been changed.\n",
      "285 of 309 (92.2330%) questions have been changed.\n"
     ]
    }
   ],
   "source": [
    "train['Changed'] = np.where(train['text_processed']==train['text_lower'], 'no', 'yes')\n",
    "print(\"{} of {} ({:.4f}%) questions have been changed.\".format(len(train[train['Changed']=='yes']), \n",
    "                                    len(train), 100*len(train[train['Changed']=='yes'])/len(train)))\n",
    "\n",
    "test['Changed'] = np.where(test['text_processed']==test['text_lower'], 'no', 'yes')\n",
    "print(\"{} of {} ({:.4f}%) questions have been changed.\".format(len(test[test['Changed']=='yes']), \n",
    "                                    len(test), 100*len(test[test['Changed']=='yes'])/len(test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_contractions(text, mapping=contraction_mapping):\n",
    "    specials = [\"’\", \"‘\", \"´\", \"`\"]\n",
    "    for s in specials:\n",
    "        text = text.replace(s, \"'\")\n",
    "    text = ' '.join([mapping[t] if t in mapping else t for t in text.split(\" \")])\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[\"text_contraction\"] = train[\"text_lower\"].apply(lambda x: clean_contractions(x))\n",
    "test[\"text_contraction\"] = test[\"text_lower\"].apply(lambda x: clean_contractions(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15 of 616 (2.4351%) questions have been changed.\n",
      "6 of 309 (1.9417%) questions have been changed.\n"
     ]
    }
   ],
   "source": [
    "train['Changed'] = np.where(train['text_contraction']==train['text_lower'], 'no', 'yes')\n",
    "print(\"{} of {} ({:.4f}%) questions have been changed.\".format(len(train[train['Changed']=='yes']), \n",
    "                                    len(train), 100*len(train[train['Changed']=='yes'])/len(train)))\n",
    "\n",
    "test['Changed'] = np.where(test['text_contraction']==test['text_lower'], 'no', 'yes')\n",
    "print(\"{} of {} ({:.4f}%) questions have been changed.\".format(len(test[test['Changed']=='yes']), \n",
    "                                    len(test), 100*len(test[test['Changed']=='yes'])/len(test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'spelling correction'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from spellchecker import SpellChecker\n",
    "\n",
    "spell = SpellChecker()\n",
    "def correct_spellings(text):\n",
    "    corrected_text = []\n",
    "    misspelled_words = spell.unknown(text.split())\n",
    "    for word in text.split():\n",
    "        if word in misspelled_words:\n",
    "            corrected_text.append(spell.correction(word))\n",
    "        else:\n",
    "            corrected_text.append(word)\n",
    "    return \" \".join(corrected_text)\n",
    "        \n",
    "# text = 'What are the effects of bhang?'\n",
    "text = \"speling correctin\"\n",
    "correct_spellings(text)\n",
    "# correct_spellings(clean_contractions(clean_caract(text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[\"text_spellchecker\"] = train[\"text_contraction\"].apply(lambda x: correct_spellings(x))\n",
    "test[\"text_spellchecker\"] = test[\"text_contraction\"].apply(lambda x: correct_spellings(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "254 of 616 (41.2338%) questions have been changed.\n",
      "144 of 309 (46.6019%) questions have been changed.\n"
     ]
    }
   ],
   "source": [
    "train['Changed'] = np.where(train['text_spellchecker']==train['text_contraction'], 'no', 'yes')\n",
    "print(\"{} of {} ({:.4f}%) questions have been changed.\".format(len(train[train['Changed']=='yes']), \n",
    "                                    len(train), 100*len(train[train['Changed']=='yes'])/len(train)))\n",
    "\n",
    "test['Changed'] = np.where(test['text_spellchecker']==test['text_contraction'], 'no', 'yes')\n",
    "print(\"{} of {} ({:.4f}%) questions have been changed.\".format(len(test[test['Changed']=='yes']), \n",
    "                                    len(test), 100*len(test[test['Changed']=='yes'])/len(test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "vocab = Counter()\n",
    "for text in train.text_spellchecker:\n",
    "    for word in text.split():\n",
    "        vocab[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "802"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('i', 439),\n",
       " ('?', 209),\n",
       " ('to', 186),\n",
       " ('how', 169),\n",
       " ('feel', 141),\n",
       " ('the', 125),\n",
       " ('of', 122),\n",
       " ('and', 120),\n",
       " ('can', 101),\n",
       " ('is', 89),\n",
       " ('what', 86),\n",
       " ('do', 84),\n",
       " ('alcohol', 84),\n",
       " ('my', 81),\n",
       " (',', 76),\n",
       " ('am', 64),\n",
       " ('stop', 54),\n",
       " ('depression', 52),\n",
       " ('in', 51),\n",
       " ('life', 50),\n",
       " ('with', 49),\n",
       " ('why', 46),\n",
       " ('a', 43),\n",
       " ('it', 41),\n",
       " ('like', 39),\n",
       " ('me', 38),\n",
       " ('are', 33),\n",
       " ('was', 29),\n",
       " ('for', 28),\n",
       " ('depressed', 27),\n",
       " ('not', 27),\n",
       " ('avoid', 26),\n",
       " ('get', 25),\n",
       " ('taking', 24),\n",
       " ('one', 24),\n",
       " ('overcome', 24),\n",
       " ('low', 23),\n",
       " ('feeling', 23),\n",
       " ('be', 22),\n",
       " ('hang', 22),\n",
       " ('sad', 22),\n",
       " ('effects', 20),\n",
       " ('so', 20),\n",
       " ('that', 19),\n",
       " ('now', 19),\n",
       " ('have', 19),\n",
       " ('suicide', 19),\n",
       " ('lonely', 19),\n",
       " ('from', 18),\n",
       " ('better', 17),\n",
       " ('should', 17),\n",
       " ('when', 16),\n",
       " ('out', 16),\n",
       " ('myself', 16),\n",
       " ('up', 16),\n",
       " ('deal', 15),\n",
       " ('this', 15),\n",
       " ('alone', 15),\n",
       " ('does', 15),\n",
       " ('weed', 14),\n",
       " ('will', 14),\n",
       " ('on', 13),\n",
       " ('stress', 13),\n",
       " ('bad', 13),\n",
       " ('causes', 13),\n",
       " ('could', 12),\n",
       " ('no', 12),\n",
       " ('alcoholism', 12),\n",
       " ('drinking', 12),\n",
       " ('people', 12),\n",
       " ('stressed', 11),\n",
       " ('world', 11),\n",
       " ('help', 11),\n",
       " ('ways', 11),\n",
       " ('an', 11),\n",
       " ('had', 11),\n",
       " ('quit', 11),\n",
       " ('who', 10),\n",
       " ('because', 10),\n",
       " ('situation', 10),\n",
       " ('good', 10),\n",
       " ('very', 10),\n",
       " ('thoughts', 10),\n",
       " ('issues', 10),\n",
       " ('everything', 9),\n",
       " ('money', 9),\n",
       " ('using', 9),\n",
       " ('hopeless', 9),\n",
       " ('addiction', 9),\n",
       " ('feelings', 9),\n",
       " ('things', 9),\n",
       " ('about', 9),\n",
       " ('best', 9),\n",
       " ('relationship', 9),\n",
       " ('way', 9),\n",
       " ('if', 9),\n",
       " ('really', 9),\n",
       " ('any', 9),\n",
       " ('financial', 8),\n",
       " ('health', 8),\n",
       " ('by', 8),\n",
       " ('friends', 8),\n",
       " ('being', 8),\n",
       " ('which', 8),\n",
       " ('family', 8),\n",
       " ('cope', 7),\n",
       " ('control', 7),\n",
       " ('just', 7),\n",
       " ('much', 7),\n",
       " ('suicidal', 7),\n",
       " ('at', 7),\n",
       " ('take', 7),\n",
       " ('peer', 7),\n",
       " ('giving', 7),\n",
       " ('more', 7),\n",
       " ('want', 7),\n",
       " ('make', 7),\n",
       " ('assistance', 7),\n",
       " ('problem', 7),\n",
       " ('school', 6),\n",
       " ('without', 6),\n",
       " ('smoking', 6),\n",
       " ('use', 6),\n",
       " ('there', 6),\n",
       " ('reduce', 6),\n",
       " ('problems', 6),\n",
       " ('lost', 6),\n",
       " ('you', 6),\n",
       " ('need', 6),\n",
       " ('harmful', 6),\n",
       " ('did', 6),\n",
       " ('someone', 6),\n",
       " ('lack', 5),\n",
       " ('helped', 5),\n",
       " ('overwhelmed', 5),\n",
       " ('drugs', 5),\n",
       " ('were', 5),\n",
       " ('drug', 5),\n",
       " ('person', 5),\n",
       " ('unwanted', 5),\n",
       " ('lot', 5),\n",
       " ('down', 5),\n",
       " ('handle', 5),\n",
       " ('issue', 5),\n",
       " ('self', 5),\n",
       " ('come', 5),\n",
       " ('but', 5),\n",
       " ('okay', 5),\n",
       " ('all', 5),\n",
       " ('would', 5),\n",
       " ('where', 5),\n",
       " ('after', 5),\n",
       " ('over', 5),\n",
       " ('discouraged', 5),\n",
       " ('away', 5),\n",
       " ('or', 5),\n",
       " ('back', 5),\n",
       " ('through', 4),\n",
       " ('hard', 4),\n",
       " ('manage', 4),\n",
       " ('smoke', 4),\n",
       " ('always', 4),\n",
       " ('know', 4),\n",
       " ('experienced', 4),\n",
       " ('place', 4),\n",
       " ('unworthy', 4),\n",
       " ('right', 4),\n",
       " ('seek', 4),\n",
       " ('company', 4),\n",
       " ('financially', 4),\n",
       " ('wasted', 4),\n",
       " ('think', 4),\n",
       " ('drink', 4),\n",
       " ('worth', 4),\n",
       " ('esteem', 4),\n",
       " ('getting', 4),\n",
       " ('addicted', 4),\n",
       " ('felt', 4),\n",
       " ('exams', 4),\n",
       " ('pressure', 4),\n",
       " ('thinking', 4),\n",
       " ('as', 4),\n",
       " ('still', 4),\n",
       " ('against', 4),\n",
       " ('going', 4),\n",
       " ('loved', 4),\n",
       " ('death', 4),\n",
       " ('committing', 4),\n",
       " ('lead', 4),\n",
       " ('even', 4),\n",
       " ('parents', 4),\n",
       " ('most', 4),\n",
       " ('dealing', 4),\n",
       " ('solution', 4),\n",
       " ('sought', 4),\n",
       " ('close', 4),\n",
       " ('live', 4),\n",
       " ('young', 4),\n",
       " ('dont', 4),\n",
       " ('stay', 4),\n",
       " ('rid', 4),\n",
       " ('talk', 4),\n",
       " ('happy', 3),\n",
       " ('support', 3),\n",
       " ('important', 3),\n",
       " ('go', 3),\n",
       " ('emotionally', 3),\n",
       " ('past', 3),\n",
       " ('difficult', 3),\n",
       " ('effect', 3),\n",
       " ('body', 3),\n",
       " ('abuse', 3),\n",
       " ('rejected', 3),\n",
       " ('main', 3),\n",
       " ('challenges', 3),\n",
       " ('confused', 3),\n",
       " ('helpless', 3),\n",
       " ('youths', 3),\n",
       " ('too', 3),\n",
       " ('many', 3),\n",
       " ('solutions', 3),\n",
       " ('used', 3),\n",
       " ('times', 3),\n",
       " ('happen', 3),\n",
       " ('isolated', 3),\n",
       " ('fine', 3),\n",
       " ('point', 3),\n",
       " ('cannon', 3),\n",
       " ('intake', 3),\n",
       " ('experiencing', 3),\n",
       " ('makes', 3),\n",
       " ('addict', 3),\n",
       " ('nobody', 3),\n",
       " ('anyone', 3),\n",
       " ('helpful', 3),\n",
       " ('counselling', 3),\n",
       " ('needs', 3),\n",
       " ('personal', 3),\n",
       " ('consumption', 3),\n",
       " ('she', 3),\n",
       " ('went', 3),\n",
       " ('unloved', 3),\n",
       " ('never', 3),\n",
       " ('habit', 3),\n",
       " ('again', 3),\n",
       " ('relatives', 3),\n",
       " ('time', 3),\n",
       " ('nothing', 3),\n",
       " ('loss', 3),\n",
       " ('alive', 3),\n",
       " ('other', 3),\n",
       " ('future', 3),\n",
       " ('guidance', 3),\n",
       " ('girlfriend', 3),\n",
       " ('f', 3),\n",
       " ('education', 3),\n",
       " ('give', 3),\n",
       " ('hallucinations', 2),\n",
       " ('ones', 2),\n",
       " ('survive', 2),\n",
       " ('ever', 2),\n",
       " ('negative', 2),\n",
       " ('sadness', 2),\n",
       " ('happiness', 2),\n",
       " ('falling', 2),\n",
       " ('apart', 2),\n",
       " ('deteriorating', 2),\n",
       " ('performance', 2),\n",
       " ('means', 2),\n",
       " ('thought', 2),\n",
       " ('their', 2),\n",
       " ('side', 2),\n",
       " ('campus', 2),\n",
       " ('resist', 2),\n",
       " ('rent', 2),\n",
       " ('exist', 2),\n",
       " ('reason', 2),\n",
       " ('suffer', 2),\n",
       " ('importance', 2),\n",
       " ('influence', 2),\n",
       " ('withdraw', 2),\n",
       " ('anxiety', 2),\n",
       " ('fear', 2),\n",
       " ('we', 2),\n",
       " ('study', 2),\n",
       " ('order', 2),\n",
       " ('social', 2),\n",
       " ('feels', 2),\n",
       " ('got', 2),\n",
       " ('classes', 2),\n",
       " ('challenging', 2),\n",
       " ('abandoned', 2),\n",
       " ('whom', 2),\n",
       " ('bored', 2),\n",
       " ('appetite', 2),\n",
       " ('affect', 2),\n",
       " ('your', 2),\n",
       " ('only', 2),\n",
       " ('looking', 2),\n",
       " ('involved', 2),\n",
       " ('love', 2),\n",
       " ('coping', 2),\n",
       " ('often', 2),\n",
       " ('hate', 2),\n",
       " ('given', 2),\n",
       " ('weaker', 2),\n",
       " ('rehabilitation', 2),\n",
       " ('broken', 2),\n",
       " ('having', 2),\n",
       " ('high-school', 2),\n",
       " ('fees', 2),\n",
       " ('has', 2),\n",
       " ('easy', 2),\n",
       " ('friend', 2),\n",
       " ('living', 2),\n",
       " ('once', 2),\n",
       " ('strong', 2),\n",
       " ('enough', 2),\n",
       " ('withdrawn', 2),\n",
       " ('heartbreak', 2),\n",
       " ('unfair', 2),\n",
       " ('tired', 2),\n",
       " ('quite', 2),\n",
       " ('coming', 2),\n",
       " ('end', 2),\n",
       " ('our', 2),\n",
       " ('motivate', 2),\n",
       " ('some', 2),\n",
       " ('forgive', 2),\n",
       " ('relieved', 2),\n",
       " ('heartbroken', 2),\n",
       " ('led', 2),\n",
       " ('behaviour', 2),\n",
       " ('beer', 2),\n",
       " ('father', 2),\n",
       " ('needed', 2),\n",
       " ('killing', 2),\n",
       " ('caused', 2),\n",
       " ('find', 2),\n",
       " ('excessive', 2),\n",
       " ('talking', 2),\n",
       " ('them', 2),\n",
       " ('they', 2),\n",
       " ('beloved', 2),\n",
       " ('wanted', 2),\n",
       " ('around', 2),\n",
       " ('deserve', 2),\n",
       " ('unstable', 2),\n",
       " ('sickly', 2),\n",
       " ('since', 2),\n",
       " ('doing', 2),\n",
       " ('deserted', 2),\n",
       " ('ex', 2),\n",
       " ('god', 2),\n",
       " ('commit', 2),\n",
       " ('g', 2),\n",
       " ('anybody', 2),\n",
       " ('results', 2),\n",
       " ('possible', 2),\n",
       " ('heal', 2),\n",
       " ('meaning', 2),\n",
       " ('benefit', 2),\n",
       " ('motivated', 2),\n",
       " ('asked', 2),\n",
       " ('boredom', 2),\n",
       " ('wanting', 2),\n",
       " ('normal', 2),\n",
       " ('awful', 2),\n",
       " ('break', 2),\n",
       " ('steps', 2),\n",
       " ('class', 2),\n",
       " ('die', 1),\n",
       " ('due', 1),\n",
       " ('heaven', 1),\n",
       " ('open', 1),\n",
       " ('us', 1),\n",
       " ('becomes', 1),\n",
       " ('unbearable', 1),\n",
       " ('aspects', 1),\n",
       " ('empty', 1),\n",
       " ('absent', 1),\n",
       " ('minded', 1),\n",
       " ('studies', 1),\n",
       " ('useless', 1),\n",
       " ('forge', 1),\n",
       " ('meditation', 1),\n",
       " ('difficulties', 1),\n",
       " ('also', 1),\n",
       " ('hatred', 1),\n",
       " ('then', 1),\n",
       " ('ta', 1),\n",
       " ('d', 1),\n",
       " ('academic', 1),\n",
       " ('both', 1),\n",
       " ('society', 1),\n",
       " ('stopped', 1),\n",
       " ('pushed', 1),\n",
       " ('corner', 1),\n",
       " ('kind', 1),\n",
       " ('painful', 1),\n",
       " ('change', 1),\n",
       " ('classwork', 1),\n",
       " ('ith', 1),\n",
       " ('responsibilities', 1),\n",
       " ('facing', 1),\n",
       " ('psychologically', 1),\n",
       " ('safely', 1),\n",
       " ('look', 1),\n",
       " ('goole', 1),\n",
       " ('follow', 1),\n",
       " ('s', 1),\n",
       " ('minimize', 1),\n",
       " ('gambling', 1),\n",
       " ('prayer', 1),\n",
       " ('keeping', 1),\n",
       " ('busy', 1),\n",
       " ('harm', 1),\n",
       " ('fo', 1),\n",
       " ('indescribable', 1),\n",
       " ('common', 1),\n",
       " ('parental', 1),\n",
       " ('depletion', 1),\n",
       " ('causing', 1),\n",
       " ('succeed', 1),\n",
       " ('downhearted', 1),\n",
       " ('such', 1),\n",
       " ('incident', 1),\n",
       " ('associated', 1),\n",
       " ('irritated', 1),\n",
       " ('inadequacy', 1),\n",
       " ('measures', 1),\n",
       " ('incomplete', 1),\n",
       " ('overthinking', 1),\n",
       " ('chaotic', 1),\n",
       " ('failing', 1),\n",
       " ('content', 1),\n",
       " ('anyway', 1),\n",
       " ('overdriving', 1),\n",
       " ('expenses', 1),\n",
       " ('pain', 1),\n",
       " ('head', 1),\n",
       " ('exploding', 1),\n",
       " ('lots', 1),\n",
       " ('burnout', 1),\n",
       " ('stagnation', 1),\n",
       " ('dropped', 1),\n",
       " ('advantages', 1),\n",
       " ('calmness', 1),\n",
       " ('terrible', 1),\n",
       " ('days', 1),\n",
       " ('flying', 1),\n",
       " ('wasting', 1),\n",
       " ('disappointed', 1),\n",
       " ('nott', 1),\n",
       " ('simply', 1),\n",
       " ('worried', 1),\n",
       " ('mental', 1),\n",
       " ('excess', 1),\n",
       " ('disclose', 1),\n",
       " ('impacts', 1),\n",
       " ('real', 1),\n",
       " ('whole', 1),\n",
       " ('dizziness', 1),\n",
       " ('insomnia', 1),\n",
       " ('headache', 1),\n",
       " ('gain', 1),\n",
       " ('momentum', 1),\n",
       " ('academics', 1),\n",
       " ('own', 1),\n",
       " ('bette', 1),\n",
       " ('usage', 1),\n",
       " ('memory', 1),\n",
       " ('lone', 1),\n",
       " ('ranger', 1),\n",
       " ('less', 1),\n",
       " ('employment', 1),\n",
       " ('well', 1),\n",
       " ('hope', 1),\n",
       " ('disease', 1),\n",
       " ('drained', 1),\n",
       " ('resources', 1),\n",
       " ('affair', 1),\n",
       " ('hw', 1),\n",
       " ('waste', 1),\n",
       " ('strategies', 1),\n",
       " ('sister', 1),\n",
       " ('faints', 1),\n",
       " ('troubles', 1),\n",
       " ('solitude', 1),\n",
       " ('withdrawal', 1),\n",
       " ('programme', 1),\n",
       " ('im', 1),\n",
       " ('larger', 1),\n",
       " ('percentage', 1),\n",
       " ('horrible', 1),\n",
       " ('unsatisfied', 1),\n",
       " ('overcoming', 1),\n",
       " ('recommend', 1),\n",
       " ('mum', 1),\n",
       " ('finds', 1),\n",
       " ('church', 1),\n",
       " ('pastor', 1),\n",
       " ('sex', 1),\n",
       " ('his', 1),\n",
       " ('daughter', 1),\n",
       " ('guide', 1),\n",
       " ('constrained', 1),\n",
       " ('background', 1),\n",
       " ('stable', 1),\n",
       " ('debts', 1),\n",
       " ('elderly', 1),\n",
       " ('brother', 1),\n",
       " ('easily', 1),\n",
       " ('job', 1),\n",
       " ('graduating', 1),\n",
       " ('advise', 1),\n",
       " ('cool', 1),\n",
       " ('lowest', 1),\n",
       " ('low-key', 1),\n",
       " ('everyday', 1),\n",
       " ('npt', 1),\n",
       " ('left', 1),\n",
       " ('trapped', 1),\n",
       " ('loneliest', 1),\n",
       " ('moments', 1),\n",
       " ('tried', 1),\n",
       " ('asking', 1),\n",
       " ('told', 1),\n",
       " ('weak', 1),\n",
       " ('desperate', 1),\n",
       " ('realization', 1),\n",
       " ('bodies', 1),\n",
       " ('frustrations', 1),\n",
       " ('heart', 1),\n",
       " ('wronged', 1),\n",
       " ('accept', 1),\n",
       " ('reconciliation', 1),\n",
       " ('especially', 1),\n",
       " ('conquered', 1),\n",
       " ('childish', 1),\n",
       " ('shun', 1),\n",
       " ('same', 1),\n",
       " ('approach', 1),\n",
       " ('mentally', 1),\n",
       " ('disturbed', 1),\n",
       " ('handled', 1),\n",
       " ('fee', 1),\n",
       " ('broke', 1),\n",
       " ('unpreparedness', 1),\n",
       " ('able', 1),\n",
       " ('cause', 1),\n",
       " ('crisis', 1),\n",
       " ('smoked', 1),\n",
       " ('form', 1),\n",
       " ('entertainment', 1),\n",
       " ('adults', 1),\n",
       " ('keep', 1),\n",
       " ('holding', 1),\n",
       " ('purpose', 1),\n",
       " ('convince', 1),\n",
       " ('interact', 1),\n",
       " ('emotions', 1),\n",
       " ('cares', 1),\n",
       " ('pleasing', 1),\n",
       " ('must', 1),\n",
       " ('almost', 1),\n",
       " ('gave', 1),\n",
       " ('permanent', 1),\n",
       " ('below', 1),\n",
       " ('severely', 1),\n",
       " ('prefer', 1),\n",
       " ('bet', 1),\n",
       " ('house', 1),\n",
       " ('uneasy', 1),\n",
       " ('difficulty', 1),\n",
       " ('sure', 1),\n",
       " ('nervous', 1),\n",
       " ('bothered', 1),\n",
       " ('center', 1),\n",
       " ('hyper', 1),\n",
       " ('arousal', 1),\n",
       " ('arise', 1),\n",
       " ('sense', 1),\n",
       " ('limit', 1),\n",
       " ('done', 1),\n",
       " ('joy', 1),\n",
       " ('beneficial', 1),\n",
       " ('frustration', 1),\n",
       " ('stronger', 1),\n",
       " ('happier', 1),\n",
       " ('devastated', 1),\n",
       " ('angry', 1),\n",
       " ('sponsorship', 1),\n",
       " ('worst', 1),\n",
       " ('experience', 1),\n",
       " ('occurs', 1),\n",
       " ('split', 1),\n",
       " ('deeply', 1),\n",
       " ('affected', 1),\n",
       " ('member', 1),\n",
       " ('reconcile', 1),\n",
       " ('looks', 1),\n",
       " ('while', 1),\n",
       " ('schoolfees', 1),\n",
       " ('old', 1),\n",
       " ('kill', 1),\n",
       " ('pleasure', 1),\n",
       " ('anything', 1),\n",
       " ('solve', 1),\n",
       " ('fearful', 1),\n",
       " ('failure', 1),\n",
       " ('start', 1),\n",
       " ('confusion', 1),\n",
       " ('disgusted', 1),\n",
       " ('depresses', 1),\n",
       " ('seeking', 1),\n",
       " ('position', 1),\n",
       " ('currently', 1),\n",
       " ('judge', 1),\n",
       " ('quickly', 1),\n",
       " ('fail', 1),\n",
       " ('challenge', 1),\n",
       " ('raising', 1),\n",
       " ('task', 1),\n",
       " ('everyone', 1),\n",
       " ('exactly', 1),\n",
       " ('ignoring', 1),\n",
       " ('compared', 1),\n",
       " ('others', 1),\n",
       " ('unpredictable', 1),\n",
       " ('ignore', 1),\n",
       " ('making', 1),\n",
       " ('efforts', 1),\n",
       " ('rather', 1),\n",
       " ('supposed', 1),\n",
       " ('born', 1),\n",
       " ('depressing', 1),\n",
       " ('man', 1),\n",
       " ('add', 1),\n",
       " ('value', 1),\n",
       " ('sleep', 1),\n",
       " ('relax', 1),\n",
       " ('consult', 1),\n",
       " ('see', 1),\n",
       " ('reality', 1),\n",
       " ('distress', 1),\n",
       " ('completely', 1),\n",
       " ('unexpected', 1),\n",
       " ('dissatisfied', 1),\n",
       " ('schooled', 1),\n",
       " ('continue', 1),\n",
       " ('viewed', 1),\n",
       " ('negativecurrently', 1),\n",
       " ('fair', 1),\n",
       " ('sold', 1),\n",
       " ('along', 1),\n",
       " ('perimeter', 1),\n",
       " ('messed', 1),\n",
       " ('reasons', 1),\n",
       " ('forget', 1),\n",
       " ('temporarily', 1),\n",
       " ('liver', 1),\n",
       " ('mattered', 1),\n",
       " ('mess', 1),\n",
       " ('matter', 1),\n",
       " ('raise', 1),\n",
       " ('deny', 1),\n",
       " ('wonder', 1),\n",
       " ('part', 1),\n",
       " ('cure', 1),\n",
       " ('whats', 1),\n",
       " ('betray', 1),\n",
       " ('understand', 1),\n",
       " ('counsellor', 1),\n",
       " ('m', 1),\n",
       " ('though', 1),\n",
       " ('consented', 1),\n",
       " ('paranoid', 1),\n",
       " ('defeat', 1),\n",
       " ('post', 1),\n",
       " ('calling', 1),\n",
       " ('off', 1),\n",
       " ('semester', 1),\n",
       " ('dot', 1),\n",
       " ('receive', 1),\n",
       " ('relief', 1),\n",
       " ('manufactured', 1),\n",
       " ('pessimistic', 1),\n",
       " ('respond', 1),\n",
       " ('cheat', 1),\n",
       " ('sacrificing', 1),\n",
       " ('her', 1),\n",
       " ('sake', 1),\n",
       " ('deferring', 1),\n",
       " ('constraints', 1),\n",
       " ('those', 1),\n",
       " ('surround', 1),\n",
       " ('dumped', 1),\n",
       " ('long', 1),\n",
       " ('single', 1),\n",
       " ('jump', 1),\n",
       " ('into', 1),\n",
       " ('another', 1),\n",
       " ('t', 1),\n",
       " ('he', 1),\n",
       " ('distracting', 1),\n",
       " ('disturbing', 1),\n",
       " ('uncle', 1),\n",
       " ('provided', 1),\n",
       " ('basic', 1),\n",
       " ('true', 1),\n",
       " ('mad', 1),\n",
       " ('emptiness', 1),\n",
       " ('felling', 1),\n",
       " ('management', 1),\n",
       " ('mechanisms', 1),\n",
       " ('prevention', 1),\n",
       " ('looses', 1),\n",
       " ('relative', 1),\n",
       " ('advice', 1),\n",
       " ('remover', 1),\n",
       " ('interest', 1),\n",
       " ('little', 1),\n",
       " ('owing', 1),\n",
       " ('fact', 1),\n",
       " ('misbehave', 1),\n",
       " ('worthless', 1),\n",
       " ('frustrated', 1),\n",
       " ('day', 1),\n",
       " ('isolate', 1),\n",
       " ('crazy', 1),\n",
       " ('took', 1),\n",
       " ('under', 1),\n",
       " ('weather', 1),\n",
       " ('romantic', 1),\n",
       " ('relationships', 1),\n",
       " ('heavy', 1),\n",
       " ('burden', 1),\n",
       " ('supplied', 1),\n",
       " ('students', 1),\n",
       " ('smooth', 1),\n",
       " ('two', 1),\n",
       " ('miscarriages', 1),\n",
       " ('positive', 1),\n",
       " ('neglected', 1),\n",
       " ('method', 1),\n",
       " ('solving', 1),\n",
       " ('shame', 1),\n",
       " ('village', 1),\n",
       " ('been', 1),\n",
       " ('occuring', 1),\n",
       " ('choices', 1),\n",
       " ('made', 1),\n",
       " ('improve', 1),\n",
       " ('incidences', 1),\n",
       " ('failed', 1),\n",
       " (\"'\", 1),\n",
       " ('self-worth', 1),\n",
       " ('uncomfortable', 1),\n",
       " ('headaches', 1),\n",
       " ('criticized', 1),\n",
       " ('leave', 1),\n",
       " ('o', 1),\n",
       " ('utilize', 1),\n",
       " ('finances', 1),\n",
       " ('bankrupt', 1),\n",
       " ('name', 1),\n",
       " ('culture', 1),\n",
       " ('unworthiness', 1),\n",
       " ('associate', 1),\n",
       " ('work', 1),\n",
       " ('expected', 1),\n",
       " ('hustle', 1),\n",
       " ('picking', 1),\n",
       " ('concentration', 1),\n",
       " ('fantasizing', 1),\n",
       " ('romance', 1),\n",
       " ('pornographic', 1),\n",
       " ('avoided', 1),\n",
       " ('between', 1),\n",
       " ('cigarette', 1),\n",
       " ('major', 1),\n",
       " ('taken', 1),\n",
       " ('losing', 1),\n",
       " ('bets', 1),\n",
       " ('prohibited', 1),\n",
       " ('yet', 1),\n",
       " ('crumbling', 1),\n",
       " ('home', 1),\n",
       " ('listens', 1),\n",
       " ('started', 1),\n",
       " ('neglecting', 1),\n",
       " ('despising', 1),\n",
       " ('energy', 1),\n",
       " ('migraines', 1),\n",
       " ('remedy', 1),\n",
       " ('ask', 1),\n",
       " ('first', 1),\n",
       " ('later', 1),\n",
       " ('daily', 1)]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab.most_common(800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_text = Counter()\n",
    "\n",
    "for text in test.text_spellchecker:\n",
    "    for word in text.split():\n",
    "        if word not in vocab.keys():\n",
    "            vocab_text[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "195"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('yes', 1),\n",
       " ('year', 1),\n",
       " ('wrong', 2),\n",
       " ('worthy', 1),\n",
       " ('worthlessness', 1),\n",
       " ('withstand', 1),\n",
       " ('wise', 1),\n",
       " ('whether', 2),\n",
       " ('wellwishers', 1),\n",
       " ('wake', 1),\n",
       " ('view', 1),\n",
       " ('vengeance', 1),\n",
       " ('users', 1),\n",
       " ('urge', 1),\n",
       " ('university', 1),\n",
       " ('unit', 1),\n",
       " ('unhappy', 2),\n",
       " ('unfaithful', 1),\n",
       " ('understands', 1),\n",
       " ('uncared', 1),\n",
       " ('ugly', 1),\n",
       " ('trying', 2),\n",
       " ('trust', 2),\n",
       " ('traumatized', 1),\n",
       " ('trauma', 1),\n",
       " ('tough', 1),\n",
       " ('than', 1),\n",
       " ('terribly', 1),\n",
       " ('terminate', 1),\n",
       " ('temptations', 1),\n",
       " ('telling', 1),\n",
       " ('tasted', 1),\n",
       " ('tackle', 1),\n",
       " ('sustain', 1),\n",
       " ('successful', 1),\n",
       " ('stressing', 1),\n",
       " ('strength', 2),\n",
       " ('street', 1),\n",
       " ('strange', 1),\n",
       " ('stagnated', 1),\n",
       " ('sort', 1),\n",
       " ('something', 1),\n",
       " ('somehow', 1),\n",
       " ('small', 1),\n",
       " ('sleepless', 1),\n",
       " ('sleeping', 1),\n",
       " ('shit', 1),\n",
       " ('session', 1),\n",
       " ('seemed', 1),\n",
       " ('scared', 1),\n",
       " ('saved', 1),\n",
       " ('ring', 1),\n",
       " ('retake', 1),\n",
       " ('resume', 1),\n",
       " ('restless', 1),\n",
       " ('resentment', 1),\n",
       " ('replace', 1),\n",
       " ('repeat', 1),\n",
       " ('releases', 1),\n",
       " ('rejects', 1),\n",
       " ('rejection', 2),\n",
       " ('regulate', 1),\n",
       " ('regular', 1),\n",
       " ('refusing', 1),\n",
       " ('recover', 1),\n",
       " ('react', 1),\n",
       " ('quantities', 1),\n",
       " ('prudent', 1),\n",
       " ('produced', 1),\n",
       " ('pregnant', 1),\n",
       " ('pointing', 1),\n",
       " ('planned', 1),\n",
       " ('pills', 1),\n",
       " ('piece', 1),\n",
       " ('peace', 3),\n",
       " ('pay', 2),\n",
       " ('pass', 1),\n",
       " ('partners', 1),\n",
       " ('pale', 1),\n",
       " ('oppressed', 1),\n",
       " ('obtain', 1),\n",
       " ('nightside', 1),\n",
       " ('next', 1),\n",
       " ('negatively', 1),\n",
       " ('moving', 1),\n",
       " ('motivationsuicidal', 1),\n",
       " ('motivates', 1),\n",
       " ('motherhood', 1),\n",
       " ('moderately', 1),\n",
       " ('mind', 6),\n",
       " ('members', 1),\n",
       " ('medicinal', 1),\n",
       " ('mean', 1),\n",
       " ('may', 1),\n",
       " ('maintain', 2),\n",
       " ('let', 1),\n",
       " ('leads', 1),\n",
       " ('lasting', 1),\n",
       " ('ladies', 1),\n",
       " ('lac', 1),\n",
       " ('its', 3),\n",
       " ('interested', 1),\n",
       " ('intense', 1),\n",
       " ('indulgence', 1),\n",
       " ('including', 1),\n",
       " ('impregnated', 1),\n",
       " ('ideas', 1),\n",
       " ('hostile', 1),\n",
       " ('hopelessness', 1),\n",
       " ('holiday', 1),\n",
       " ('high', 1),\n",
       " ('helplessness', 1),\n",
       " ('hates', 1),\n",
       " ('guilty', 1),\n",
       " ('guilt', 1),\n",
       " ('guardians', 1),\n",
       " ('great', 1),\n",
       " ('girl', 1),\n",
       " ('generation', 1),\n",
       " ('funds', 1),\n",
       " ('fretting', 1),\n",
       " ('found', 2),\n",
       " ('fingers', 1),\n",
       " ('feared', 1),\n",
       " ('factors', 1),\n",
       " ('face', 1),\n",
       " ('expectations', 1),\n",
       " ('examinations', 1),\n",
       " ('escaping', 1),\n",
       " ('emotional', 2),\n",
       " ('emotion', 1),\n",
       " ('else', 2),\n",
       " ('each', 1),\n",
       " ('during', 1),\n",
       " ('drunkard', 1),\n",
       " ('downrecovering', 1),\n",
       " ('downplayed', 1),\n",
       " ('disillusioned', 1),\n",
       " ('devastating', 1),\n",
       " ('defer', 2),\n",
       " ('deep', 1),\n",
       " ('decision', 1),\n",
       " ('debt', 1),\n",
       " ('damage', 1),\n",
       " ('dad', 1),\n",
       " ('cycle', 1),\n",
       " ('crashing', 1),\n",
       " ('continuing', 1),\n",
       " ('contentment', 1),\n",
       " ('consuming', 1),\n",
       " ('consume', 1),\n",
       " ('considered', 1),\n",
       " ('conditions', 1),\n",
       " ('community', 1),\n",
       " ('committed', 1),\n",
       " ('comfort', 1),\n",
       " ('closure', 1),\n",
       " ('clear', 1),\n",
       " ('child', 3),\n",
       " ('cheated', 1),\n",
       " ('characters', 1),\n",
       " ('call', 1),\n",
       " ('building', 1),\n",
       " ('brought', 1),\n",
       " ('brings', 1),\n",
       " ('breakup', 1),\n",
       " ('breaking', 1),\n",
       " ('boyfriend', 2),\n",
       " ('bother', 1),\n",
       " ('blacking', 1),\n",
       " ('bitterness', 2),\n",
       " ('bitter', 1),\n",
       " ('birth', 2),\n",
       " ('beyond', 1),\n",
       " ('betrayed', 1),\n",
       " ('belong', 1),\n",
       " ('becoming', 1),\n",
       " ('awkward', 1),\n",
       " ('attend', 1),\n",
       " ('assist', 1),\n",
       " ('appreciate', 1),\n",
       " ('anxious', 2),\n",
       " ('anger', 2),\n",
       " ('amounts', 1),\n",
       " ('allow', 1),\n",
       " ('age', 1),\n",
       " ('advisable', 1),\n",
       " ('activities', 1),\n",
       " ('actively', 1),\n",
       " ('achieving', 1),\n",
       " ('abusing', 1),\n",
       " ('abstain', 1),\n",
       " ('abort', 2),\n",
       " ('ability', 1),\n",
       " ('2', 1)]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(vocab_text.most_common(200), reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>text_processed</th>\n",
       "      <th>Changed</th>\n",
       "      <th>text_lower</th>\n",
       "      <th>text_contraction</th>\n",
       "      <th>text_spellchecker</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SUAVK39Z</td>\n",
       "      <td>I feel that it was better I dieAm happy</td>\n",
       "      <td>Depression</td>\n",
       "      <td>I feel that it was better  I die Am happy</td>\n",
       "      <td>yes</td>\n",
       "      <td>i feel that it was better  i die am happy</td>\n",
       "      <td>i feel that it was better  i die am happy</td>\n",
       "      <td>i feel that it was better i die am happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9JDAGUV3</td>\n",
       "      <td>Why do I get hallucinations?</td>\n",
       "      <td>Drugs</td>\n",
       "      <td>Why do  I get hallucinations ?</td>\n",
       "      <td>yes</td>\n",
       "      <td>why do  i get hallucinations ?</td>\n",
       "      <td>why do  i get hallucinations ?</td>\n",
       "      <td>why do i get hallucinations ?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>419WR1LQ</td>\n",
       "      <td>I am stresseed due to lack of financial suppor...</td>\n",
       "      <td>Depression</td>\n",
       "      <td>I am stresseed due to lack of financial suppor...</td>\n",
       "      <td>yes</td>\n",
       "      <td>i am stresseed due to lack of financial suppor...</td>\n",
       "      <td>i am stresseed due to lack of financial suppor...</td>\n",
       "      <td>i am stressed due to lack of financial support...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6UY7DX6Q</td>\n",
       "      <td>Why is life important?</td>\n",
       "      <td>Suicide</td>\n",
       "      <td>Why is life important ?</td>\n",
       "      <td>no</td>\n",
       "      <td>why is life important ?</td>\n",
       "      <td>why is life important ?</td>\n",
       "      <td>why is life important ?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>FYC0FTFB</td>\n",
       "      <td>How could I be helped to go through the depres...</td>\n",
       "      <td>Depression</td>\n",
       "      <td>How could  I be helped to go through the depre...</td>\n",
       "      <td>yes</td>\n",
       "      <td>how could  i be helped to go through the depre...</td>\n",
       "      <td>how could  i be helped to go through the depre...</td>\n",
       "      <td>how could i be helped to go through the depres...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         ID  ...                                  text_spellchecker\n",
       "0  SUAVK39Z  ...           i feel that it was better i die am happy\n",
       "1  9JDAGUV3  ...                      why do i get hallucinations ?\n",
       "2  419WR1LQ  ...  i am stressed due to lack of financial support...\n",
       "3  6UY7DX6Q  ...                            why is life important ?\n",
       "4  FYC0FTFB  ...  how could i be helped to go through the depres...\n",
       "\n",
       "[5 rows x 8 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>text_processed</th>\n",
       "      <th>Changed</th>\n",
       "      <th>text_lower</th>\n",
       "      <th>text_contraction</th>\n",
       "      <th>text_spellchecker</th>\n",
       "      <th>text_stop</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SUAVK39Z</td>\n",
       "      <td>I feel that it was better I dieAm happy</td>\n",
       "      <td>Depression</td>\n",
       "      <td>I feel that it was better  I die Am happy</td>\n",
       "      <td>yes</td>\n",
       "      <td>i feel that it was better  i die am happy</td>\n",
       "      <td>i feel that it was better  i die am happy</td>\n",
       "      <td>i feel that it was better i die am happy</td>\n",
       "      <td>feel better die happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9JDAGUV3</td>\n",
       "      <td>Why do I get hallucinations?</td>\n",
       "      <td>Drugs</td>\n",
       "      <td>Why do  I get hallucinations ?</td>\n",
       "      <td>yes</td>\n",
       "      <td>why do  i get hallucinations ?</td>\n",
       "      <td>why do  i get hallucinations ?</td>\n",
       "      <td>why do i get hallucinations ?</td>\n",
       "      <td>get hallucinations ?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>419WR1LQ</td>\n",
       "      <td>I am stresseed due to lack of financial suppor...</td>\n",
       "      <td>Depression</td>\n",
       "      <td>I am stresseed due to lack of financial suppor...</td>\n",
       "      <td>yes</td>\n",
       "      <td>i am stresseed due to lack of financial suppor...</td>\n",
       "      <td>i am stresseed due to lack of financial suppor...</td>\n",
       "      <td>i am stressed due to lack of financial support...</td>\n",
       "      <td>stressed due lack financial support school</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6UY7DX6Q</td>\n",
       "      <td>Why is life important?</td>\n",
       "      <td>Suicide</td>\n",
       "      <td>Why is life important ?</td>\n",
       "      <td>no</td>\n",
       "      <td>why is life important ?</td>\n",
       "      <td>why is life important ?</td>\n",
       "      <td>why is life important ?</td>\n",
       "      <td>life important ?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>FYC0FTFB</td>\n",
       "      <td>How could I be helped to go through the depres...</td>\n",
       "      <td>Depression</td>\n",
       "      <td>How could  I be helped to go through the depre...</td>\n",
       "      <td>yes</td>\n",
       "      <td>how could  i be helped to go through the depre...</td>\n",
       "      <td>how could  i be helped to go through the depre...</td>\n",
       "      <td>how could i be helped to go through the depres...</td>\n",
       "      <td>could helped go depression ?</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         ID  ...                                   text_stop\n",
       "0  SUAVK39Z  ...                       feel better die happy\n",
       "1  9JDAGUV3  ...                        get hallucinations ?\n",
       "2  419WR1LQ  ...  stressed due lack financial support school\n",
       "3  6UY7DX6Q  ...                            life important ?\n",
       "4  FYC0FTFB  ...                could helped go depression ?\n",
       "\n",
       "[5 rows x 9 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "STOPWORDS = set(stopwords.words('english'))\n",
    "def remove_stopwords(text):\n",
    "    \"\"\"custom function to remove the stopwords\"\"\"\n",
    "    return \" \".join([word for word in str(text).split() if word not in STOPWORDS])\n",
    "\n",
    "train[\"text_stop\"] = train[\"text_spellchecker\"].apply(lambda text: remove_stopwords(text))\n",
    "test[\"text_stop\"] = test[\"text_spellchecker\"].apply(lambda text: remove_stopwords(text))\n",
    "\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "599 of 616 (97.2403%) questions have been changed.\n",
      "304 of 309 (98.3819%) questions have been changed.\n"
     ]
    }
   ],
   "source": [
    "train['Changed'] = np.where(train['text_stop']==train['text_spellchecker'], 'no', 'yes')\n",
    "print(\"{} of {} ({:.4f}%) questions have been changed.\".format(len(train[train['Changed']=='yes']), \n",
    "                                    len(train), 100*len(train[train['Changed']=='yes'])/len(train)))\n",
    "\n",
    "test['Changed'] = np.where(test['text_stop']==test['text_spellchecker'], 'no', 'yes')\n",
    "print(\"{} of {} ({:.4f}%) questions have been changed.\".format(len(test[test['Changed']=='yes']), \n",
    "                                    len(test), 100*len(test[test['Changed']=='yes'])/len(test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test['text_processed'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('?', 209),\n",
       " ('feel', 141),\n",
       " ('alcohol', 84),\n",
       " (',', 76),\n",
       " ('stop', 54),\n",
       " ('depression', 52),\n",
       " ('life', 50),\n",
       " ('like', 39),\n",
       " ('depressed', 27),\n",
       " ('avoid', 26)]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "cnt = Counter()\n",
    "for text in train[\"text_stop\"].values:\n",
    "    for word in text.split():\n",
    "        cnt[word] += 1\n",
    "        \n",
    "cnt.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FREQWORDS : {'feel', 'stop', 'depression', ',', 'life', '?', 'alcohol', 'avoid', 'like', 'depressed'}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>text_processed</th>\n",
       "      <th>Changed</th>\n",
       "      <th>text_lower</th>\n",
       "      <th>text_contraction</th>\n",
       "      <th>text_spellchecker</th>\n",
       "      <th>text_stop</th>\n",
       "      <th>text_wo_stopfreq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SUAVK39Z</td>\n",
       "      <td>I feel that it was better I dieAm happy</td>\n",
       "      <td>Depression</td>\n",
       "      <td>I feel that it was better  I die Am happy</td>\n",
       "      <td>yes</td>\n",
       "      <td>i feel that it was better  i die am happy</td>\n",
       "      <td>i feel that it was better  i die am happy</td>\n",
       "      <td>i feel that it was better i die am happy</td>\n",
       "      <td>feel better die happy</td>\n",
       "      <td>better die happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9JDAGUV3</td>\n",
       "      <td>Why do I get hallucinations?</td>\n",
       "      <td>Drugs</td>\n",
       "      <td>Why do  I get hallucinations ?</td>\n",
       "      <td>yes</td>\n",
       "      <td>why do  i get hallucinations ?</td>\n",
       "      <td>why do  i get hallucinations ?</td>\n",
       "      <td>why do i get hallucinations ?</td>\n",
       "      <td>get hallucinations ?</td>\n",
       "      <td>get hallucinations</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>419WR1LQ</td>\n",
       "      <td>I am stresseed due to lack of financial suppor...</td>\n",
       "      <td>Depression</td>\n",
       "      <td>I am stresseed due to lack of financial suppor...</td>\n",
       "      <td>yes</td>\n",
       "      <td>i am stresseed due to lack of financial suppor...</td>\n",
       "      <td>i am stresseed due to lack of financial suppor...</td>\n",
       "      <td>i am stressed due to lack of financial support...</td>\n",
       "      <td>stressed due lack financial support school</td>\n",
       "      <td>stressed due lack financial support school</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6UY7DX6Q</td>\n",
       "      <td>Why is life important?</td>\n",
       "      <td>Suicide</td>\n",
       "      <td>Why is life important ?</td>\n",
       "      <td>yes</td>\n",
       "      <td>why is life important ?</td>\n",
       "      <td>why is life important ?</td>\n",
       "      <td>why is life important ?</td>\n",
       "      <td>life important ?</td>\n",
       "      <td>important</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>FYC0FTFB</td>\n",
       "      <td>How could I be helped to go through the depres...</td>\n",
       "      <td>Depression</td>\n",
       "      <td>How could  I be helped to go through the depre...</td>\n",
       "      <td>yes</td>\n",
       "      <td>how could  i be helped to go through the depre...</td>\n",
       "      <td>how could  i be helped to go through the depre...</td>\n",
       "      <td>how could i be helped to go through the depres...</td>\n",
       "      <td>could helped go depression ?</td>\n",
       "      <td>could helped go</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         ID  ...                            text_wo_stopfreq\n",
       "0  SUAVK39Z  ...                            better die happy\n",
       "1  9JDAGUV3  ...                          get hallucinations\n",
       "2  419WR1LQ  ...  stressed due lack financial support school\n",
       "3  6UY7DX6Q  ...                                   important\n",
       "4  FYC0FTFB  ...                             could helped go\n",
       "\n",
       "[5 rows x 10 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FREQWORDS = set([w for (w, wc) in cnt.most_common(10)])\n",
    "print(f\"FREQWORDS : {FREQWORDS}\")\n",
    "def remove_freqwords(text):\n",
    "    \"\"\"custom function to remove the frequent words\"\"\"\n",
    "    return \" \".join([word for word in str(text).split() if word not in FREQWORDS])\n",
    "\n",
    "train[\"text_wo_stopfreq\"] = train[\"text_stop\"].apply(lambda text: remove_freqwords(text))\n",
    "test[\"text_wo_stopfreq\"] = test[\"text_stop\"].apply(lambda text: remove_freqwords(text))\n",
    "\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "468 of 616 (75.9740%) questions have been changed.\n",
      "233 of 309 (75.4045%) questions have been changed.\n"
     ]
    }
   ],
   "source": [
    "train['Changed'] = np.where(train['text_wo_stopfreq']==train['text_stop'], 'no', 'yes')\n",
    "print(\"{} of {} ({:.4f}%) questions have been changed.\".format(len(train[train['Changed']=='yes']), \n",
    "                                    len(train), 100*len(train[train['Changed']=='yes'])/len(train)))\n",
    "\n",
    "test['Changed'] = np.where(test['text_wo_stopfreq']==test['text_stop'], 'no', 'yes')\n",
    "print(\"{} of {} ({:.4f}%) questions have been changed.\".format(len(test[test['Changed']=='yes']), \n",
    "                                    len(test), 100*len(test[test['Changed']=='yes'])/len(test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAREWORDS : {'despising', 'become', 'first', 'later', 'energy', 'oneself', 'daily', 'remedy', 'ask', 'migraines'}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>text_processed</th>\n",
       "      <th>Changed</th>\n",
       "      <th>text_lower</th>\n",
       "      <th>text_contraction</th>\n",
       "      <th>text_spellchecker</th>\n",
       "      <th>text_stop</th>\n",
       "      <th>text_wo_stopfreq</th>\n",
       "      <th>text_wo_stopfreqrare</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SUAVK39Z</td>\n",
       "      <td>I feel that it was better I dieAm happy</td>\n",
       "      <td>Depression</td>\n",
       "      <td>I feel that it was better  I die Am happy</td>\n",
       "      <td>yes</td>\n",
       "      <td>i feel that it was better  i die am happy</td>\n",
       "      <td>i feel that it was better  i die am happy</td>\n",
       "      <td>i feel that it was better i die am happy</td>\n",
       "      <td>feel better die happy</td>\n",
       "      <td>better die happy</td>\n",
       "      <td>better die happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9JDAGUV3</td>\n",
       "      <td>Why do I get hallucinations?</td>\n",
       "      <td>Drugs</td>\n",
       "      <td>Why do  I get hallucinations ?</td>\n",
       "      <td>yes</td>\n",
       "      <td>why do  i get hallucinations ?</td>\n",
       "      <td>why do  i get hallucinations ?</td>\n",
       "      <td>why do i get hallucinations ?</td>\n",
       "      <td>get hallucinations ?</td>\n",
       "      <td>get hallucinations</td>\n",
       "      <td>get hallucinations</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>419WR1LQ</td>\n",
       "      <td>I am stresseed due to lack of financial suppor...</td>\n",
       "      <td>Depression</td>\n",
       "      <td>I am stresseed due to lack of financial suppor...</td>\n",
       "      <td>no</td>\n",
       "      <td>i am stresseed due to lack of financial suppor...</td>\n",
       "      <td>i am stresseed due to lack of financial suppor...</td>\n",
       "      <td>i am stressed due to lack of financial support...</td>\n",
       "      <td>stressed due lack financial support school</td>\n",
       "      <td>stressed due lack financial support school</td>\n",
       "      <td>stressed due lack financial support school</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6UY7DX6Q</td>\n",
       "      <td>Why is life important?</td>\n",
       "      <td>Suicide</td>\n",
       "      <td>Why is life important ?</td>\n",
       "      <td>yes</td>\n",
       "      <td>why is life important ?</td>\n",
       "      <td>why is life important ?</td>\n",
       "      <td>why is life important ?</td>\n",
       "      <td>life important ?</td>\n",
       "      <td>important</td>\n",
       "      <td>important</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>FYC0FTFB</td>\n",
       "      <td>How could I be helped to go through the depres...</td>\n",
       "      <td>Depression</td>\n",
       "      <td>How could  I be helped to go through the depre...</td>\n",
       "      <td>yes</td>\n",
       "      <td>how could  i be helped to go through the depre...</td>\n",
       "      <td>how could  i be helped to go through the depre...</td>\n",
       "      <td>how could i be helped to go through the depres...</td>\n",
       "      <td>could helped go depression ?</td>\n",
       "      <td>could helped go</td>\n",
       "      <td>could helped go</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         ID  ...                        text_wo_stopfreqrare\n",
       "0  SUAVK39Z  ...                            better die happy\n",
       "1  9JDAGUV3  ...                          get hallucinations\n",
       "2  419WR1LQ  ...  stressed due lack financial support school\n",
       "3  6UY7DX6Q  ...                                   important\n",
       "4  FYC0FTFB  ...                             could helped go\n",
       "\n",
       "[5 rows x 11 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Drop the two columns which are no more needed \n",
    "# df.drop([\"text_wo_punct\", \"text_wo_stop\"], axis=1, inplace=True)\n",
    "\n",
    "n_rare_words = 10\n",
    "RAREWORDS = set([w for (w, wc) in cnt.most_common()[:-n_rare_words-1:-1]])\n",
    "print(f\"RAREWORDS : {RAREWORDS}\")\n",
    "\n",
    "def remove_rarewords(text):\n",
    "    \"\"\"custom function to remove the rare words\"\"\"\n",
    "    return \" \".join([word for word in str(text).split() if word not in RAREWORDS])\n",
    "\n",
    "train[\"text_wo_stopfreqrare\"] = train[\"text_wo_stopfreq\"].apply(lambda text: remove_rarewords(text))\n",
    "test[\"text_wo_stopfreqrare\"] = test[\"text_wo_stopfreq\"].apply(lambda text: remove_rarewords(text))\n",
    "\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RAREWORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7 of 616 (1.1364%) questions have been changed.\n",
      "3 of 309 (0.9709%) questions have been changed.\n"
     ]
    }
   ],
   "source": [
    "train['Changed'] = np.where(train['text_wo_stopfreq']==train['text_wo_stopfreqrare'], 'no', 'yes')\n",
    "print(\"{} of {} ({:.4f}%) questions have been changed.\".format(len(train[train['Changed']=='yes']), \n",
    "                                    len(train), 100*len(train[train['Changed']=='yes'])/len(train)))\n",
    "\n",
    "test['Changed'] = np.where(test['text_wo_stopfreqrare']==test['text_wo_stopfreq'], 'no', 'yes')\n",
    "print(\"{} of {} ({:.4f}%) questions have been changed.\".format(len(test[test['Changed']=='yes']), \n",
    "                                    len(test), 100*len(test[test['Changed']=='yes'])/len(test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>text_processed</th>\n",
       "      <th>Changed</th>\n",
       "      <th>text_lower</th>\n",
       "      <th>text_contraction</th>\n",
       "      <th>text_spellchecker</th>\n",
       "      <th>text_stop</th>\n",
       "      <th>text_wo_stopfreq</th>\n",
       "      <th>text_wo_stopfreqrare</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SUAVK39Z</td>\n",
       "      <td>I feel that it was better I dieAm happy</td>\n",
       "      <td>Depression</td>\n",
       "      <td>I feel that it was better  I die Am happy</td>\n",
       "      <td>no</td>\n",
       "      <td>i feel that it was better  i die am happy</td>\n",
       "      <td>i feel that it was better  i die am happy</td>\n",
       "      <td>i feel that it was better i die am happy</td>\n",
       "      <td>feel better die happy</td>\n",
       "      <td>better die happy</td>\n",
       "      <td>better die happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9JDAGUV3</td>\n",
       "      <td>Why do I get hallucinations?</td>\n",
       "      <td>Drugs</td>\n",
       "      <td>Why do  I get hallucinations ?</td>\n",
       "      <td>no</td>\n",
       "      <td>why do  i get hallucinations ?</td>\n",
       "      <td>why do  i get hallucinations ?</td>\n",
       "      <td>why do i get hallucinations ?</td>\n",
       "      <td>get hallucinations ?</td>\n",
       "      <td>get hallucinations</td>\n",
       "      <td>get hallucinations</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>419WR1LQ</td>\n",
       "      <td>I am stresseed due to lack of financial suppor...</td>\n",
       "      <td>Depression</td>\n",
       "      <td>I am stresseed due to lack of financial suppor...</td>\n",
       "      <td>no</td>\n",
       "      <td>i am stresseed due to lack of financial suppor...</td>\n",
       "      <td>i am stresseed due to lack of financial suppor...</td>\n",
       "      <td>i am stressed due to lack of financial support...</td>\n",
       "      <td>stressed due lack financial support school</td>\n",
       "      <td>stressed due lack financial support school</td>\n",
       "      <td>stressed due lack financial support school</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6UY7DX6Q</td>\n",
       "      <td>Why is life important?</td>\n",
       "      <td>Suicide</td>\n",
       "      <td>Why is life important ?</td>\n",
       "      <td>no</td>\n",
       "      <td>why is life important ?</td>\n",
       "      <td>why is life important ?</td>\n",
       "      <td>why is life important ?</td>\n",
       "      <td>life important ?</td>\n",
       "      <td>important</td>\n",
       "      <td>important</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>FYC0FTFB</td>\n",
       "      <td>How could I be helped to go through the depres...</td>\n",
       "      <td>Depression</td>\n",
       "      <td>How could  I be helped to go through the depre...</td>\n",
       "      <td>no</td>\n",
       "      <td>how could  i be helped to go through the depre...</td>\n",
       "      <td>how could  i be helped to go through the depre...</td>\n",
       "      <td>how could i be helped to go through the depres...</td>\n",
       "      <td>could helped go depression ?</td>\n",
       "      <td>could helped go</td>\n",
       "      <td>could helped go</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         ID  ...                        text_wo_stopfreqrare\n",
       "0  SUAVK39Z  ...                            better die happy\n",
       "1  9JDAGUV3  ...                          get hallucinations\n",
       "2  419WR1LQ  ...  stressed due lack financial support school\n",
       "3  6UY7DX6Q  ...                                   important\n",
       "4  FYC0FTFB  ...                             could helped go\n",
       "\n",
       "[5 rows x 11 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "wordnet_map = {\"N\":wordnet.NOUN, \"V\":wordnet.VERB, \"J\":wordnet.ADJ, \"R\":wordnet.ADV}\n",
    "def lemmatize_words(text):\n",
    "    pos_tagged_text = nltk.pos_tag(text.split())\n",
    "#     ipdb.set_trace()\n",
    "    return \" \".join([lemmatizer.lemmatize(word, wordnet_map.get(pos[0], wordnet.NOUN)) for word, pos in pos_tagged_text])\n",
    "\n",
    "# train[\"text_lemmatized\"] = train[\"text_spellchecker\"].apply(lambda text: lemmatize_words(text))\n",
    "# test[\"text_lemmatized\"] = test[\"text_spellchecker\"].apply(lambda text: lemmatize_words(text))\n",
    "\n",
    "# train[\"text_lemmatized\"] = train[\"text_wo_stopfreqrare\"].apply(lambda text: lemmatize_words(text))\n",
    "# test[\"text_lemmatized\"] = test[\"text_wo_stopfreqrare\"].apply(lambda text: lemmatize_words(text))\n",
    "\n",
    "# train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'talking'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer.lemmatize(\"talking\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk.stem??\n",
    "# (\"talking\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[0;32m<ipython-input-16-c4500ae1ca42>\u001b[0m(6)\u001b[0;36mlemmatize_words\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m      5 \u001b[0;31m    \u001b[0mipdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m----> 6 \u001b[0;31m    \u001b[0;32mreturn\u001b[0m \u001b[0;34m\" \"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlemmatizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlemmatize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwordnet_map\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpos\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwordnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNOUN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpos_tagged_text\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m      7 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  pos_tagged_text\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('talking', 'VBG')]\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  q\n"
     ]
    },
    {
     "ename": "BdbQuit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mBdbQuit\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-0cf6bb44f9a7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlemmatize_words\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"talking\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-16-c4500ae1ca42>\u001b[0m in \u001b[0;36mlemmatize_words\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mpos_tagged_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpos_tag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mipdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m\" \"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlemmatizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlemmatize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwordnet_map\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpos\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwordnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNOUN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpos_tagged_text\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# train[\"text_lemmatized\"] = train[\"text_spellchecker\"].apply(lambda text: lemmatize_words(text))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-16-c4500ae1ca42>\u001b[0m in \u001b[0;36mlemmatize_words\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mpos_tagged_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpos_tag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mipdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m\" \"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlemmatizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlemmatize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwordnet_map\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpos\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwordnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNOUN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpos_tagged_text\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# train[\"text_lemmatized\"] = train[\"text_spellchecker\"].apply(lambda text: lemmatize_words(text))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/aims/lib/python3.7/bdb.py\u001b[0m in \u001b[0;36mtrace_dispatch\u001b[0;34m(self, frame, event, arg)\u001b[0m\n\u001b[1;32m     86\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;31m# None\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mevent\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'line'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mevent\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'call'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/aims/lib/python3.7/bdb.py\u001b[0m in \u001b[0;36mdispatch_line\u001b[0;34m(self, frame)\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_here\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbreak_here\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muser_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquitting\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mraise\u001b[0m \u001b[0mBdbQuit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrace_dispatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mBdbQuit\u001b[0m: "
     ]
    }
   ],
   "source": [
    "lemmatize_words(\"talking\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "318 of 616 (51.6234%) questions have been changed.\n",
      "175 of 309 (56.6343%) questions have been changed.\n"
     ]
    }
   ],
   "source": [
    "train['Changed'] = np.where(train['text_wo_stopfreqrare']==train['text_lemmatized'], 'no', 'yes')\n",
    "print(\"{} of {} ({:.4f}%) questions have been changed.\".format(len(train[train['Changed']=='yes']), \n",
    "                                    len(train), 100*len(train[train['Changed']=='yes'])/len(train)))\n",
    "\n",
    "test['Changed'] = np.where(test['text_wo_stopfreqrare']==test['text_lemmatized'], 'no', 'yes')\n",
    "print(\"{} of {} ({:.4f}%) questions have been changed.\".format(len(test[test['Changed']=='yes']), \n",
    "                                    len(test), 100*len(test[test['Changed']=='yes'])/len(test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train['Changed'] = np.where(train['text_spellchecker']==train['text_lemmatized'], 'no', 'yes')\n",
    "# print(\"{} of {} ({:.4f}%) questions have been changed.\".format(len(train[train['Changed']=='yes']), \n",
    "#                                     len(train), 100*len(train[train['Changed']=='yes'])/len(train)))\n",
    "\n",
    "# test['Changed'] = np.where(test['text_spellchecker']==test['text_lemmatized'], 'no', 'yes')\n",
    "# print(\"{} of {} ({:.4f}%) questions have been changed.\".format(len(test[test['Changed']=='yes']), \n",
    "#                                     len(test), 100*len(test[test['Changed']=='yes'])/len(test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save processed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>text_processed</th>\n",
       "      <th>Changed</th>\n",
       "      <th>text_lower</th>\n",
       "      <th>text_contraction</th>\n",
       "      <th>text_spellchecker</th>\n",
       "      <th>text_stop</th>\n",
       "      <th>text_wo_stopfreq</th>\n",
       "      <th>text_wo_stopfreqrare</th>\n",
       "      <th>text_lemmatized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SUAVK39Z</td>\n",
       "      <td>I feel that it was better I dieAm happy</td>\n",
       "      <td>Depression</td>\n",
       "      <td>I feel that it was better  I die Am happy</td>\n",
       "      <td>yes</td>\n",
       "      <td>i feel that it was better  i die am happy</td>\n",
       "      <td>i feel that it was better  i die am happy</td>\n",
       "      <td>i feel that it was better i die am happy</td>\n",
       "      <td>feel better die happy</td>\n",
       "      <td>better die happy</td>\n",
       "      <td>better die happy</td>\n",
       "      <td>well die happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9JDAGUV3</td>\n",
       "      <td>Why do I get hallucinations?</td>\n",
       "      <td>Drugs</td>\n",
       "      <td>Why do  I get hallucinations ?</td>\n",
       "      <td>yes</td>\n",
       "      <td>why do  i get hallucinations ?</td>\n",
       "      <td>why do  i get hallucinations ?</td>\n",
       "      <td>why do i get hallucinations ?</td>\n",
       "      <td>get hallucinations ?</td>\n",
       "      <td>get hallucinations</td>\n",
       "      <td>get hallucinations</td>\n",
       "      <td>get hallucination</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>419WR1LQ</td>\n",
       "      <td>I am stresseed due to lack of financial suppor...</td>\n",
       "      <td>Depression</td>\n",
       "      <td>I am stresseed due to lack of financial suppor...</td>\n",
       "      <td>yes</td>\n",
       "      <td>i am stresseed due to lack of financial suppor...</td>\n",
       "      <td>i am stresseed due to lack of financial suppor...</td>\n",
       "      <td>i am stressed due to lack of financial support...</td>\n",
       "      <td>stressed due lack financial support school</td>\n",
       "      <td>stressed due lack financial support school</td>\n",
       "      <td>stressed due lack financial support school</td>\n",
       "      <td>stress due lack financial support school</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6UY7DX6Q</td>\n",
       "      <td>Why is life important?</td>\n",
       "      <td>Suicide</td>\n",
       "      <td>Why is life important ?</td>\n",
       "      <td>no</td>\n",
       "      <td>why is life important ?</td>\n",
       "      <td>why is life important ?</td>\n",
       "      <td>why is life important ?</td>\n",
       "      <td>life important ?</td>\n",
       "      <td>important</td>\n",
       "      <td>important</td>\n",
       "      <td>important</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>FYC0FTFB</td>\n",
       "      <td>How could I be helped to go through the depres...</td>\n",
       "      <td>Depression</td>\n",
       "      <td>How could  I be helped to go through the depre...</td>\n",
       "      <td>yes</td>\n",
       "      <td>how could  i be helped to go through the depre...</td>\n",
       "      <td>how could  i be helped to go through the depre...</td>\n",
       "      <td>how could i be helped to go through the depres...</td>\n",
       "      <td>could helped go depression ?</td>\n",
       "      <td>could helped go</td>\n",
       "      <td>could helped go</td>\n",
       "      <td>could help go</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         ID  ...                           text_lemmatized\n",
       "0  SUAVK39Z  ...                            well die happy\n",
       "1  9JDAGUV3  ...                         get hallucination\n",
       "2  419WR1LQ  ...  stress due lack financial support school\n",
       "3  6UY7DX6Q  ...                                 important\n",
       "4  FYC0FTFB  ...                             could help go\n",
       "\n",
       "[5 rows x 12 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SUAVK39Z</td>\n",
       "      <td>i feel that it was better i die am happy</td>\n",
       "      <td>Depression</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9JDAGUV3</td>\n",
       "      <td>why do i get hallucinations ?</td>\n",
       "      <td>Drugs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>419WR1LQ</td>\n",
       "      <td>i am stressed due to lack of financial support...</td>\n",
       "      <td>Depression</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6UY7DX6Q</td>\n",
       "      <td>why is life important ?</td>\n",
       "      <td>Suicide</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>FYC0FTFB</td>\n",
       "      <td>how could i be helped to go through the depres...</td>\n",
       "      <td>Depression</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         ID                                               text       label\n",
       "0  SUAVK39Z           i feel that it was better i die am happy  Depression\n",
       "1  9JDAGUV3                      why do i get hallucinations ?       Drugs\n",
       "2  419WR1LQ  i am stressed due to lack of financial support...  Depression\n",
       "3  6UY7DX6Q                            why is life important ?     Suicide\n",
       "4  FYC0FTFB  how could i be helped to go through the depres...  Depression"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['text'] = train['text_spellchecker']\n",
    "test['text'] = test['text_spellchecker']\n",
    "\n",
    "proccessed_train = train[['ID', 'text', 'label']]\n",
    "proccessed_test = test[['ID', 'text']]\n",
    "\n",
    "proccessed_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "proccessed_train.to_csv('./data/Train_process.csv', index=False)\n",
    "proccessed_test.to_csv('./data/Test_process.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>304</th>\n",
       "      <td>Z9A6ACLK</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>305</th>\n",
       "      <td>ZDUOIGKN</td>\n",
       "      <td>my girlfriend dumped me</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>306</th>\n",
       "      <td>ZHQ60CCH</td>\n",
       "      <td>how can i go back to being my old self ?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>307</th>\n",
       "      <td>ZVIJMA4O</td>\n",
       "      <td>is it true hang is medicinal ?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>308</th>\n",
       "      <td>ZYIFAY98</td>\n",
       "      <td>how can i overcome the problem ?</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           ID                                      text\n",
       "304  Z9A6ACLK                                       yes\n",
       "305  ZDUOIGKN                   my girlfriend dumped me\n",
       "306  ZHQ60CCH  how can i go back to being my old self ?\n",
       "307  ZVIJMA4O            is it true hang is medicinal ?\n",
       "308  ZYIFAY98          how can i overcome the problem ?"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "proccessed_test.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
