{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 446,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ipdb in /usr/local/lib/python3.6/dist-packages (0.13.3)\n",
      "Requirement already satisfied: ipython>=5.1.0; python_version >= \"3.4\" in /usr/local/lib/python3.6/dist-packages (from ipdb) (5.5.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from ipdb) (47.3.1)\n",
      "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.6/dist-packages (from ipython>=5.1.0; python_version >= \"3.4\"->ipdb) (4.3.3)\n",
      "Requirement already satisfied: pickleshare in /usr/local/lib/python3.6/dist-packages (from ipython>=5.1.0; python_version >= \"3.4\"->ipdb) (0.7.5)\n",
      "Requirement already satisfied: decorator in /usr/local/lib/python3.6/dist-packages (from ipython>=5.1.0; python_version >= \"3.4\"->ipdb) (4.4.2)\n",
      "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.6/dist-packages (from ipython>=5.1.0; python_version >= \"3.4\"->ipdb) (0.8.1)\n",
      "Requirement already satisfied: pygments in /usr/local/lib/python3.6/dist-packages (from ipython>=5.1.0; python_version >= \"3.4\"->ipdb) (2.1.3)\n",
      "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.6/dist-packages (from ipython>=5.1.0; python_version >= \"3.4\"->ipdb) (1.0.18)\n",
      "Requirement already satisfied: pexpect; sys_platform != \"win32\" in /usr/local/lib/python3.6/dist-packages (from ipython>=5.1.0; python_version >= \"3.4\"->ipdb) (4.8.0)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from traitlets>=4.2->ipython>=5.1.0; python_version >= \"3.4\"->ipdb) (1.15.0)\n",
      "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.6/dist-packages (from traitlets>=4.2->ipython>=5.1.0; python_version >= \"3.4\"->ipdb) (0.2.0)\n",
      "Requirement already satisfied: wcwidth in /usr/local/lib/python3.6/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython>=5.1.0; python_version >= \"3.4\"->ipdb) (0.2.5)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.6/dist-packages (from pexpect; sys_platform != \"win32\"->ipython>=5.1.0; python_version >= \"3.4\"->ipdb) (0.6.0)\n",
      "Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (3.0.1)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
      "Requirement already satisfied: tokenizers==0.8.0-rc4 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.8.0rc4)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.19.0)\n",
      "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
      "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.43)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
      "Requirement already satisfied: sentencepiece!=0.1.92 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.1.91)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (1.15.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.9)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.6.20)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.15.1)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
      "Requirement already satisfied: optuna in /usr/local/lib/python3.6/dist-packages (1.5.0)\n",
      "Requirement already satisfied: cliff in /usr/local/lib/python3.6/dist-packages (from optuna) (3.3.0)\n",
      "Requirement already satisfied: sqlalchemy>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from optuna) (1.3.17)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from optuna) (4.41.1)\n",
      "Requirement already satisfied: scipy!=1.4.0 in /usr/local/lib/python3.6/dist-packages (from optuna) (1.4.1)\n",
      "Requirement already satisfied: alembic in /usr/local/lib/python3.6/dist-packages (from optuna) (1.4.2)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from optuna) (0.15.1)\n",
      "Requirement already satisfied: cmaes>=0.5.0 in /usr/local/lib/python3.6/dist-packages (from optuna) (0.5.1)\n",
      "Requirement already satisfied: colorlog in /usr/local/lib/python3.6/dist-packages (from optuna) (4.1.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from optuna) (1.19.0)\n",
      "Requirement already satisfied: pbr!=2.1.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from cliff->optuna) (5.4.5)\n",
      "Requirement already satisfied: stevedore>=1.20.0 in /usr/local/lib/python3.6/dist-packages (from cliff->optuna) (2.0.1)\n",
      "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from cliff->optuna) (1.15.0)\n",
      "Requirement already satisfied: PyYAML>=3.12 in /usr/local/lib/python3.6/dist-packages (from cliff->optuna) (3.13)\n",
      "Requirement already satisfied: pyparsing>=2.1.0 in /usr/local/lib/python3.6/dist-packages (from cliff->optuna) (2.4.7)\n",
      "Requirement already satisfied: cmd2!=0.8.3,>=0.8.0 in /usr/local/lib/python3.6/dist-packages (from cliff->optuna) (1.1.0)\n",
      "Requirement already satisfied: PrettyTable<0.8,>=0.7.2 in /usr/local/lib/python3.6/dist-packages (from cliff->optuna) (0.7.2)\n",
      "Requirement already satisfied: python-editor>=0.3 in /usr/local/lib/python3.6/dist-packages (from alembic->optuna) (1.0.4)\n",
      "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.6/dist-packages (from alembic->optuna) (2.8.1)\n",
      "Requirement already satisfied: Mako in /usr/local/lib/python3.6/dist-packages (from alembic->optuna) (1.1.3)\n",
      "Requirement already satisfied: pyperclip>=1.6 in /usr/local/lib/python3.6/dist-packages (from cmd2!=0.8.3,>=0.8.0->cliff->optuna) (1.8.0)\n",
      "Requirement already satisfied: setuptools>=34.4 in /usr/local/lib/python3.6/dist-packages (from cmd2!=0.8.3,>=0.8.0->cliff->optuna) (47.3.1)\n",
      "Requirement already satisfied: wcwidth>=0.1.7 in /usr/local/lib/python3.6/dist-packages (from cmd2!=0.8.3,>=0.8.0->cliff->optuna) (0.2.5)\n",
      "Requirement already satisfied: colorama>=0.3.7 in /usr/local/lib/python3.6/dist-packages (from cmd2!=0.8.3,>=0.8.0->cliff->optuna) (0.4.3)\n",
      "Requirement already satisfied: attrs>=16.3.0 in /usr/local/lib/python3.6/dist-packages (from cmd2!=0.8.3,>=0.8.0->cliff->optuna) (19.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.6/dist-packages (from Mako->alembic->optuna) (1.1.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install ipdb \n",
    "!pip install transformers\n",
    "!pip install optuna"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Package loading "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sb\n",
    "import numpy as np\n",
    "\n",
    "import ipdb\n",
    "import spacy\n",
    "spacy.load(\"en_core_web_sm\")\n",
    "import torch\n",
    "from torchtext import data\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchsummary import summary\n",
    "\n",
    "from transformers import RobertaTokenizer, BertTokenizer, BertModel, TransfoXLTokenizer, TransfoXLModel\n",
    "\n",
    "import optuna\n",
    "from sklearn.model_selection import KFold, StratifiedKFold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"./data/Train_process.csv\")\n",
    "test = pd.read_csv(\"./data/Test_process.csv\")\n",
    "\n",
    "sample = pd.read_csv(\"./data/SampleSubmission.csv\")\n",
    "y_kfold = train.label.values # this is used down in the the kfold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>611</th>\n",
       "      <td>BOHSNXCN</td>\n",
       "      <td>what should i do to stop alcoholism ?</td>\n",
       "      <td>Alcohol</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>612</th>\n",
       "      <td>GVDXRQPY</td>\n",
       "      <td>how to become my oneself again</td>\n",
       "      <td>Suicide</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>613</th>\n",
       "      <td>IO4JHIQS</td>\n",
       "      <td>how can someone stop it ?</td>\n",
       "      <td>Alcohol</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>614</th>\n",
       "      <td>1DS3P1XO</td>\n",
       "      <td>i feel unworthy</td>\n",
       "      <td>Depression</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>615</th>\n",
       "      <td>ORF71PVQ</td>\n",
       "      <td>i feel so discouraged with life</td>\n",
       "      <td>Depression</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           ID                                   text       label\n",
       "611  BOHSNXCN  what should i do to stop alcoholism ?     Alcohol\n",
       "612  GVDXRQPY         how to become my oneself again     Suicide\n",
       "613  IO4JHIQS              how can someone stop it ?     Alcohol\n",
       "614  1DS3P1XO                        i feel unworthy  Depression\n",
       "615  ORF71PVQ        i feel so discouraged with life  Depression"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check = pd.read_csv(\"./data/Test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "616"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check.text.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let check the distribution of the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>304</th>\n",
       "      <td>Z9A6ACLK</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>305</th>\n",
       "      <td>ZDUOIGKN</td>\n",
       "      <td>my girlfriend dumped me</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>306</th>\n",
       "      <td>ZHQ60CCH</td>\n",
       "      <td>how can i go back to being my old self ?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>307</th>\n",
       "      <td>ZVIJMA4O</td>\n",
       "      <td>is it true hang is medicinal ?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>308</th>\n",
       "      <td>ZYIFAY98</td>\n",
       "      <td>how can i overcome the problem ?</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           ID                                      text\n",
       "304  Z9A6ACLK                                       yes\n",
       "305  ZDUOIGKN                   my girlfriend dumped me\n",
       "306  ZHQ60CCH  how can i go back to being my old self ?\n",
       "307  ZVIJMA4O            is it true hang is medicinal ?\n",
       "308  ZYIFAY98          how can i overcome the problem ?"
      ]
     },
     "execution_count": 452,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Depression</th>\n",
       "      <th>Alcohol</th>\n",
       "      <th>Suicide</th>\n",
       "      <th>Drugs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>304</th>\n",
       "      <td>Z9A6ACLK</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>305</th>\n",
       "      <td>ZDUOIGKN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>306</th>\n",
       "      <td>ZHQ60CCH</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>307</th>\n",
       "      <td>ZVIJMA4O</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>308</th>\n",
       "      <td>ZYIFAY98</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           ID  Depression  Alcohol  Suicide  Drugs\n",
       "304  Z9A6ACLK           0        0        0      0\n",
       "305  ZDUOIGKN           0        0        0      0\n",
       "306  ZHQ60CCH           0        0        0      0\n",
       "307  ZVIJMA4O           0        0        0      0\n",
       "308  ZYIFAY98           0        0        0      0"
      ]
     },
     "execution_count": 453,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Depression    352\n",
       "Alcohol       140\n",
       "Suicide        66\n",
       "Drugs          58\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 454,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Alcohol', 'Depression', 'Drugs', 'Suicide']"
      ]
     },
     "execution_count": 455,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(np.unique(train.label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "616"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# This plot need to be checked \n",
    "plt.pie(list(train.label.value_counts()), labels=list(np.unique(train.label)))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Alcohol', 'Depression', 'Drugs', 'Suicide']"
      ]
     },
     "execution_count": 457,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(np.unique(train.label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "metadata": {},
   "outputs": [],
   "source": [
    "def_len = lambda x: len(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train['len'] = train.text.apply(def_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train.text.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train.len.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # This plot need to be checked \n",
    "# plt.pie(list(train.len.value_counts()), labels=list(np.unique(train.len)))\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing Data\n",
    "\n",
    "One of the main concepts of TorchText is the `Field`. These define how your data should be processed. In our sentiment classification task the data consists of both the raw string of the review and the sentiment, either \"pos\" or \"neg\".\n",
    "\n",
    "The parameters of a `Field` specify how the data should be processed. \n",
    "\n",
    "We use the `TEXT` field to define how the review should be processed, and the `LABEL` field to process the sentiment. \n",
    "\n",
    "Our `TEXT` field has `tokenize='spacy'` as an argument. This defines that the \"tokenization\" (the act of splitting the string into discrete \"tokens\") should be done using the [spaCy](https://spacy.io) tokenizer. If no `tokenize` argument is passed, the default is simply splitting the string on spaces.\n",
    "\n",
    "`LABEL` is defined by a `LabelField`, a special subset of the `Field` class specifically used for handling labels. We will explain the `dtype` argument later.\n",
    "\n",
    "For more on `Fields`, go [here](https://github.com/pytorch/text/blob/master/torchtext/data/field.py).\n",
    "\n",
    "We also set the random seeds for reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SUAVK39Z</td>\n",
       "      <td>i feel that it was better i die am happy</td>\n",
       "      <td>Depression</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9JDAGUV3</td>\n",
       "      <td>why do i get hallucinations ?</td>\n",
       "      <td>Drugs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>419WR1LQ</td>\n",
       "      <td>i am stressed due to lack of financial support...</td>\n",
       "      <td>Depression</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6UY7DX6Q</td>\n",
       "      <td>why is life important ?</td>\n",
       "      <td>Suicide</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>FYC0FTFB</td>\n",
       "      <td>how could i be helped to go through the depres...</td>\n",
       "      <td>Depression</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         ID                                               text       label\n",
       "0  SUAVK39Z           i feel that it was better i die am happy  Depression\n",
       "1  9JDAGUV3                      why do i get hallucinations ?       Drugs\n",
       "2  419WR1LQ  i am stressed due to lack of financial support...  Depression\n",
       "3  6UY7DX6Q                            why is life important ?     Suicide\n",
       "4  FYC0FTFB  how could i be helped to go through the depres...  Depression"
      ]
     },
     "execution_count": 463,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "metadata": {},
   "outputs": [],
   "source": [
    "# label_test = le.transform(test.label)\n",
    "\n",
    "# test['label'] = label_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>304</th>\n",
       "      <td>Z9A6ACLK</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>305</th>\n",
       "      <td>ZDUOIGKN</td>\n",
       "      <td>my girlfriend dumped me</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>306</th>\n",
       "      <td>ZHQ60CCH</td>\n",
       "      <td>how can i go back to being my old self ?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>307</th>\n",
       "      <td>ZVIJMA4O</td>\n",
       "      <td>is it true hang is medicinal ?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>308</th>\n",
       "      <td>ZYIFAY98</td>\n",
       "      <td>how can i overcome the problem ?</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           ID                                      text\n",
       "304  Z9A6ACLK                                       yes\n",
       "305  ZDUOIGKN                   my girlfriend dumped me\n",
       "306  ZHQ60CCH  how can i go back to being my old self ?\n",
       "307  ZVIJMA4O            is it true hang is medicinal ?\n",
       "308  ZYIFAY98          how can i overcome the problem ?"
      ]
     },
     "execution_count": 465,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Depression</th>\n",
       "      <th>Alcohol</th>\n",
       "      <th>Suicide</th>\n",
       "      <th>Drugs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>02V56KMO</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>03BMGTOK</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>03LZVFM6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0EPULUM5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0GM4C5GD</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         ID  Depression  Alcohol  Suicide  Drugs\n",
       "0  02V56KMO           0        0        0      0\n",
       "1  03BMGTOK           0        0        0      0\n",
       "2  03LZVFM6           0        0        0      0\n",
       "3  0EPULUM5           0        0        0      0\n",
       "4  0GM4C5GD           0        0        0      0"
      ]
     },
     "execution_count": 466,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 467,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RobertaTokenizer.from_pretrained??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] [SEP] [PAD] [UNK]\n"
     ]
    }
   ],
   "source": [
    "# tokenizer = RobertaTokenizer.from_pretrained('roberta-base') # roberta-base, bert-base-uncased\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "# tokenizer = TransfoXLTokenizer.from_pretrained('transfo-xl-wt103')\n",
    "\n",
    "init_token = tokenizer.cls_token\n",
    "eos_token = tokenizer.sep_token\n",
    "pad_token = tokenizer.pad_token\n",
    "unk_token = tokenizer.unk_token\n",
    "\n",
    "print(init_token, eos_token, pad_token, unk_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 469,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "101 102 0 100\n"
     ]
    }
   ],
   "source": [
    "init_token_idx = tokenizer.convert_tokens_to_ids(init_token)\n",
    "eos_token_idx = tokenizer.convert_tokens_to_ids(eos_token)\n",
    "pad_token_idx = tokenizer.convert_tokens_to_ids(pad_token)\n",
    "unk_token_idx = tokenizer.convert_tokens_to_ids(unk_token)\n",
    "\n",
    "print(init_token_idx, eos_token_idx, pad_token_idx, unk_token_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 470,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 1234\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "# TEXT = data.Field(tokenize = 'spacy')\n",
    "LABELS = data.LabelField(dtype = torch.float)\n",
    "\n",
    "TEXT = data.Field(batch_first = True,\n",
    "                  use_vocab = False,\n",
    "#                   tokenize = tokenize_and_cut,\n",
    "                  preprocessing = tokenizer.convert_tokens_to_ids,\n",
    "                  init_token = init_token_idx,\n",
    "                  eos_token = eos_token_idx,\n",
    "                  pad_token = pad_token_idx,\n",
    "                  unk_token = unk_token_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data= data.TabularDataset.splits(\n",
    "    path='./data', train='Train_process.csv',\n",
    "    test='Test_process.csv', format='csv', skip_header=True,\n",
    "    fields=[('ID', None),\n",
    "            ('text', TEXT),\n",
    "            ('label', LABELS)])\n",
    "\n",
    "\n",
    "# train_data = data.TabularDataset.splits(\n",
    "#     path='./data', train='Train.csv',\n",
    "#     format='csv', skip_header=True,\n",
    "#     fields=[('ID', None),\n",
    "#             ('text', TEXT),\n",
    "#             ('label', LABEL)])\n",
    "\n",
    "# test_data = data.TabularDataset.splits(\n",
    "#     path='./data', test='Test.csv', format='csv', \n",
    "#     skip_header=True,\n",
    "#     fields=[('ID', None),\n",
    "#             ('text', TEXT)])\n",
    "\n",
    "# train_iter, val_iter, test_iter = data.BucketIterator.splits(\n",
    "#     (train, val, test), batch_sizes=(16, 256, 256),\n",
    "#     sort_key=lambda x: len(x.text), device=0)\n",
    "\n",
    "# TEXT.build_vocab(train)\n",
    "# LABELS.build_vocab(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 616\n",
      "Number of testing examples: 309\n"
     ]
    }
   ],
   "source": [
    "print(f'Number of training examples: {len(train_data)}')\n",
    "print(f'Number of testing examples: {len(test_data)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let check one example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 473,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': [1045, 2514, 2008, 2009, 2001, 2488, 1045, 3280, 2572, 3407], 'label': 'Depression'}\n"
     ]
    }
   ],
   "source": [
    "print(vars(train_data.examples[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 474,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'feel', 'that', 'it', 'was', 'better', 'i', 'die', 'am', 'happy']\n"
     ]
    }
   ],
   "source": [
    "tokens = tokenizer.convert_ids_to_tokens(vars(train_data.examples[0])['text'])\n",
    "\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 475,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "train_data, valid_data = train_data.split(random_state = random.seed(SEED), \n",
    "                                          stratified=True, split_ratio=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 476,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 493\n",
      "Number of validation examples: 123\n",
      "Number of testing examples: 309\n"
     ]
    }
   ],
   "source": [
    "print(f'Number of training examples: {len(train_data)}')\n",
    "print(f'Number of validation examples: {len(valid_data)}')\n",
    "print(f'Number of testing examples: {len(test_data)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 477,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAX_VOCAB_SIZE = 700   # 751\n",
    "\n",
    "# # TEXT.build_vocab(train_data, max_size = MAX_VOCAB_SIZE)\n",
    "# # LABEL.build_vocab(train_data)\n",
    "\n",
    "# # MAX_VOCAB_SIZE = 25_000\n",
    "\n",
    "# TEXT.build_vocab(train_data, \n",
    "#                  max_size = MAX_VOCAB_SIZE, \n",
    "#                  vectors = \"glove.6B.100d\", \n",
    "#                  unk_init = torch.Tensor.normal_)\n",
    "\n",
    "LABELS.build_vocab(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 478,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique tokens in LABEL vocabulary: 4\n"
     ]
    }
   ],
   "source": [
    "# print(f\"Unique tokens in TEXT vocabulary: {len(TEXT.vocab)}\")\n",
    "print(f\"Unique tokens in LABEL vocabulary: {len(LABELS.vocab)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 479,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<function _default_unk_index at 0x7fe1fc769bf8>, {'Depression': 0, 'Alcohol': 1, 'Suicide': 2, 'Drugs': 3})\n"
     ]
    }
   ],
   "source": [
    "print(LABELS.vocab.stoi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 480,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(TEXT.vocab.freqs.most_common(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 481,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(TEXT.vocab.itos[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 482,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': [2129, 2000, 4468, 6544], 'label': 'Alcohol'}\n"
     ]
    }
   ],
   "source": [
    "print(vars(valid_data.examples[-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final step of preparing the data is creating the iterators. We iterate over these in the training/evaluation loop, and they return a batch of examples (indexed and converted into tensors) at each iteration.\n",
    "\n",
    "We'll use a BucketIterator which is a special type of iterator that will return a batch of examples where each example is of a similar length, minimizing the amount of padding per example.\n",
    "\n",
    "We also want to place the tensors returned by the iterator on the GPU (if you're using one). PyTorch handles this using torch.device, we then pass this device to the iterator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 483,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(\n",
    "    (train_data, valid_data, test_data), \n",
    "    batch_size = BATCH_SIZE,\n",
    "    sort=False,\n",
    "    device = device)\n",
    "\n",
    "# train_iterator, test_iterator = data.BucketIterator.splits(\n",
    "#     (train_data, test_data), \n",
    "#     batch_size = BATCH_SIZE,\n",
    "#     sort=False,\n",
    "#     device = device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 484,
   "metadata": {},
   "outputs": [],
   "source": [
    "# next(test_iterator.__iter__())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 485,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaModel, BertModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 486,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = RobertaModel.from_pretrained('roberta-base') # bert-base-cased, bert-large-cased\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "# model = TransfoXLModel.from_pretrained('transfo-xl-wt103')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 487,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class BERTLinearSentiment(nn.Module):\n",
    "#     def __init__(self,\n",
    "#                  bert,\n",
    "#                  hidden_dim,\n",
    "#                  output_dim,\n",
    "#                  n_layers,\n",
    "#                  bidirectional,\n",
    "#                  dropout):\n",
    "        \n",
    "#         super().__init__()\n",
    "        \n",
    "#         self.bert = bert\n",
    "        \n",
    "#         embedding_dim = bert.config.to_dict()['hidden_size']\n",
    "        \n",
    "#         self.lin_1 = nn.Linear(768, 500)\n",
    "#         self.linear_out = nn.Linear(500, output_dim)\n",
    "        \n",
    "#         self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "#     def forward(self, text):\n",
    "        \n",
    "#         #text = [batch size, sent len]\n",
    "                \n",
    "#         with torch.no_grad():\n",
    "# #             ipdb.set_trace()\n",
    "#             embedded = self.bert(text)[1]\n",
    "        \n",
    "#         output = self.dropout(self.linear_out(self.dropout(self.lin_1(embedded))))\n",
    "        \n",
    "#         #output = [batch size, out dim]\n",
    "        \n",
    "#         return F.sigmoid(output) #torch.sigmoid(output) # the output between 0 and 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTGRUSentiment(nn.Module):\n",
    "    def __init__(self,\n",
    "                 bert,\n",
    "                 hidden_dim,\n",
    "                 output_dim,\n",
    "                 n_layers,\n",
    "                 bidirectional,\n",
    "                 dropout):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.bert = bert\n",
    "        \n",
    "        embedding_dim = bert.config.to_dict()['hidden_size']\n",
    "        \n",
    "        self.rnn = nn.GRU(embedding_dim,\n",
    "                          hidden_dim,\n",
    "                          num_layers = n_layers,\n",
    "                          bidirectional = bidirectional,\n",
    "                          batch_first = True,\n",
    "                          dropout = 0 if n_layers < 2 else dropout)\n",
    "        \n",
    "        self.linear = nn.Linear(hidden_dim * 2 if bidirectional else hidden_dim, output_dim)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        \n",
    "    def forward(self, text):\n",
    "        \n",
    "        #text = [batch size, sent len]\n",
    "                \n",
    "#         ipdb.set_trace()\n",
    "        with torch.no_grad():\n",
    "#             ipdb.set_trace()\n",
    "            embedded = self.bert(text)[0]\n",
    "                \n",
    "#         embedded = [batch size, sent len, emb dim]\n",
    "                \n",
    "        _, hidden = self.rnn(embedded)\n",
    "        \n",
    "#         hidden = [n layers * n directions, batch size, emb dim]\n",
    "        \n",
    "        if self.rnn.bidirectional:\n",
    "            hidden = self.dropout(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1))\n",
    "        else:\n",
    "            hidden = self.dropout(hidden[-1,:,:])\n",
    "                \n",
    "        #hidden = [batch size, hid dim]\n",
    "#         ipdb.set_trace()\n",
    "        output = self.linear(hidden)\n",
    "                \n",
    "        #output = [batch size, out dim]\n",
    "        \n",
    "        return output # F.sigmoid(output) # the output between 0 and 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 489,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # can view the summary if you like\n",
    "# model.to(device)\n",
    "# summary(model, (16, 32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 490,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAX_VOCAB_SIZE = 25_000\n",
    "\n",
    "# TEXT.build_vocab(train_data, \n",
    "#                  max_size = MAX_VOCAB_SIZE, \n",
    "#                  vectors = \"glove.6B.100d\", \n",
    "#                  unk_init = torch.Tensor.normal_)\n",
    "\n",
    "# LABEL.build_vocab(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 491,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INPUT_DIM = len(TEXT.vocab)\n",
    "# EMBEDDING_DIM = 100\n",
    "# N_FILTERS = 100\n",
    "# FILTER_SIZES = [2,3,4]\n",
    "# OUTPUT_DIM = len(LABEL.vocab)\n",
    "# DROPOUT = 0.5\n",
    "# PAD_IDX = TEXT.vocab.stoi[TEXT.pad_token]\n",
    "\n",
    "# model = CNN(INPUT_DIM, EMBEDDING_DIM, N_FILTERS, FILTER_SIZES, OUTPUT_DIM, DROPOUT, PAD_IDX)\n",
    "\n",
    "HIDDEN_DIM = 500\n",
    "OUTPUT_DIM = len(LABELS.vocab)\n",
    "N_LAYERS = 2\n",
    "BIDIRECTIONAL = False\n",
    "DROPOUT = 0.3\n",
    "\n",
    "model = BERTGRUSentiment(model,\n",
    "                         HIDDEN_DIM,\n",
    "                         OUTPUT_DIM,\n",
    "                         N_LAYERS,\n",
    "                         BIDIRECTIONAL,\n",
    "                         DROPOUT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also create a function that will tell us how many trainable parameters our model has so we can compare the number of parameters across different models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 492,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 112,892,244 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 493,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, param in model.named_parameters():                \n",
    "    if name.startswith('bert'):\n",
    "        param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 494,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 3,410,004 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 495,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rnn.weight_ih_l0\n",
      "rnn.weight_hh_l0\n",
      "rnn.bias_ih_l0\n",
      "rnn.bias_hh_l0\n",
      "rnn.weight_ih_l1\n",
      "rnn.weight_hh_l1\n",
      "rnn.bias_ih_l1\n",
      "rnn.bias_hh_l1\n",
      "linear.weight\n",
      "linear.bias\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():                \n",
    "    if param.requires_grad:\n",
    "        print(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll load our pre-trained embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another different to the previous notebooks is our loss function (aka criterion). Before we used `BCEWithLogitsLoss`, however now we use `CrossEntropyLoss`. Without going into too much detail, `CrossEntropyLoss` performs a *softmax* function over our model outputs and the loss is given by the *cross entropy* between that and the label.\n",
    "\n",
    "Generally:\n",
    "- `CrossEntropyLoss` is used when our examples exclusively belong to one of $C$ classes\n",
    "- `BCEWithLogitsLoss` is used when our examples exclusively belong to only 2 classes (0 and 1) and is also used in the case where our examples belong to between 0 and $C$ classes (aka multilabel classification)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 496,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Depression', 352), ('Alcohol', 140), ('Suicide', 66), ('Drugs', 58)]"
      ]
     },
     "execution_count": 496,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "class_distrbution = Counter()\n",
    "for text in train[\"label\"].values:\n",
    "    class_distrbution[text] += 1\n",
    "        \n",
    "class_distrbution.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 497,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "616"
      ]
     },
     "execution_count": 497,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(list(class_distrbution.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 498,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'Alcohol': 140, 'Depression': 352, 'Drugs': 58, 'Suicide': 66})"
      ]
     },
     "execution_count": 498,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_distrbution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 499,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['Depression', 'Alcohol', 'Suicide', 'Drugs'])"
      ]
     },
     "execution_count": 499,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LABELS.vocab.stoi.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 500,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([1.0000, 2.5143, 5.3333, 6.0690]),\n",
       " Counter({'Alcohol': 140, 'Depression': 352, 'Drugs': 58, 'Suicide': 66}))"
      ]
     },
     "execution_count": 500,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# criterion = nn.CrossEntropyLoss(weight = x)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# train_loader_origin.dataset.classes,\n",
    "class_weights = [class_distrbution[i] for i in LABELS.vocab.stoi.keys()]\n",
    "class_weights_normalized = [max(class_weights)/i for i in class_weights]\n",
    "\n",
    "class_weights_normalized ,torch.Tensor(class_weights_normalized)\n",
    "\n",
    "w = torch.Tensor(class_weights_normalized)\n",
    "w = w.to(device)\n",
    "# x = x\n",
    "w,class_distrbution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 501,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([352, 140, 66, 58], tensor([1.0000, 2.5143, 5.3333, 6.0690]))"
      ]
     },
     "execution_count": 501,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_weights, w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 502,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optim.Adam?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 503,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# optimizer = optim.SGD(model.parameters(), lr = 1e-4, momentum=0.9, \n",
    "#                       weight_decay=0, dampening=0, nesterov=True)\n",
    "\n",
    "# optimizer = optim.Adam(model.parameters(), lr = 1e-4, betas=(0.9, 0.999), \n",
    "#                       weight_decay=0.0, amsgrad=False)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "# criterion = nn.CrossEntropyLoss(weight=w)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "# model = BERTGRUSentiment(model,\n",
    "#                          HIDDEN_DIM,\n",
    "#                          OUTPUT_DIM,\n",
    "#                          N_LAYERS,\n",
    "#                          BIDIRECTIONAL,\n",
    "#                          DROPOUT)\n",
    "\n",
    "model = model.to(device)\n",
    "criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before, we had a function that calculated accuracy in the binary label case, where we said if the value was over 0.5 then we would assume it is positive. In the case where we have more than 2 classes, our model outputs a $C$ dimensional vector, where the value of each element is the beleief that the example belongs to that class. \n",
    "\n",
    "For example, in our labels we have: 'HUM' = 0, 'ENTY' = 1, 'DESC' = 2, 'NUM' = 3, 'LOC' = 4 and 'ABBR' = 5. If the output of our model was something like: **[5.1, 0.3, 0.1, 2.1, 0.2, 0.6]** this means that the model strongly believes the example belongs to class 0, a question about a human, and slightly believes the example belongs to class 3, a numerical question.\n",
    "\n",
    "We calculate the accuracy by performing an `argmax` to get the index of the maximum value in the prediction for each element in the batch, and then counting how many times this equals the actual label. We then average this across the batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 504,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score\n",
    "\n",
    "def categorical_accuracy(preds, y):\n",
    "    \"\"\"\n",
    "    Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8, NOT 8\n",
    "    \"\"\"\n",
    "    max_preds = preds.argmax(dim = 1, keepdim = True) # get the index of the max probability\n",
    "    \n",
    "    correct = max_preds.squeeze(1).eq(y)\n",
    "#     ipdb.set_trace()\n",
    "    pr = correct.sum().item() / torch.FloatTensor([y.shape[0]])\n",
    "    \n",
    "#     pr = precision_score(y.detach().numpy(), \n",
    "#                            max_preds.argmax(dim = 1, \n",
    "#                                         keepdim = True).detach().numpy(), average='macro')\n",
    "#     ipdb.set_trace()\n",
    "    return pr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training loop is similar to before, without the need to `squeeze` the model predictions as `CrossEntropyLoss` expects the input to be **[batch size, n classes]** and the label to be **[batch size]**.\n",
    "\n",
    "The label needs to be a `LongTensor`, which it is by default as we did not set the `dtype` to a `FloatTensor` as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 505,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for batch in iterator:\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        \n",
    "        predictions = model(batch.text)\n",
    "#         ipdb.set_trace()\n",
    "        \n",
    "        loss = criterion(predictions, batch.label.long())\n",
    "        \n",
    "        acc = categorical_accuracy(predictions, batch.label.long())\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The evaluation loop is, again, similar to before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 506,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, criterion):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for batch in iterator:\n",
    "\n",
    "            predictions = model(batch.text)\n",
    "            \n",
    "            loss = criterion(predictions, batch.label.long())\n",
    "            \n",
    "            acc = categorical_accuracy(predictions, batch.label.long())\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 507,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we train our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 508,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Epoch Time: 0m 23s\n",
      "\tTrain Loss: 1.316 | Train Acc: 49.47%\n",
      "\t Val. Loss: 1.264 |  Val. Acc: 54.69%\n",
      "Epoch: 02 | Epoch Time: 0m 22s\n",
      "\tTrain Loss: 1.118 | Train Acc: 56.10%\n",
      "\t Val. Loss: 1.217 |  Val. Acc: 54.69%\n",
      "Epoch: 03 | Epoch Time: 0m 23s\n",
      "\tTrain Loss: 0.982 | Train Acc: 62.14%\n",
      "\t Val. Loss: 1.159 |  Val. Acc: 61.95%\n",
      "Epoch: 04 | Epoch Time: 0m 23s\n",
      "\tTrain Loss: 0.761 | Train Acc: 72.10%\n",
      "\t Val. Loss: 0.563 |  Val. Acc: 78.33%\n",
      "Epoch: 05 | Epoch Time: 0m 22s\n",
      "\tTrain Loss: 0.606 | Train Acc: 77.16%\n",
      "\t Val. Loss: 0.477 |  Val. Acc: 84.43%\n",
      "Epoch: 06 | Epoch Time: 0m 22s\n",
      "\tTrain Loss: 0.441 | Train Acc: 83.52%\n",
      "\t Val. Loss: 0.350 |  Val. Acc: 84.43%\n",
      "Epoch: 07 | Epoch Time: 0m 22s\n",
      "\tTrain Loss: 0.333 | Train Acc: 88.01%\n",
      "\t Val. Loss: 0.474 |  Val. Acc: 82.38%\n",
      "Epoch: 08 | Epoch Time: 0m 23s\n",
      "\tTrain Loss: 0.438 | Train Acc: 85.38%\n",
      "\t Val. Loss: 0.405 |  Val. Acc: 85.71%\n",
      "Epoch: 09 | Epoch Time: 0m 23s\n",
      "\tTrain Loss: 0.306 | Train Acc: 88.48%\n",
      "\t Val. Loss: 0.363 |  Val. Acc: 87.36%\n",
      "Epoch: 10 | Epoch Time: 0m 22s\n",
      "\tTrain Loss: 0.218 | Train Acc: 92.29%\n",
      "\t Val. Loss: 0.296 |  Val. Acc: 90.62%\n",
      "Saving Model ...\n",
      "*****************************************************\n",
      "best record: [epoch 9], [val loss 0.29598], [val acc 0.90625]\n",
      "*****************************************************\n"
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 10\n",
    "\n",
    "best_valid_loss = 0.30 #float('inf')\n",
    "best_valid_acc = 0.88\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "\n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_loss, train_acc = train(model, train_iterator, optimizer, criterion)\n",
    "    valid_loss, valid_acc = evaluate(model, valid_iterator, criterion)\n",
    "    \n",
    "    end_time = time.time()\n",
    "\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')\n",
    "    \n",
    "    if valid_loss < best_valid_loss and abs(valid_loss - best_valid_loss) < 1e-1:\n",
    "        best_valid_loss = valid_loss\n",
    "        print('Saving Model ...')\n",
    "        torch.save(model.state_dict(), 'Model_Bert_'+str(best_valid_loss)[:4]+'.pt')\n",
    "        print('*****************************************************')\n",
    "        print('best record: [epoch %d], [val loss %.5f], [val acc %.5f]' % (epoch, valid_loss, valid_acc))\n",
    "        print('*****************************************************')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Epoch: 01 | Epoch Time: 0m 24s\n",
    "# \tTrain Loss: 1.316 | Train Acc: 49.47%\n",
    "# \t Val. Loss: 1.264 |  Val. Acc: 54.69%\n",
    "# Epoch: 02 | Epoch Time: 0m 23s\n",
    "# \tTrain Loss: 1.118 | Train Acc: 56.10%\n",
    "# \t Val. Loss: 1.217 |  Val. Acc: 54.69%\n",
    "# Epoch: 03 | Epoch Time: 0m 24s\n",
    "# \tTrain Loss: 0.982 | Train Acc: 62.14%\n",
    "# \t Val. Loss: 1.159 |  Val. Acc: 61.95%\n",
    "# Epoch: 04 | Epoch Time: 0m 24s\n",
    "# \tTrain Loss: 0.761 | Train Acc: 72.10%\n",
    "# \t Val. Loss: 0.563 |  Val. Acc: 78.33%\n",
    "# Epoch: 05 | Epoch Time: 0m 24s\n",
    "# \tTrain Loss: 0.606 | Train Acc: 77.16%\n",
    "# \t Val. Loss: 0.477 |  Val. Acc: 84.43%\n",
    "# Epoch: 06 | Epoch Time: 0m 24s\n",
    "# \tTrain Loss: 0.441 | Train Acc: 83.52%\n",
    "# \t Val. Loss: 0.350 |  Val. Acc: 84.43%\n",
    "# Epoch: 07 | Epoch Time: 0m 23s\n",
    "# \tTrain Loss: 0.333 | Train Acc: 88.01%\n",
    "# \t Val. Loss: 0.474 |  Val. Acc: 82.38%\n",
    "# Epoch: 08 | Epoch Time: 0m 24s\n",
    "# \tTrain Loss: 0.438 | Train Acc: 85.38%\n",
    "# \t Val. Loss: 0.405 |  Val. Acc: 85.71%\n",
    "# Epoch: 09 | Epoch Time: 0m 24s\n",
    "# \tTrain Loss: 0.306 | Train Acc: 88.48%\n",
    "# \t Val. Loss: 0.363 |  Val. Acc: 87.36%\n",
    "# Epoch: 10 | Epoch Time: 0m 23s\n",
    "# \tTrain Loss: 0.218 | Train Acc: 92.29%\n",
    "# \t Val. Loss: 0.296 |  Val. Acc: 90.62%\n",
    "# Epoch: 11 | Epoch Time: 0m 23s\n",
    "# \tTrain Loss: 0.160 | Train Acc: 94.73%\n",
    "# \t Val. Loss: 0.271 |  Val. Acc: 90.48%\n",
    "# Epoch: 12 | Epoch Time: 0m 24s\n",
    "# \tTrain Loss: 0.132 | Train Acc: 95.42%\n",
    "# \t Val. Loss: 0.416 |  Val. Acc: 82.03%\n",
    "# Epoch: 13 | Epoch Time: 0m 24s\n",
    "# \tTrain Loss: 0.102 | Train Acc: 95.91%\n",
    "# \t Val. Loss: 0.345 |  Val. Acc: 89.70%\n",
    "# Epoch: 14 | Epoch Time: 0m 24s\n",
    "# \tTrain Loss: 0.060 | Train Acc: 97.37%\n",
    "# \t Val. Loss: 0.474 |  Val. Acc: 85.79%\n",
    "# Epoch: 15 | Epoch Time: 0m 23s\n",
    "# \tTrain Loss: 0.062 | Train Acc: 96.98%\n",
    "# \t Val. Loss: 0.400 |  Val. Acc: 89.06%\n",
    "# Epoch: 16 | Epoch Time: 0m 23s\n",
    "# \tTrain Loss: 0.072 | Train Acc: 96.78%\n",
    "# \t Val. Loss: 0.316 |  Val. Acc: 89.55%\n",
    "# Epoch: 17 | Epoch Time: 0m 23s\n",
    "# \tTrain Loss: 0.087 | Train Acc: 97.07%\n",
    "# \t Val. Loss: 0.335 |  Val. Acc: 90.62%\n",
    "# Epoch: 18 | Epoch Time: 0m 23s\n",
    "# \tTrain Loss: 0.102 | Train Acc: 96.59%\n",
    "# \t Val. Loss: 0.409 |  Val. Acc: 88.77%\n",
    "# Epoch: 19 | Epoch Time: 0m 24s\n",
    "# \tTrain Loss: 0.049 | Train Acc: 98.83%\n",
    "# \t Val. Loss: 0.269 |  Val. Acc: 89.55%\n",
    "# Epoch: 20 | Epoch Time: 0m 23s\n",
    "# \tTrain Loss: 0.030 | Train Acc: 99.22%\n",
    "# \t Val. Loss: 0.364 |  Val. Acc: 91.12%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 382,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abs(0.218-0.296) < 1e-1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTGRUSentiment(nn.Module):\n",
    "    def __init__(self,\n",
    "                 bert,\n",
    "                 hidden_dim,\n",
    "                 output_dim,\n",
    "                 n_layers,\n",
    "                 bidirectional,\n",
    "                 dropout):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.bert = bert\n",
    "        \n",
    "        embedding_dim = bert.config.to_dict()['hidden_size']\n",
    "        \n",
    "        self.rnn = nn.GRU(embedding_dim,\n",
    "                          hidden_dim,\n",
    "                          num_layers = n_layers,\n",
    "                          bidirectional = bidirectional,\n",
    "                          batch_first = True,\n",
    "                          dropout = 0 if n_layers < 2 else dropout)\n",
    "        \n",
    "        self.linear = nn.Linear(hidden_dim * 2 if bidirectional else hidden_dim, output_dim)\n",
    "        \n",
    "#         self.linear2 = nn.Linear(500, output_dim)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        \n",
    "    def forward(self, text):\n",
    "        \n",
    "        #text = [batch size, sent len]\n",
    "                \n",
    "#         ipdb.set_trace()\n",
    "        with torch.no_grad():\n",
    "#             ipdb.set_trace()\n",
    "            embedded = self.bert(text)[0]\n",
    "                \n",
    "#         embedded = [batch size, sent len, emb dim]\n",
    "                \n",
    "        _, hidden = self.rnn(embedded)\n",
    "        \n",
    "#         hidden = [n layers * n directions, batch size, emb dim]\n",
    "        \n",
    "        if self.rnn.bidirectional:\n",
    "            hidden = self.dropout(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1))\n",
    "        else:\n",
    "            hidden = self.dropout(hidden[-1,:,:])\n",
    "                \n",
    "        #hidden = [batch size, hid dim]\n",
    "#         ipdb.set_trace()\n",
    "        output = self.linear(hidden)\n",
    "                \n",
    "        # output = [batch size, out dim]\n",
    "        \n",
    "#         output = self.sigmoid(output)\n",
    "        \n",
    "        return output #torch.softmax(output, dim=1) # # the output between 0 and 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 536,
   "metadata": {},
   "outputs": [],
   "source": [
    "class load_data(object):\n",
    "    def __init__(self, SEED=1234):\n",
    "        torch.manual_seed(SEED)\n",
    "        torch.cuda.manual_seed(SEED)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "\n",
    "        tokenizer = RobertaTokenizer.from_pretrained('roberta-base') # roberta-base, bert-base-uncased\n",
    "#         tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "        \n",
    "        init_token = tokenizer.cls_token\n",
    "        eos_token = tokenizer.sep_token\n",
    "        pad_token = tokenizer.pad_token\n",
    "        unk_token = tokenizer.unk_token\n",
    "        \n",
    "        init_token_idx = tokenizer.convert_tokens_to_ids(init_token)\n",
    "        eos_token_idx = tokenizer.convert_tokens_to_ids(eos_token)\n",
    "        pad_token_idx = tokenizer.convert_tokens_to_ids(pad_token)\n",
    "        unk_token_idx = tokenizer.convert_tokens_to_ids(unk_token)\n",
    "        \n",
    "        LABELS = data.LabelField(dtype = torch.float)\n",
    "\n",
    "        TEXT = data.Field(batch_first = True,\n",
    "                          use_vocab = False,\n",
    "        #                   tokenize = tokenize_and_cut,\n",
    "                          preprocessing = tokenizer.convert_tokens_to_ids,\n",
    "                          init_token = init_token_idx,\n",
    "                          eos_token = eos_token_idx,\n",
    "                          pad_token = pad_token_idx,\n",
    "                          unk_token = unk_token_idx)\n",
    "\n",
    "\n",
    "#         self.train_data, self.test_data = datasets.IMDB.splits(TEXT, LABEL)\n",
    "        \n",
    "        self.train_data, self.test_data = data.TabularDataset.splits(\n",
    "            path='./data', train='Train_process.csv',\n",
    "            test='Test_process.csv', format='csv', skip_header=True,\n",
    "            fields=[('ID', None),\n",
    "                    ('text', TEXT),\n",
    "                    ('label', LABELS)])\n",
    "\n",
    "        self.SEED = SEED\n",
    "\n",
    "\n",
    "    def get_fold_data(self, num_folds=10):\n",
    "        \"\"\"\n",
    "        More details about 'fields' are available at \n",
    "        https://github.com/pytorch/text/blob/master/torchtext/datasets/imdb.py\n",
    "        \"\"\"\n",
    "\n",
    "#         tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "        tokenizer = RobertaTokenizer.from_pretrained('roberta-base') # roberta-base, bert-base-uncased\n",
    "        \n",
    "        init_token = tokenizer.cls_token\n",
    "        eos_token = tokenizer.sep_token\n",
    "        pad_token = tokenizer.pad_token\n",
    "        unk_token = tokenizer.unk_token\n",
    "        \n",
    "        init_token_idx = tokenizer.convert_tokens_to_ids(init_token)\n",
    "        eos_token_idx = tokenizer.convert_tokens_to_ids(eos_token)\n",
    "        pad_token_idx = tokenizer.convert_tokens_to_ids(pad_token)\n",
    "        unk_token_idx = tokenizer.convert_tokens_to_ids(unk_token)\n",
    "        \n",
    "        LABELS = data.LabelField(dtype = torch.float)\n",
    "\n",
    "        TEXT = data.Field(batch_first = True,\n",
    "                          use_vocab = False,\n",
    "        #                   tokenize = tokenize_and_cut,\n",
    "                          preprocessing = tokenizer.convert_tokens_to_ids,\n",
    "                          init_token = init_token_idx,\n",
    "                          eos_token = eos_token_idx,\n",
    "                          pad_token = pad_token_idx,\n",
    "                          unk_token = unk_token_idx)\n",
    "\n",
    "        fields = [('text', TEXT), ('label', LABELS)]\n",
    "        \n",
    "#         kf = KFold(n_splits=num_folds, shuffle=True, random_state=self.SEED)\n",
    "        kf = StratifiedKFold(n_splits=num_folds, random_state=self.SEED)\n",
    "        \n",
    "        train_data_arr = np.array(self.train_data.examples)\n",
    "\n",
    "#         ipdb.set_trace()\n",
    "        for train_index, val_index in kf.split(train_data_arr, y_kfold):\n",
    "            yield(\n",
    "                TEXT,\n",
    "                LABELS,\n",
    "                data.Dataset(train_data_arr[train_index], fields=fields),\n",
    "                data.Dataset(train_data_arr[val_index], fields=fields),\n",
    "            )\n",
    "    \n",
    "    def get_test_data(self):\n",
    "        return self.test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 537,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py:296: FutureWarning:\n",
      "\n",
      "Setting a random_state has no effect since shuffle is False. This will raise an error in 0.24. You should leave random_state to its default (None), or set shuffle=True.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** Running Training *****\n",
      "Now fold: 1 / 5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8fc92be175ea4f2990043132adf52a11",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=481.0, style=ProgressStyle(description_"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c47de5dfd2245328a4c209df206b205",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=501200538.0, style=ProgressStyle(descri"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "| Epoch: 01 | Train Loss: 1.191 | Train Acc: 53.19%\n",
      "| Epoch: 01 | Valid Loss: 1.009 | Valid Acc: 63.28%\n",
      "################################################################################\n",
      "| Epoch: 02 | Train Loss: 1.010 | Train Acc: 60.16%\n",
      "| Epoch: 02 | Valid Loss: 1.036 | Valid Acc: 63.17%\n",
      "################################################################################\n",
      "| Epoch: 03 | Train Loss: 1.069 | Train Acc: 57.88%\n",
      "| Epoch: 03 | Valid Loss: 1.249 | Valid Acc: 46.32%\n",
      "################################################################################\n",
      "| Epoch: 04 | Train Loss: 1.038 | Train Acc: 58.20%\n",
      "| Epoch: 04 | Valid Loss: 0.991 | Valid Acc: 61.38%\n",
      "################################################################################\n",
      "| Epoch: 05 | Train Loss: 0.908 | Train Acc: 62.04%\n",
      "| Epoch: 05 | Valid Loss: 0.975 | Valid Acc: 61.16%\n",
      "################################################################################\n",
      "| Epoch: 06 | Train Loss: 0.887 | Train Acc: 64.78%\n",
      "| Epoch: 06 | Valid Loss: 0.923 | Valid Acc: 63.50%\n",
      "################################################################################\n",
      "| Epoch: 07 | Train Loss: 0.845 | Train Acc: 68.03%\n",
      "| Epoch: 07 | Valid Loss: 0.846 | Valid Acc: 68.53%\n",
      "################################################################################\n",
      "| Epoch: 08 | Train Loss: 0.833 | Train Acc: 67.90%\n",
      "| Epoch: 08 | Valid Loss: 0.821 | Valid Acc: 65.40%\n",
      "################################################################################\n",
      "| Epoch: 09 | Train Loss: 0.749 | Train Acc: 70.64%\n",
      "| Epoch: 09 | Valid Loss: 1.086 | Valid Acc: 59.93%\n",
      "################################################################################\n",
      "| Epoch: 10 | Train Loss: 0.814 | Train Acc: 66.99%\n",
      "| Epoch: 10 | Valid Loss: 0.862 | Valid Acc: 65.51%\n",
      "################################################################################\n",
      "Val. Loss: 0.827 | Val. Acc: 67.86% |\n",
      "***** Running Training *****\n",
      "Now fold: 2 / 5\n",
      "| Epoch: 01 | Train Loss: 1.207 | Train Acc: 53.85%\n",
      "| Epoch: 01 | Valid Loss: 1.111 | Valid Acc: 57.00%\n",
      "################################################################################\n",
      "| Epoch: 02 | Train Loss: 1.054 | Train Acc: 59.71%\n",
      "| Epoch: 02 | Valid Loss: 1.287 | Valid Acc: 54.02%\n",
      "################################################################################\n",
      "| Epoch: 03 | Train Loss: 1.031 | Train Acc: 62.83%\n",
      "| Epoch: 03 | Valid Loss: 1.013 | Valid Acc: 59.35%\n",
      "################################################################################\n",
      "| Epoch: 04 | Train Loss: 0.947 | Train Acc: 64.96%\n",
      "| Epoch: 04 | Valid Loss: 1.160 | Valid Acc: 59.64%\n",
      "################################################################################\n",
      "| Epoch: 05 | Train Loss: 0.955 | Train Acc: 64.89%\n",
      "| Epoch: 05 | Valid Loss: 0.922 | Valid Acc: 62.33%\n",
      "################################################################################\n",
      "| Epoch: 06 | Train Loss: 0.939 | Train Acc: 65.47%\n",
      "| Epoch: 06 | Valid Loss: 0.876 | Valid Acc: 64.81%\n",
      "################################################################################\n",
      "| Epoch: 07 | Train Loss: 0.912 | Train Acc: 65.17%\n",
      "| Epoch: 07 | Valid Loss: 0.805 | Valid Acc: 70.14%\n",
      "################################################################################\n",
      "| Epoch: 08 | Train Loss: 0.790 | Train Acc: 70.43%\n",
      "| Epoch: 08 | Valid Loss: 0.888 | Valid Acc: 67.30%\n",
      "################################################################################\n",
      "| Epoch: 09 | Train Loss: 0.781 | Train Acc: 70.15%\n",
      "| Epoch: 09 | Valid Loss: 0.811 | Valid Acc: 68.52%\n",
      "################################################################################\n",
      "| Epoch: 10 | Train Loss: 0.733 | Train Acc: 71.11%\n",
      "| Epoch: 10 | Valid Loss: 0.760 | Valid Acc: 70.43%\n",
      "################################################################################\n",
      "Val. Loss: 0.788 | Val. Acc: 73.21% |\n",
      "***** Running Training *****\n",
      "Now fold: 3 / 5\n",
      "| Epoch: 01 | Train Loss: 1.211 | Train Acc: 49.19%\n",
      "| Epoch: 01 | Valid Loss: 1.123 | Valid Acc: 61.98%\n",
      "################################################################################\n",
      "| Epoch: 02 | Train Loss: 1.109 | Train Acc: 55.30%\n",
      "| Epoch: 02 | Valid Loss: 1.024 | Valid Acc: 57.00%\n",
      "################################################################################\n",
      "| Epoch: 03 | Train Loss: 1.069 | Train Acc: 59.89%\n",
      "| Epoch: 03 | Valid Loss: 1.231 | Valid Acc: 49.33%\n",
      "################################################################################\n",
      "| Epoch: 04 | Train Loss: 0.995 | Train Acc: 61.55%\n",
      "| Epoch: 04 | Valid Loss: 0.925 | Valid Acc: 62.12%\n",
      "################################################################################\n",
      "| Epoch: 05 | Train Loss: 0.956 | Train Acc: 63.82%\n",
      "| Epoch: 05 | Valid Loss: 0.911 | Valid Acc: 62.12%\n",
      "################################################################################\n",
      "| Epoch: 06 | Train Loss: 0.923 | Train Acc: 61.37%\n",
      "| Epoch: 06 | Valid Loss: 0.926 | Valid Acc: 64.32%\n",
      "################################################################################\n",
      "| Epoch: 07 | Train Loss: 0.855 | Train Acc: 66.35%\n",
      "| Epoch: 07 | Valid Loss: 0.834 | Valid Acc: 71.93%\n",
      "################################################################################\n",
      "| Epoch: 08 | Train Loss: 0.819 | Train Acc: 68.00%\n",
      "| Epoch: 08 | Valid Loss: 0.761 | Valid Acc: 69.85%\n",
      "################################################################################\n",
      "| Epoch: 09 | Train Loss: 0.775 | Train Acc: 70.33%\n",
      "| Epoch: 09 | Valid Loss: 0.769 | Valid Acc: 72.71%\n",
      "################################################################################\n",
      "| Epoch: 10 | Train Loss: 0.782 | Train Acc: 71.11%\n",
      "| Epoch: 10 | Valid Loss: 0.714 | Valid Acc: 72.71%\n",
      "################################################################################\n",
      "Val. Loss: 0.708 | Val. Acc: 73.06% |\n",
      "***** Running Training *****\n",
      "Now fold: 4 / 5\n",
      "| Epoch: 01 | Train Loss: 1.246 | Train Acc: 50.83%\n",
      "| Epoch: 01 | Valid Loss: 1.103 | Valid Acc: 57.00%\n",
      "################################################################################\n",
      "| Epoch: 02 | Train Loss: 1.130 | Train Acc: 53.88%\n",
      "| Epoch: 02 | Valid Loss: 1.059 | Valid Acc: 56.86%\n",
      "################################################################################\n",
      "| Epoch: 03 | Train Loss: 1.071 | Train Acc: 59.50%\n",
      "| Epoch: 03 | Valid Loss: 1.307 | Valid Acc: 42.56%\n",
      "################################################################################\n",
      "| Epoch: 04 | Train Loss: 1.009 | Train Acc: 61.55%\n",
      "| Epoch: 04 | Valid Loss: 0.911 | Valid Acc: 64.47%\n",
      "################################################################################\n",
      "| Epoch: 05 | Train Loss: 0.986 | Train Acc: 60.01%\n",
      "| Epoch: 05 | Valid Loss: 1.061 | Valid Acc: 55.79%\n",
      "################################################################################\n",
      "| Epoch: 06 | Train Loss: 0.876 | Train Acc: 63.72%\n",
      "| Epoch: 06 | Valid Loss: 0.910 | Valid Acc: 65.89%\n",
      "################################################################################\n",
      "| Epoch: 07 | Train Loss: 0.849 | Train Acc: 67.13%\n",
      "| Epoch: 07 | Valid Loss: 1.031 | Valid Acc: 57.64%\n",
      "################################################################################\n",
      "| Epoch: 08 | Train Loss: 0.835 | Train Acc: 67.04%\n",
      "| Epoch: 08 | Valid Loss: 1.059 | Valid Acc: 55.79%\n",
      "################################################################################\n",
      "| Epoch: 09 | Train Loss: 0.783 | Train Acc: 70.43%\n",
      "| Epoch: 09 | Valid Loss: 0.902 | Valid Acc: 57.00%\n",
      "################################################################################\n",
      "| Epoch: 10 | Train Loss: 0.770 | Train Acc: 71.41%\n",
      "| Epoch: 10 | Valid Loss: 1.143 | Valid Acc: 60.91%\n",
      "################################################################################\n",
      "Val. Loss: 1.156 | Val. Acc: 60.76% |\n",
      "***** Running Training *****\n",
      "Now fold: 5 / 5\n",
      "| Epoch: 01 | Train Loss: 1.211 | Train Acc: 52.87%\n",
      "| Epoch: 01 | Valid Loss: 1.084 | Valid Acc: 56.71%\n",
      "################################################################################\n",
      "| Epoch: 02 | Train Loss: 1.110 | Train Acc: 56.61%\n",
      "| Epoch: 02 | Valid Loss: 1.017 | Valid Acc: 64.47%\n",
      "################################################################################\n",
      "| Epoch: 03 | Train Loss: 1.000 | Train Acc: 60.97%\n",
      "| Epoch: 03 | Valid Loss: 1.203 | Valid Acc: 50.17%\n",
      "################################################################################\n",
      "| Epoch: 04 | Train Loss: 1.071 | Train Acc: 58.34%\n",
      "| Epoch: 04 | Valid Loss: 0.959 | Valid Acc: 62.67%\n",
      "################################################################################\n",
      "| Epoch: 05 | Train Loss: 0.914 | Train Acc: 64.39%\n",
      "| Epoch: 05 | Valid Loss: 1.281 | Valid Acc: 59.69%\n",
      "################################################################################\n",
      "| Epoch: 06 | Train Loss: 0.960 | Train Acc: 64.41%\n",
      "| Epoch: 06 | Valid Loss: 0.854 | Valid Acc: 69.59%\n",
      "################################################################################\n",
      "| Epoch: 07 | Train Loss: 0.833 | Train Acc: 69.07%\n",
      "| Epoch: 07 | Valid Loss: 1.083 | Valid Acc: 55.44%\n",
      "################################################################################\n",
      "| Epoch: 08 | Train Loss: 0.870 | Train Acc: 67.04%\n",
      "| Epoch: 08 | Valid Loss: 0.776 | Valid Acc: 71.30%\n",
      "################################################################################\n",
      "| Epoch: 09 | Train Loss: 0.787 | Train Acc: 70.45%\n",
      "| Epoch: 09 | Valid Loss: 0.922 | Valid Acc: 65.10%\n",
      "################################################################################\n",
      "| Epoch: 10 | Train Loss: 0.756 | Train Acc: 74.04%\n",
      "| Epoch: 10 | Valid Loss: 0.735 | Valid Acc: 68.66%\n",
      "################################################################################\n",
      "Val. Loss: 0.761 | Val. Acc: 70.57% |\n",
      "***** Cross Validation Result *****\n",
      "LOSS: 0.8478997617959976, ACC: 0.6909226238727569\n"
     ]
    }
   ],
   "source": [
    "data_generator = load_data()\n",
    "_history = []\n",
    "device = None\n",
    "model = None\n",
    "criterion = None\n",
    "fold_index = 0\n",
    "num_folds = 5\n",
    "batch_size = 32\n",
    "epochs = 10\n",
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "# optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "best_valid_acc = 0.9\n",
    "best_val_loss = 0.29\n",
    "\n",
    "for TEXT, LABELS, train_data, val_data in data_generator.get_fold_data(num_folds=num_folds):\n",
    "    print(\"***** Running Training *****\")\n",
    "    print(f\"Now fold: {fold_index + 1} / {num_folds}\")\n",
    "\n",
    "#     print(f'Embedding size: {TEXT.vocab.vectors.size()}.')\n",
    "    LABELS.build_vocab(train_data) # For converting str into float labels.\n",
    "\n",
    "#     Model(len(TEXT.vocab), embedding_dim, hidden_dim,\n",
    "#         output_dim, num_layers, dropout, TEXT.vocab.vectors, embedding_trainable)\n",
    "\n",
    "    HIDDEN_DIM = 500\n",
    "    OUTPUT_DIM = len(LABELS.vocab)\n",
    "    N_LAYERS = 2\n",
    "    BIDIRECTIONAL = False\n",
    "    DROPOUT = 0.3\n",
    "    \n",
    "#     model = BertModel.from_pretrained('bert-base-uncased')\n",
    "    model = RobertaModel.from_pretrained('roberta-base') # bert-base-cased, bert-large-cased\n",
    "\n",
    "    model = BERTGRUSentiment(model,\n",
    "                     HIDDEN_DIM,\n",
    "                     OUTPUT_DIM,\n",
    "                     N_LAYERS,\n",
    "                     BIDIRECTIONAL,\n",
    "                     DROPOUT)\n",
    "\n",
    "    \n",
    "    for name, param in model.named_parameters():                \n",
    "        if name.startswith('bert'):\n",
    "            param.requires_grad = False\n",
    "        \n",
    "#     def count_parameters(model):\n",
    "#         return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "#     print(f'The model has {count_parameters(model):,} trainable parameters')\n",
    "\n",
    "    # Get w\n",
    "    from collections import Counter\n",
    "    class_distrbution = Counter()\n",
    "    for text in y_kfold:\n",
    "        class_distrbution[text] += 1\n",
    "\n",
    "\n",
    "    class_weights = [class_distrbution[i] for i in LABELS.vocab.stoi.keys()]\n",
    "    class_weights_normalized = [max(class_weights)/i for i in class_weights]\n",
    "\n",
    "\n",
    "    w = torch.Tensor(class_weights_normalized)\n",
    "    w = w.to(device)\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters())\n",
    "#         criterion = nn.CrossEntropyLoss(weight=w)\n",
    "\n",
    "#     optimizer = optim.SGD(model.parameters(), lr = lr, momentum=momentum, \n",
    "#                   weight_decay=0, dampening=0, nesterov=False)\n",
    "\n",
    "#     optimizer = optim.Adam(model.parameters(), lr = 1e-3, betas=(0.9, 0.999), \n",
    "#                   weight_decay=0.0, amsgrad=False)\n",
    "\n",
    "#     criterion = nn.CrossEntropyLoss(weight = w)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = model.to(device)\n",
    "    criterion = criterion.to(device)\n",
    "\n",
    "    train_iterator = data.Iterator(train_data, batch_size=batch_size, sort_key=lambda x: len(x.text), device=device)\n",
    "    val_iterator = data.Iterator(val_data, batch_size=batch_size, sort_key=lambda x: len(x.text), device=device)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        train_loss, train_acc = train(model, train_iterator, optimizer, criterion)\n",
    "        val_loss, val_acc = evaluate(model, val_iterator, criterion)\n",
    "        print(f'| Epoch: {epoch+1:02} | Train Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
    "        print(f'| Epoch: {epoch+1:02} | Valid Loss: {val_loss:.3f} | Valid Acc: {val_acc*100:.2f}%')\n",
    "        print('################################################################################')\n",
    "        if val_loss < best_val_loss and abs(valid_loss - best_valid_loss) < 1e-1:\n",
    "            best_val_loss = val_loss\n",
    "            print('Saving Model ...')\n",
    "            torch.save(model.state_dict(), 'Best_Model_'+str(best_val_loss)+'.pt')\n",
    "            print('*****************************************************')\n",
    "            print('best record: [epoch %d], [val loss %.5f], [val acc %.5f]' % (epoch, val_loss, val_acc))\n",
    "            print('*****************************************************')\n",
    "\n",
    "    val_loss, val_acc = evaluate(model, val_iterator, criterion) \n",
    "\n",
    "    if val_loss < best_val_loss and abs(valid_loss - best_valid_loss) < 1e-1:\n",
    "        best_val_loss = val_loss\n",
    "        print('Saving Model ...')\n",
    "        torch.save(model.state_dict(), 'Best_Model_'+str(best_val_loss)+'.pt')\n",
    "        print('*****************************************************')\n",
    "        print('best record: [epoch %d], [val loss %.5f], [val acc %.5f]' % (epoch, val_loss, val_acc))\n",
    "        print('*****************************************************')\n",
    "\n",
    "#     if val_acc > best_valid_acc:\n",
    "#         best_valid_acc = val_acc\n",
    "#         print('Saving Model ...')\n",
    "#         torch.save(model.state_dict(), 'Best_Model_'+str(val_acc)+'.pt')\n",
    "#         print('*****************************************************')\n",
    "#         print('best record: [epoch %d], [val loss %.5f], [val acc %.5f]' % (epoch, val_loss, val_acc))\n",
    "#         print('*****************************************************')\n",
    "\n",
    "    print(f'Val. Loss: {val_loss:.3f} | Val. Acc: {val_acc*100:.2f}% |')\n",
    "\n",
    "    _history.append([val_loss, val_acc])\n",
    "    fold_index += 1\n",
    "\n",
    "_history = np.asarray(_history)\n",
    "loss = np.mean(_history[:, 0])\n",
    "acc = np.mean(_history[:, 1])\n",
    "\n",
    "print('***** Cross Validation Result *****')\n",
    "print(f'LOSS: {loss}, ACC: {acc}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOSS: 0.47000654861330987, ACC: 0.839839619398117"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyper-Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "class load_data(object):\n",
    "    def __init__(self, SEED=1234):\n",
    "        torch.manual_seed(SEED)\n",
    "        torch.cuda.manual_seed(SEED)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "\n",
    "        tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "        \n",
    "        init_token = tokenizer.cls_token\n",
    "        eos_token = tokenizer.sep_token\n",
    "        pad_token = tokenizer.pad_token\n",
    "        unk_token = tokenizer.unk_token\n",
    "        \n",
    "        init_token_idx = tokenizer.convert_tokens_to_ids(init_token)\n",
    "        eos_token_idx = tokenizer.convert_tokens_to_ids(eos_token)\n",
    "        pad_token_idx = tokenizer.convert_tokens_to_ids(pad_token)\n",
    "        unk_token_idx = tokenizer.convert_tokens_to_ids(unk_token)\n",
    "        \n",
    "        LABELS = data.LabelField(dtype = torch.float)\n",
    "\n",
    "        TEXT = data.Field(batch_first = True,\n",
    "                          use_vocab = False,\n",
    "        #                   tokenize = tokenize_and_cut,\n",
    "                          preprocessing = tokenizer.convert_tokens_to_ids,\n",
    "                          init_token = init_token_idx,\n",
    "                          eos_token = eos_token_idx,\n",
    "                          pad_token = pad_token_idx,\n",
    "                          unk_token = unk_token_idx)\n",
    "\n",
    "\n",
    "#         self.train_data, self.test_data = datasets.IMDB.splits(TEXT, LABEL)\n",
    "        \n",
    "        self.train_data, self.test_data = data.TabularDataset.splits(\n",
    "            path='./data', train='Train_process.csv',\n",
    "            test='Test_process.csv', format='csv', skip_header=True,\n",
    "            fields=[('ID', None),\n",
    "                    ('text', TEXT),\n",
    "                    ('label', LABELS)])\n",
    "\n",
    "        self.SEED = SEED\n",
    "\n",
    "\n",
    "    def get_fold_data(self, num_folds=10):\n",
    "        \"\"\"\n",
    "        More details about 'fields' are available at \n",
    "        https://github.com/pytorch/text/blob/master/torchtext/datasets/imdb.py\n",
    "        \"\"\"\n",
    "\n",
    "        tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "        \n",
    "        init_token = tokenizer.cls_token\n",
    "        eos_token = tokenizer.sep_token\n",
    "        pad_token = tokenizer.pad_token\n",
    "        unk_token = tokenizer.unk_token\n",
    "        \n",
    "        init_token_idx = tokenizer.convert_tokens_to_ids(init_token)\n",
    "        eos_token_idx = tokenizer.convert_tokens_to_ids(eos_token)\n",
    "        pad_token_idx = tokenizer.convert_tokens_to_ids(pad_token)\n",
    "        unk_token_idx = tokenizer.convert_tokens_to_ids(unk_token)\n",
    "        \n",
    "        LABELS = data.LabelField(dtype = torch.float)\n",
    "\n",
    "        TEXT = data.Field(batch_first = True,\n",
    "                          use_vocab = False,\n",
    "        #                   tokenize = tokenize_and_cut,\n",
    "                          preprocessing = tokenizer.convert_tokens_to_ids,\n",
    "                          init_token = init_token_idx,\n",
    "                          eos_token = eos_token_idx,\n",
    "                          pad_token = pad_token_idx,\n",
    "                          unk_token = unk_token_idx)\n",
    "\n",
    "        fields = [('text', TEXT), ('label', LABELS)]\n",
    "        \n",
    "#         kf = KFold(n_splits=num_folds, shuffle=True, random_state=self.SEED)\n",
    "        kf = StratifiedKFold(n_splits=num_folds, random_state=self.SEED)\n",
    "        \n",
    "        train_data_arr = np.array(self.train_data.examples)\n",
    "\n",
    "#         ipdb.set_trace()\n",
    "        for train_index, val_index in kf.split(train_data_arr, y_kfold):\n",
    "            yield(\n",
    "                TEXT,\n",
    "                LABELS,\n",
    "                data.Dataset(train_data_arr[train_index], fields=fields),\n",
    "                data.Dataset(train_data_arr[val_index], fields=fields),\n",
    "            )\n",
    "    \n",
    "    def get_test_data(self):\n",
    "        return self.test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best_loss_val = 0.8\n",
    "\n",
    "def objective(trial):\n",
    "    \n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')    \n",
    "    \n",
    "    \n",
    "    lr  = trial.suggest_loguniform('lr', 1e-5, 1e-1)\n",
    "#     optim_ = trial.suggest_categorical('optim_',[optim.SGD, optim.RMSprop,optim.Adam])\n",
    "    \n",
    "    \n",
    "    momentum = trial.suggest_uniform('momentum', 0, 0.9)\n",
    "    \n",
    "    \n",
    "    \n",
    "    data_generator = load_data()\n",
    "    _history = []\n",
    "    device = None\n",
    "    model = BertModel.from_pretrained('bert-base-uncased')\n",
    "    criterion = None\n",
    "    fold_index = 0\n",
    "    num_folds = 5\n",
    "    batch_size = 32\n",
    "    epochs = 20\n",
    "\n",
    "    import torch.optim as optim\n",
    "\n",
    "    # optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "    best_valid_acc = 0.9\n",
    "    best_val_loss = 0.2\n",
    "\n",
    "    for TEXT, LABELS, train_data, val_data in data_generator.get_fold_data(num_folds=num_folds):\n",
    "#         print(\"***** Running Training *****\")\n",
    "#         print(f\"Now fold: {fold_index + 1} / {num_folds}\"\n",
    "\n",
    "#         print(f'Embedding size: {TEXT.vocab.vectors.size()}.')\n",
    "        LABELS.build_vocab(train_data) # For converting str into float labels.\n",
    "\n",
    "    #     Model(len(TEXT.vocab), embedding_dim, hidden_dim,\n",
    "    #         output_dim, num_layers, dropout, TEXT.vocab.vectors, embedding_trainable)\n",
    "\n",
    "        HIDDEN_DIM = 500\n",
    "        OUTPUT_DIM = len(LABELS.vocab)\n",
    "        N_LAYERS = 2\n",
    "        BIDIRECTIONAL = True\n",
    "        DROPOUT = 0.4\n",
    "\n",
    "        model = BertModel.from_pretrained('bert-base-uncased')\n",
    "        model = BERTGRUSentiment(model,\n",
    "                     HIDDEN_DIM,\n",
    "                     OUTPUT_DIM,\n",
    "                     N_LAYERS,\n",
    "                     BIDIRECTIONAL,\n",
    "                     DROPOUT)\n",
    "\n",
    "\n",
    "        # Get w\n",
    "        from collections import Counter\n",
    "        class_distrbution = Counter()\n",
    "        for text in y_kfold:\n",
    "            class_distrbution[text] += 1\n",
    "\n",
    "\n",
    "        class_weights = [class_distrbution[i] for i in LABELS.vocab.stoi.keys()]\n",
    "        class_weights_normalized = [max(class_weights)/i for i in class_weights]\n",
    "\n",
    "\n",
    "        w = torch.Tensor(class_weights_normalized)\n",
    "        w = w.to(device)\n",
    "\n",
    "#         optimizer = optim.Adam(model.parameters())\n",
    "#         criterion = nn.CrossEntropyLoss(weight=w)\n",
    "        \n",
    "        optimizer = optim.SGD(model.parameters(), lr = lr, momentum=momentum, \n",
    "                      weight_decay=0, dampening=0, nesterov=False)\n",
    "        \n",
    "#         optimizer = optim.Adam(model.parameters(), lr = 1e-4, betas=(0.9, 0.999), \n",
    "#                       weight_decay=0.0, amsgrad=False)\n",
    "    \n",
    "        criterion = nn.CrossEntropyLoss(weight = w)\n",
    "#         criterion = nn.CrossEntropyLoss()\n",
    "        \n",
    "        \n",
    "\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        model = model.to(device)\n",
    "        criterion = criterion.to(device)\n",
    "\n",
    "        train_iterator = data.Iterator(train_data, batch_size=batch_size, sort_key=lambda x: len(x.text), device=device)\n",
    "        val_iterator = data.Iterator(val_data, batch_size=batch_size, sort_key=lambda x: len(x.text), device=device)\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            train_loss, train_acc = train(model, train_iterator, optimizer, criterion)\n",
    "            val_loss, val_acc = evaluate(model, val_iterator, criterion)\n",
    "#             print(f'| Epoch: {epoch+1:02} | Train Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
    "#             print(f'| Epoch: {epoch+1:02} | Valid Loss: {val_loss:.3f} | Valid Acc: {val_acc*100:.2f}%')\n",
    "#             print('################################################################################')\n",
    "#             if val_loss < best_val_loss:\n",
    "#                 best_val_loss = val_loss\n",
    "#                 print('Saving Model ...')\n",
    "#                 torch.save(model.state_dict(), 'Best_Model_'+str(best_val_loss)+'.pt')\n",
    "#                 print('*****************************************************')\n",
    "#                 print('best record: [epoch %d], [val loss %.5f], [val acc %.5f]' % (epoch, val_loss, val_acc))\n",
    "#                 print('*****************************************************')\n",
    "\n",
    "        val_loss, val_acc = evaluate(model, val_iterator, criterion) \n",
    "\n",
    "#         if val_loss < best_val_loss:\n",
    "#                 best_val_loss = val_loss\n",
    "#                 print('Saving Model ...')\n",
    "#                 torch.save(model.state_dict(), 'Best_Model_'+str(best_val_loss)+'.pt')\n",
    "#                 print('*****************************************************')\n",
    "#                 print('best record: [epoch %d], [val loss %.5f], [val acc %.5f]' % (epoch, val_loss, val_acc))\n",
    "#                 print('*****************************************************')\n",
    "\n",
    "    #     if val_acc > best_valid_acc:\n",
    "    #         best_valid_acc = val_acc\n",
    "    #         print('Saving Model ...')\n",
    "    #         torch.save(model.state_dict(), 'Best_Model_'+str(val_acc)+'.pt')\n",
    "    #         print('*****************************************************')\n",
    "    #         print('best record: [epoch %d], [val loss %.5f], [val acc %.5f]' % (epoch, val_loss, val_acc))\n",
    "    #         print('*****************************************************')\n",
    "\n",
    "#         print(f'Val. Loss: {val_loss:.3f} | Val. Acc: {val_acc*100:.2f}% |')\n",
    "\n",
    "        _history.append([val_loss, val_acc])\n",
    "        fold_index += 1\n",
    "\n",
    "    _history = np.asarray(_history)\n",
    "    loss = np.mean(_history[:, 0])\n",
    "    acc = np.mean(_history[:, 1])\n",
    "\n",
    "    print('***** Cross Validation Result *****')\n",
    "    print(f'LOSS: {loss}, ACC: {acc}')\n",
    "    \n",
    "    # Handle pruning based on the intermediate value.\n",
    "    if trial.should_prune():\n",
    "        raise optuna.exceptions.TrialPruned()\n",
    "            \n",
    "            \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py:296: FutureWarning:\n",
      "\n",
      "Setting a random_state has no effect since shuffle is False. This will raise an error in 0.24. You should leave random_state to its default (None), or set shuffle=True.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** Cross Validation Result *****\n",
      "LOSS: 1.2632326126098632, ACC: 0.631597226858139\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2020-07-02 19:17:18,354] Finished trial#0 with value: 1.2632326126098632 with parameters: {'lr': 0.01656533244818996, 'momentum': 0.5708424880410802}. Best is trial#0 with value: 1.2632326126098632.\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py:296: FutureWarning:\n",
      "\n",
      "Setting a random_state has no effect since shuffle is False. This will raise an error in 0.24. You should leave random_state to its default (None), or set shuffle=True.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** Cross Validation Result *****\n",
      "LOSS: 1.0990735411643981, ACC: 0.6626901462674141\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2020-07-02 19:18:50,957] Finished trial#1 with value: 1.0990735411643981 with parameters: {'lr': 0.03341222492983985, 'momentum': 0.6695281778877668}. Best is trial#1 with value: 1.0990735411643981.\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py:296: FutureWarning:\n",
      "\n",
      "Setting a random_state has no effect since shuffle is False. This will raise an error in 0.24. You should leave random_state to its default (None), or set shuffle=True.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** Cross Validation Result *****\n",
      "LOSS: 1.387843507528305, ACC: 0.1165095902979374\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2020-07-02 19:20:26,416] Finished trial#2 with value: 1.387843507528305 with parameters: {'lr': 1.768750612750496e-05, 'momentum': 0.8332452018302067}. Best is trial#1 with value: 1.0990735411643981.\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py:296: FutureWarning:\n",
      "\n",
      "Setting a random_state has no effect since shuffle is False. This will raise an error in 0.24. You should leave random_state to its default (None), or set shuffle=True.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** Cross Validation Result *****\n",
      "LOSS: 1.385456645488739, ACC: 0.23207671977579594\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2020-07-02 19:22:05,493] Finished trial#3 with value: 1.385456645488739 with parameters: {'lr': 0.0008824863499858702, 'momentum': 0.26940294602755455}. Best is trial#1 with value: 1.0990735411643981.\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py:296: FutureWarning:\n",
      "\n",
      "Setting a random_state has no effect since shuffle is False. This will raise an error in 0.24. You should leave random_state to its default (None), or set shuffle=True.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** Cross Validation Result *****\n",
      "LOSS: 1.3826733946800231, ACC: 0.45181051790714266\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2020-07-02 19:23:46,643] Finished trial#4 with value: 1.3826733946800231 with parameters: {'lr': 0.002251732456079096, 'momentum': 0.1377034432555457}. Best is trial#1 with value: 1.0990735411643981.\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py:296: FutureWarning:\n",
      "\n",
      "Setting a random_state has no effect since shuffle is False. This will raise an error in 0.24. You should leave random_state to its default (None), or set shuffle=True.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** Cross Validation Result *****\n",
      "LOSS: 1.1296089351177216, ACC: 0.6744543716311455\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2020-07-02 19:25:24,621] Finished trial#5 with value: 1.1296089351177216 with parameters: {'lr': 0.07801164079750648, 'momentum': 0.04914387420384062}. Best is trial#1 with value: 1.0990735411643981.\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py:296: FutureWarning:\n",
      "\n",
      "Setting a random_state has no effect since shuffle is False. This will raise an error in 0.24. You should leave random_state to its default (None), or set shuffle=True.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** Cross Validation Result *****\n",
      "LOSS: 1.383430939912796, ACC: 0.40239748954772947\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2020-07-02 19:27:01,124] Finished trial#6 with value: 1.383430939912796 with parameters: {'lr': 0.002007787346112528, 'momentum': 0.09563409631028257}. Best is trial#1 with value: 1.0990735411643981.\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py:296: FutureWarning:\n",
      "\n",
      "Setting a random_state has no effect since shuffle is False. This will raise an error in 0.24. You should leave random_state to its default (None), or set shuffle=True.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** Cross Validation Result *****\n",
      "LOSS: 1.3878051042556763, ACC: 0.1165095902979374\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2020-07-02 19:28:33,148] Finished trial#7 with value: 1.3878051042556763 with parameters: {'lr': 8.35686602227251e-05, 'momentum': 0.30602359136580665}. Best is trial#1 with value: 1.0990735411643981.\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py:296: FutureWarning:\n",
      "\n",
      "Setting a random_state has no effect since shuffle is False. This will raise an error in 0.24. You should leave random_state to its default (None), or set shuffle=True.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** Cross Validation Result *****\n",
      "LOSS: 1.386549025774002, ACC: 0.16569940596818925\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2020-07-02 19:30:07,548] Finished trial#8 with value: 1.386549025774002 with parameters: {'lr': 9.300467196345544e-05, 'momentum': 0.8675134179835261}. Best is trial#1 with value: 1.0990735411643981.\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py:296: FutureWarning:\n",
      "\n",
      "Setting a random_state has no effect since shuffle is False. This will raise an error in 0.24. You should leave random_state to its default (None), or set shuffle=True.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** Cross Validation Result *****\n",
      "LOSS: 1.2632727444171905, ACC: 0.6235615104436875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2020-07-02 19:31:40,640] Finished trial#9 with value: 1.2632727444171905 with parameters: {'lr': 0.025695501453289766, 'momentum': 0.33151908850067097}. Best is trial#1 with value: 1.0990735411643981.\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py:296: FutureWarning:\n",
      "\n",
      "Setting a random_state has no effect since shuffle is False. This will raise an error in 0.24. You should leave random_state to its default (None), or set shuffle=True.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** Cross Validation Result *****\n",
      "LOSS: 1.0441286504268645, ACC: 0.7695767253637313\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2020-07-02 19:33:13,086] Finished trial#10 with value: 1.0441286504268645 with parameters: {'lr': 0.07200685981328922, 'momentum': 0.6323613796529635}. Best is trial#10 with value: 1.0441286504268645.\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py:296: FutureWarning:\n",
      "\n",
      "Setting a random_state has no effect since shuffle is False. This will raise an error in 0.24. You should leave random_state to its default (None), or set shuffle=True.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** Cross Validation Result *****\n",
      "LOSS: 1.0184863805770874, ACC: 0.7660052955150605\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2020-07-02 19:34:45,976] Finished trial#11 with value: 1.0184863805770874 with parameters: {'lr': 0.0905775709279894, 'momentum': 0.6342968889114206}. Best is trial#11 with value: 1.0184863805770874.\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py:296: FutureWarning:\n",
      "\n",
      "Setting a random_state has no effect since shuffle is False. This will raise an error in 0.24. You should leave random_state to its default (None), or set shuffle=True.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** Cross Validation Result *****\n",
      "LOSS: 1.034672024846077, ACC: 0.753662371635437\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2020-07-02 19:36:18,960] Finished trial#12 with value: 1.034672024846077 with parameters: {'lr': 0.08567646069819349, 'momentum': 0.6504411800887437}. Best is trial#11 with value: 1.0184863805770874.\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py:296: FutureWarning:\n",
      "\n",
      "Setting a random_state has no effect since shuffle is False. This will raise an error in 0.24. You should leave random_state to its default (None), or set shuffle=True.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** Cross Validation Result *****\n",
      "LOSS: 1.3538657903671265, ACC: 0.6260582059621811\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2020-07-02 19:37:51,964] Finished trial#13 with value: 1.3538657903671265 with parameters: {'lr': 0.008031498746672886, 'momentum': 0.4986171615285489}. Best is trial#11 with value: 1.0184863805770874.\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py:296: FutureWarning:\n",
      "\n",
      "Setting a random_state has no effect since shuffle is False. This will raise an error in 0.24. You should leave random_state to its default (None), or set shuffle=True.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** Cross Validation Result *****\n",
      "LOSS: 1.0176321357488631, ACC: 0.7632192522287369\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2020-07-02 19:39:26,501] Finished trial#14 with value: 1.0176321357488631 with parameters: {'lr': 0.0816320997313823, 'momentum': 0.7602964044135365}. Best is trial#14 with value: 1.0176321357488631.\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py:296: FutureWarning:\n",
      "\n",
      "Setting a random_state has no effect since shuffle is False. This will raise an error in 0.24. You should leave random_state to its default (None), or set shuffle=True.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** Cross Validation Result *****\n",
      "LOSS: 1.3144559264183044, ACC: 0.634250995516777\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2020-07-02 19:40:59,297] Finished trial#15 with value: 1.3144559264183044 with parameters: {'lr': 0.00657748148856956, 'momentum': 0.7604995780562103}. Best is trial#14 with value: 1.0176321357488631.\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py:296: FutureWarning:\n",
      "\n",
      "Setting a random_state has no effect since shuffle is False. This will raise an error in 0.24. You should leave random_state to its default (None), or set shuffle=True.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sampler = optuna.samplers.TPESampler()\n",
    "study = optuna.create_study(sampler=sampler, direction='minimize')\n",
    "study.optimize(func=objective, n_trials=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's run our model on the test set!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.load_state_dict(torch.load('tut5-model.pt'))\n",
    "\n",
    "# test_loss, test_acc = evaluate(model, test_iterator, criterion)\n",
    "\n",
    "# print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 512,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sentiment(model, tokenizer, sentence):\n",
    "    model.eval()\n",
    "    tokens = tokenizer.tokenize(sentence)\n",
    "    tokens = tokens#[:max_input_length-2]\n",
    "    indexed = [init_token_idx] + tokenizer.convert_tokens_to_ids(tokens) + [eos_token_idx]\n",
    "    tensor = torch.LongTensor(indexed).to(device)\n",
    "    tensor = tensor.unsqueeze(0)\n",
    "#     prediction = torch.sigmoid(model(tensor))\n",
    "    prediction = torch.softmax(model(tensor), dim=1)\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 513,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<function _default_unk_index at 0x7fe1fc769bf8>, {'Depression': 0, 'Alcohol': 1, 'Suicide': 2, 'Drugs': 3})\n"
     ]
    }
   ],
   "source": [
    "print(LABELS.vocab.stoi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 514,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0077, 0.1835, 0.0271, 0.7817]], grad_fn=<SoftmaxBackward>)"
      ]
     },
     "execution_count": 514,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_sentiment(model, tokenizer, \"weed addiction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 515,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict(model, test_iterator, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 516,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>02V56KMO</td>\n",
       "      <td>how to overcome bad feelings and emotions</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>03BMGTOK</td>\n",
       "      <td>i feel like giving up in life</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>03LZVFM6</td>\n",
       "      <td>i was so depressed feel like got no strength t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0EPULUM5</td>\n",
       "      <td>i feel so low especially since i had no one to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0GM4C5GD</td>\n",
       "      <td>can i be successful when i am a drug addict ?</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         ID                                               text\n",
       "0  02V56KMO          how to overcome bad feelings and emotions\n",
       "1  03BMGTOK                      i feel like giving up in life\n",
       "2  03LZVFM6  i was so depressed feel like got no strength t...\n",
       "3  0EPULUM5  i feel so low especially since i had no one to...\n",
       "4  0GM4C5GD      can i be successful when i am a drug addict ?"
      ]
     },
     "execution_count": 516,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data = pd.read_csv(\"./data/Test_process.csv\")\n",
    "test_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 517,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'how to overcome bad feelings and emotions'"
      ]
     },
     "execution_count": 517,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.iloc[0]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 518,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference\n",
    "\n",
    "# Load Best Model\n",
    "model.load_state_dict(torch.load('Model_Bert_0.29.pt'))\n",
    "\n",
    "y_pred = np.zeros((309, 4))\n",
    "for i in range(len(test_data)):\n",
    "    y_pred[i] = predict_sentiment(model, tokenizer, test_data.iloc[i]['text']).data.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 519,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(309, 4)"
      ]
     },
     "execution_count": 519,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 520,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: optuna in /usr/local/lib/python3.6/dist-packages (1.5.0)\n",
      "Requirement already satisfied: sqlalchemy>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from optuna) (1.3.17)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from optuna) (4.41.1)\n",
      "Requirement already satisfied: alembic in /usr/local/lib/python3.6/dist-packages (from optuna) (1.4.2)\n",
      "Requirement already satisfied: colorlog in /usr/local/lib/python3.6/dist-packages (from optuna) (4.1.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from optuna) (1.19.0)\n",
      "Requirement already satisfied: cmaes>=0.5.0 in /usr/local/lib/python3.6/dist-packages (from optuna) (0.5.1)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from optuna) (0.15.1)\n",
      "Requirement already satisfied: scipy!=1.4.0 in /usr/local/lib/python3.6/dist-packages (from optuna) (1.4.1)\n",
      "Requirement already satisfied: cliff in /usr/local/lib/python3.6/dist-packages (from optuna) (3.3.0)\n",
      "Requirement already satisfied: Mako in /usr/local/lib/python3.6/dist-packages (from alembic->optuna) (1.1.3)\n",
      "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.6/dist-packages (from alembic->optuna) (2.8.1)\n",
      "Requirement already satisfied: python-editor>=0.3 in /usr/local/lib/python3.6/dist-packages (from alembic->optuna) (1.0.4)\n",
      "Requirement already satisfied: PyYAML>=3.12 in /usr/local/lib/python3.6/dist-packages (from cliff->optuna) (3.13)\n",
      "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from cliff->optuna) (1.15.0)\n",
      "Requirement already satisfied: pbr!=2.1.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from cliff->optuna) (5.4.5)\n",
      "Requirement already satisfied: pyparsing>=2.1.0 in /usr/local/lib/python3.6/dist-packages (from cliff->optuna) (2.4.7)\n",
      "Requirement already satisfied: cmd2!=0.8.3,>=0.8.0 in /usr/local/lib/python3.6/dist-packages (from cliff->optuna) (1.1.0)\n",
      "Requirement already satisfied: PrettyTable<0.8,>=0.7.2 in /usr/local/lib/python3.6/dist-packages (from cliff->optuna) (0.7.2)\n",
      "Requirement already satisfied: stevedore>=1.20.0 in /usr/local/lib/python3.6/dist-packages (from cliff->optuna) (2.0.1)\n",
      "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.6/dist-packages (from Mako->alembic->optuna) (1.1.1)\n",
      "Requirement already satisfied: setuptools>=34.4 in /usr/local/lib/python3.6/dist-packages (from cmd2!=0.8.3,>=0.8.0->cliff->optuna) (47.3.1)\n",
      "Requirement already satisfied: wcwidth>=0.1.7 in /usr/local/lib/python3.6/dist-packages (from cmd2!=0.8.3,>=0.8.0->cliff->optuna) (0.2.5)\n",
      "Requirement already satisfied: colorama>=0.3.7 in /usr/local/lib/python3.6/dist-packages (from cmd2!=0.8.3,>=0.8.0->cliff->optuna) (0.4.3)\n",
      "Requirement already satisfied: attrs>=16.3.0 in /usr/local/lib/python3.6/dist-packages (from cmd2!=0.8.3,>=0.8.0->cliff->optuna) (19.3.0)\n",
      "Requirement already satisfied: pyperclip>=1.6 in /usr/local/lib/python3.6/dist-packages (from cmd2!=0.8.3,>=0.8.0->cliff->optuna) (1.8.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kfold=KFold(n_splits=5)\n",
    "\n",
    "# best_val_acc = 0.88\n",
    "\n",
    "# lr = 2e-4 # 0.001\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "# optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "# acc = []\n",
    "\n",
    "# for i, (train_index, validate_index) in enumerate(kfold.split(train_data)):\n",
    "#     #print(\"train index:\", train_index, \"validate index:\", validate_index)\n",
    "#     train_data_i = torch.utils.data.Subset(train_data, train_index)\n",
    "#     validation_data_i = torch.utils.data.Subset(train_data, validate_index)\n",
    "#     train_loader = torch.utils.data.DataLoader(train_data_i, batch_size=batch_size, shuffle=True, num_workers=num_workers, pin_memory=False)\n",
    "#     valid_loader = torch.utils.data.DataLoader(validation_data_i, batch_size=batch_size, shuffle=True, num_workers=num_workers, pin_memory=False)\n",
    "    \n",
    "# #     print(\"Number of Samples in Train: \",len(train))\n",
    "# #     print(\"Number of Samples in Valid: \",len(validation))\n",
    "    \n",
    "#     epoch_num = 5\n",
    "    \n",
    "#     model = Additional(resnet_model, num_fits, freeze = False).to(device)\n",
    "    \n",
    "#     print('------------------------------------------------------------')\n",
    "#     print(f'          ------------- Fold {i} -----------')\n",
    "#     print('------------------------------------------------------------')\n",
    "    \n",
    "#     total_loss_val, total_acc_val = [],[]\n",
    "#     for epoch in range(1, epoch_num+1):\n",
    "#         loss_train, acc_train = train(train_loader, model, criterion, optimizer, epoch)\n",
    "#         loss_val, acc_val = validate(valid_loader, model, criterion, optimizer, epoch)\n",
    "#         total_loss_val.append(loss_val)\n",
    "#         total_acc_val.append(acc_val)\n",
    "        \n",
    "#         if acc_val > best_val_acc:\n",
    "#             best_val_acc = acc_val\n",
    "#             torch.save(model.state_dict(), model_name+'freeze_'+str(best_val_acc)[:4]+'.ckpt')\n",
    "#             print('*****************************************************')\n",
    "#             print('best record: [epoch %d], [val loss %.5f], [val acc %.5f]' % (epoch, loss_val, acc_val))\n",
    "#             print('*****************************************************')\n",
    "            \n",
    "#     acc.append(acc_val)\n",
    "\n",
    "# print(f'Average Accuracy : {np.array(acc).mean()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best_loss_val = 0.8\n",
    "\n",
    "def objective(trial):\n",
    "    \n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    drop  = trial.suggest_loguniform('drop', 0.1, 0.4)\n",
    "    \n",
    "#     model = model = torch.hub.load('pytorch/vision:v0.5.0', 'resnet18', pretrained=True)\n",
    "\n",
    "    model = BertModel.from_pretrained('bert-base-uncased') # bert-base-cased, bert-large-cased\n",
    "\n",
    "\n",
    "    HIDDEN_DIM = 256\n",
    "    OUTPUT_DIM = len(LABELS.vocab)\n",
    "    N_LAYERS = 2\n",
    "    BIDIRECTIONAL = True\n",
    "    DROPOUT = drop\n",
    "\n",
    "    model = BERTGRUSentiment(model,\n",
    "                             HIDDEN_DIM,\n",
    "                             OUTPUT_DIM,\n",
    "                             N_LAYERS,\n",
    "                             BIDIRECTIONAL,\n",
    "                             DROPOUT)\n",
    "\n",
    "    \n",
    "    model = model.to(device)\n",
    "    \n",
    "#     class_weights = [class_distrbution[i] for i in train_loader.dataset.classes]\n",
    "#     class_weights_normalized = [max(class_weights)/i for i in class_weights]\n",
    "\n",
    "#     class_weights_normalized,torch.Tensor(class_weights_normalized)\n",
    "\n",
    "#     weights = torch.Tensor(class_weights_normalized)\n",
    "#     weights = weights.to(device)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    lr  = trial.suggest_loguniform('lr', 1e-5, 1e-1)\n",
    "    optim_ = trial.suggest_categorical('optim_',[optim.SGD, optim.RMSprop,optim.Adam])\n",
    "    momentum = trial.suggest_uniform('momentum', 0.4, 0.99)\n",
    "    \n",
    "    \n",
    "#     optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "    optimizer = optim_(model.parameters(), lr=lr)\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss(weight = w)\n",
    "#     criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    epoch_num = 5\n",
    "    best_acc_val = 0.8\n",
    "    best_loss_val = 0.8\n",
    "    total_loss_val, total_acc_val = [],[]\n",
    "    for epoch in range(1, epoch_num+1):\n",
    "#         loss_train, acc_train = train(train_loader, model, criterion, optimizer, epoch,device)\n",
    "#         loss_val, acc_val = validate(valid_loader, model, criterion, optimizer, epoch,device)\n",
    "        \n",
    "        loss_train, acc_train = train(model, train_iterator, optimizer, criterion)\n",
    "        loss_val, acc_val = evaluate(model, valid_iterator, criterion)\n",
    "        \n",
    "        total_loss_val.append(loss_val)\n",
    "        total_acc_val.append(acc_val)\n",
    "        if acc_val > best_acc_val:\n",
    "            best_acc_val = acc_val\n",
    "            torch.save(model.state_dict(), 'model_'+str(best_acc_val)[:4]+'.ckpt')\n",
    "            print('*****************************************************')\n",
    "            print('best record: [epoch %d], [val loss %.5f], [val acc %.5f]' % (epoch, loss_val, acc_val))\n",
    "            print('*****************************************************')\n",
    "            \n",
    "     # Handle pruning based on the intermediate value.\n",
    "        if trial.should_prune():\n",
    "            raise optuna.exceptions.TrialPruned()\n",
    "            \n",
    "            \n",
    "    return np.mean(total_acc_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/optuna/distributions.py:410: UserWarning:\n",
      "\n",
      "Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains <class 'torch.optim.sgd.SGD'> which is of type type.\n",
      "\n",
      "/usr/local/lib/python3.6/dist-packages/optuna/distributions.py:410: UserWarning:\n",
      "\n",
      "Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains <class 'torch.optim.rmsprop.RMSprop'> which is of type type.\n",
      "\n",
      "/usr/local/lib/python3.6/dist-packages/optuna/distributions.py:410: UserWarning:\n",
      "\n",
      "Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains <class 'torch.optim.adam.Adam'> which is of type type.\n",
      "\n",
      "[I 2020-07-01 12:31:02,459] Finished trial#0 with value: 0.1890625 with parameters: {'drop': 0.23151365316264638, 'lr': 0.017801424403706422, 'optim_': <class 'torch.optim.rmsprop.RMSprop'>, 'momentum': 0.7064461965471499}. Best is trial#0 with value: 0.1890625.\n",
      "[I 2020-07-01 12:33:21,818] Finished trial#1 with value: 0.5014204546809197 with parameters: {'drop': 0.39547460832926185, 'lr': 0.00656911225949057, 'optim_': <class 'torch.optim.sgd.SGD'>, 'momentum': 0.7412688399307483}. Best is trial#1 with value: 0.5014204546809197.\n",
      "[I 2020-07-01 12:35:50,437] Finished trial#2 with value: 0.4046875 with parameters: {'drop': 0.1451614224557499, 'lr': 0.03214542772027602, 'optim_': <class 'torch.optim.rmsprop.RMSprop'>, 'momentum': 0.8295986843188028}. Best is trial#1 with value: 0.5014204546809197.\n",
      "[I 2020-07-01 12:38:10,998] Finished trial#3 with value: 0.574147728830576 with parameters: {'drop': 0.3503666191721918, 'lr': 0.0016615511785918666, 'optim_': <class 'torch.optim.sgd.SGD'>, 'momentum': 0.47662305021673596}. Best is trial#3 with value: 0.574147728830576.\n",
      "[I 2020-07-01 12:40:33,280] Finished trial#4 with value: 0.5593750014901161 with parameters: {'drop': 0.25950138202271456, 'lr': 1.0043312446291438e-05, 'optim_': <class 'torch.optim.rmsprop.RMSprop'>, 'momentum': 0.7146344733785255}. Best is trial#3 with value: 0.574147728830576.\n",
      "[I 2020-07-01 12:42:54,944] Finished trial#5 with value: 0.6214488670229912 with parameters: {'drop': 0.19419233671464214, 'lr': 0.00010223330736246305, 'optim_': <class 'torch.optim.adam.Adam'>, 'momentum': 0.4423144266157881}. Best is trial#5 with value: 0.6214488670229912.\n",
      "[I 2020-07-01 12:45:16,832] Finished trial#6 with value: 0.5495738685131073 with parameters: {'drop': 0.3893097609469198, 'lr': 0.00020665065428540202, 'optim_': <class 'torch.optim.adam.Adam'>, 'momentum': 0.8899642544284962}. Best is trial#5 with value: 0.6214488670229912.\n",
      "[I 2020-07-01 12:47:56,393] Finished trial#7 with value: 0.4163352273404598 with parameters: {'drop': 0.1390020530639312, 'lr': 0.002332292228774461, 'optim_': <class 'torch.optim.sgd.SGD'>, 'momentum': 0.788840081803827}. Best is trial#5 with value: 0.6214488670229912.\n",
      "[I 2020-07-01 12:50:19,330] Finished trial#8 with value: 0.40184659212827684 with parameters: {'drop': 0.3165966704460081, 'lr': 0.08699259047735268, 'optim_': <class 'torch.optim.sgd.SGD'>, 'momentum': 0.8569989329408276}. Best is trial#5 with value: 0.6214488670229912.\n",
      "[I 2020-07-01 12:52:40,943] Finished trial#9 with value: 0.5508522748947143 with parameters: {'drop': 0.3022561560245174, 'lr': 0.0010580083848408448, 'optim_': <class 'torch.optim.adam.Adam'>, 'momentum': 0.7784940875199808}. Best is trial#5 with value: 0.6214488670229912.\n",
      "[I 2020-07-01 12:55:01,898] Finished trial#10 with value: 0.5126420482993126 with parameters: {'drop': 0.18124453320655212, 'lr': 7.04480204153525e-05, 'optim_': <class 'torch.optim.adam.Adam'>, 'momentum': 0.46513715353280755}. Best is trial#5 with value: 0.6214488670229912.\n",
      "[I 2020-07-01 12:57:23,255] Finished trial#11 with value: 0.5911931842565536 with parameters: {'drop': 0.1902467211689706, 'lr': 0.0001404780920983748, 'optim_': <class 'torch.optim.adam.Adam'>, 'momentum': 0.43300115647026266}. Best is trial#5 with value: 0.6214488670229912.\n",
      "[I 2020-07-01 12:59:44,870] Finished trial#12 with value: 0.5407670475542545 with parameters: {'drop': 0.10328175870034893, 'lr': 8.463944162701583e-05, 'optim_': <class 'torch.optim.adam.Adam'>, 'momentum': 0.5881761598487134}. Best is trial#5 with value: 0.6214488670229912.\n",
      "[I 2020-07-01 13:02:07,332] Finished trial#13 with value: 0.47897727489471437 with parameters: {'drop': 0.1822475705320944, 'lr': 1.678542244919679e-05, 'optim_': <class 'torch.optim.adam.Adam'>, 'momentum': 0.5778639635151999}. Best is trial#5 with value: 0.6214488670229912.\n",
      "[I 2020-07-01 13:04:28,610] Finished trial#14 with value: 0.6339488655328751 with parameters: {'drop': 0.15151645707164402, 'lr': 0.0002608026964688712, 'optim_': <class 'torch.optim.adam.Adam'>, 'momentum': 0.41800701445763316}. Best is trial#14 with value: 0.6339488655328751.\n",
      "[I 2020-07-01 13:06:49,541] Finished trial#15 with value: 0.6214488677680492 with parameters: {'drop': 0.14132035221637368, 'lr': 0.00031514882565672974, 'optim_': <class 'torch.optim.adam.Adam'>, 'momentum': 0.970903035078729}. Best is trial#14 with value: 0.6339488655328751.\n",
      "[I 2020-07-01 13:09:08,505] Finished trial#16 with value: 0.5745738662779332 with parameters: {'drop': 0.10330827276520294, 'lr': 0.0004109388643860157, 'optim_': <class 'torch.optim.adam.Adam'>, 'momentum': 0.9870417677058544}. Best is trial#14 with value: 0.6339488655328751.\n",
      "[I 2020-07-01 13:11:26,548] Finished trial#17 with value: 0.5863636374473572 with parameters: {'drop': 0.1267905350074008, 'lr': 2.6647454590075348e-05, 'optim_': <class 'torch.optim.adam.Adam'>, 'momentum': 0.9684988284100633}. Best is trial#14 with value: 0.6339488655328751.\n",
      "[I 2020-07-01 13:13:45,027] Finished trial#18 with value: 0.614772728830576 with parameters: {'drop': 0.15133121589838902, 'lr': 0.0003868894031485633, 'optim_': <class 'torch.optim.adam.Adam'>, 'momentum': 0.5525461826273421}. Best is trial#14 with value: 0.6339488655328751.\n",
      "[I 2020-07-01 13:16:02,757] Finished trial#19 with value: 0.5490056835114956 with parameters: {'drop': 0.12168472372757094, 'lr': 0.0005184156132030855, 'optim_': <class 'torch.optim.adam.Adam'>, 'momentum': 0.6218717103752891}. Best is trial#14 with value: 0.6339488655328751.\n",
      "[I 2020-07-01 13:18:18,824] Finished trial#20 with value: 0.6288352310657501 with parameters: {'drop': 0.15890223431865377, 'lr': 3.665146891784493e-05, 'optim_': <class 'torch.optim.rmsprop.RMSprop'>, 'momentum': 0.923733584493053}. Best is trial#14 with value: 0.6339488655328751.\n",
      "[I 2020-07-01 13:20:35,026] Finished trial#21 with value: 0.4683238655328751 with parameters: {'drop': 0.16225386342183976, 'lr': 2.700029126426658e-05, 'optim_': <class 'torch.optim.rmsprop.RMSprop'>, 'momentum': 0.9195023693818025}. Best is trial#14 with value: 0.6339488655328751.\n",
      "[I 2020-07-01 13:22:51,781] Finished trial#22 with value: 0.5474431842565537 with parameters: {'drop': 0.11902699403314043, 'lr': 4.458921815752682e-05, 'optim_': <class 'torch.optim.rmsprop.RMSprop'>, 'momentum': 0.9483317081729549}. Best is trial#14 with value: 0.6339488655328751.\n",
      "[I 2020-07-01 13:25:10,310] Finished trial#23 with value: 0.6586647778749466 with parameters: {'drop': 0.2240753907070478, 'lr': 0.00023198838597744247, 'optim_': <class 'torch.optim.rmsprop.RMSprop'>, 'momentum': 0.8939467789874719}. Best is trial#23 with value: 0.6586647778749466.\n",
      "[I 2020-07-01 13:27:28,305] Finished trial#24 with value: 0.5254261367022991 with parameters: {'drop': 0.22625606392007452, 'lr': 4.635481559155916e-05, 'optim_': <class 'torch.optim.rmsprop.RMSprop'>, 'momentum': 0.8872858059335165}. Best is trial#23 with value: 0.6586647778749466.\n",
      "[I 2020-07-01 13:29:44,880] Finished trial#25 with value: 0.603409095108509 with parameters: {'drop': 0.21927769117792992, 'lr': 0.00016171538419162818, 'optim_': <class 'torch.optim.rmsprop.RMSprop'>, 'momentum': 0.6395978760331736}. Best is trial#23 with value: 0.6586647778749466.\n",
      "[I 2020-07-01 13:32:01,781] Finished trial#26 with value: 0.30639204680919646 with parameters: {'drop': 0.1686084156381915, 'lr': 0.0008158426428860864, 'optim_': <class 'torch.optim.rmsprop.RMSprop'>, 'momentum': 0.8228028663151887}. Best is trial#23 with value: 0.6586647778749466.\n",
      "[I 2020-07-01 13:34:19,440] Finished trial#27 with value: 0.2515625 with parameters: {'drop': 0.20696163545865134, 'lr': 0.003838124708601664, 'optim_': <class 'torch.optim.rmsprop.RMSprop'>, 'momentum': 0.93350698133338}. Best is trial#23 with value: 0.6586647778749466.\n",
      "[I 2020-07-01 13:36:37,665] Finished trial#28 with value: 0.5051136374473572 with parameters: {'drop': 0.2579398509156453, 'lr': 1.7474382989370983e-05, 'optim_': <class 'torch.optim.rmsprop.RMSprop'>, 'momentum': 0.6695640935399669}. Best is trial#23 with value: 0.6586647778749466.\n",
      "[I 2020-07-01 13:38:55,650] Finished trial#29 with value: 0.6427556835114956 with parameters: {'drop': 0.25322814618581485, 'lr': 0.00024833129206887773, 'optim_': <class 'torch.optim.rmsprop.RMSprop'>, 'momentum': 0.401263320025294}. Best is trial#23 with value: 0.6586647778749466.\n",
      "[I 2020-07-01 13:41:11,933] Finished trial#30 with value: 0.17045454680919647 with parameters: {'drop': 0.2500912441330009, 'lr': 0.0009613913689407041, 'optim_': <class 'torch.optim.rmsprop.RMSprop'>, 'momentum': 0.5262732580182722}. Best is trial#23 with value: 0.6586647778749466.\n",
      "[I 2020-07-01 13:43:29,203] Finished trial#31 with value: 0.6223011374473572 with parameters: {'drop': 0.16521127064939614, 'lr': 0.00023716836737312628, 'optim_': <class 'torch.optim.rmsprop.RMSprop'>, 'momentum': 0.5007620309917656}. Best is trial#23 with value: 0.6586647778749466.\n",
      "[I 2020-07-01 13:45:46,628] Finished trial#32 with value: 0.5794034108519555 with parameters: {'drop': 0.28180605923181845, 'lr': 5.5289432090993965e-05, 'optim_': <class 'torch.optim.rmsprop.RMSprop'>, 'momentum': 0.4094796768751836}. Best is trial#23 with value: 0.6586647778749466.\n",
      "[I 2020-07-01 13:48:03,479] Finished trial#33 with value: 0.609375000745058 with parameters: {'drop': 0.2130391550343721, 'lr': 0.00011320869627807308, 'optim_': <class 'torch.optim.rmsprop.RMSprop'>, 'momentum': 0.8902206000465105}. Best is trial#23 with value: 0.6586647778749466.\n",
      "[I 2020-07-01 13:51:24,412] Finished trial#34 with value: 0.6465909108519554 with parameters: {'drop': 0.1560329315131922, 'lr': 0.000591044276948825, 'optim_': <class 'torch.optim.rmsprop.RMSprop'>, 'momentum': 0.4156342671219952}. Best is trial#23 with value: 0.6586647778749466.\n",
      "[I 2020-07-01 13:55:16,472] Finished trial#35 with value: 0.25 with parameters: {'drop': 0.23369343540890036, 'lr': 0.010210190172379608, 'optim_': <class 'torch.optim.rmsprop.RMSprop'>, 'momentum': 0.41039339061228647}. Best is trial#23 with value: 0.6586647778749466.\n",
      "[I 2020-07-01 13:59:19,809] Finished trial#36 with value: 0.2102272741496563 with parameters: {'drop': 0.1291173231867794, 'lr': 0.0006932948808446053, 'optim_': <class 'torch.optim.sgd.SGD'>, 'momentum': 0.4018101561096035}. Best is trial#23 with value: 0.6586647778749466.\n",
      "[I 2020-07-01 14:03:31,121] Finished trial#37 with value: 0.3265625 with parameters: {'drop': 0.24483480393048165, 'lr': 0.00155421739225852, 'optim_': <class 'torch.optim.rmsprop.RMSprop'>, 'momentum': 0.4921919551781277}. Best is trial#23 with value: 0.6586647778749466.\n",
      "[I 2020-07-01 14:07:47,608] Finished trial#38 with value: 0.574573865532875 with parameters: {'drop': 0.27634490048566024, 'lr': 0.0002343334981046579, 'optim_': <class 'torch.optim.rmsprop.RMSprop'>, 'momentum': 0.4511214204025398}. Best is trial#23 with value: 0.6586647778749466.\n",
      "[I 2020-07-01 14:10:25,467] Finished trial#39 with value: 0.316761364787817 with parameters: {'drop': 0.20146314227246304, 'lr': 0.003135875678832249, 'optim_': <class 'torch.optim.sgd.SGD'>, 'momentum': 0.5307350295901279}. Best is trial#23 with value: 0.6586647778749466.\n",
      "[I 2020-07-01 14:13:00,095] Finished trial#40 with value: 0.4977272734045982 with parameters: {'drop': 0.17732620584361242, 'lr': 0.0005705868150234766, 'optim_': <class 'torch.optim.rmsprop.RMSprop'>, 'momentum': 0.7570440823270661}. Best is trial#23 with value: 0.6586647778749466.\n",
      "[I 2020-07-01 14:15:23,660] Finished trial#41 with value: 0.21974431872367858 with parameters: {'drop': 0.15215297260204205, 'lr': 0.001533973340808498, 'optim_': <class 'torch.optim.rmsprop.RMSprop'>, 'momentum': 0.8359346974412255}. Best is trial#23 with value: 0.6586647778749466.\n",
      "[I 2020-07-01 14:17:50,729] Finished trial#42 with value: 0.6526988662779332 with parameters: {'drop': 0.15587008264233843, 'lr': 0.00023512066070567497, 'optim_': <class 'torch.optim.rmsprop.RMSprop'>, 'momentum': 0.43122543415544895}. Best is trial#23 with value: 0.6586647778749466.\n",
      "[I 2020-07-01 14:20:11,983] Finished trial#43 with value: 0.5447443217039108 with parameters: {'drop': 0.1367546154441452, 'lr': 0.00028082444968204463, 'optim_': <class 'torch.optim.rmsprop.RMSprop'>, 'momentum': 0.42682635457301193}. Best is trial#23 with value: 0.6586647778749466.\n",
      "[I 2020-07-01 14:22:34,711] Finished trial#44 with value: 0.6103693202137948 with parameters: {'drop': 0.3490200708364031, 'lr': 0.0001571583268602355, 'optim_': <class 'torch.optim.rmsprop.RMSprop'>, 'momentum': 0.46538206309395214}. Best is trial#23 with value: 0.6586647778749466.\n",
      "[I 2020-07-01 14:24:55,945] Finished trial#45 with value: 0.6232954576611519 with parameters: {'drop': 0.19336475103987016, 'lr': 7.84466819411134e-05, 'optim_': <class 'torch.optim.rmsprop.RMSprop'>, 'momentum': 0.4008143098230329}. Best is trial#23 with value: 0.6586647778749466.\n",
      "[I 2020-07-01 14:27:18,199] Finished trial#46 with value: 0.5917613670229912 with parameters: {'drop': 0.1128448228853576, 'lr': 0.0004143196049146042, 'optim_': <class 'torch.optim.rmsprop.RMSprop'>, 'momentum': 0.7207246883553072}. Best is trial#23 with value: 0.6586647778749466.\n",
      "[I 2020-07-01 14:29:37,437] Finished trial#47 with value: 0.21590909287333487 with parameters: {'drop': 0.1533813308051333, 'lr': 0.00017320612857184644, 'optim_': <class 'torch.optim.sgd.SGD'>, 'momentum': 0.4320304489595192}. Best is trial#23 with value: 0.6586647778749466.\n",
      "[I 2020-07-01 14:31:59,366] Finished trial#48 with value: 0.21207386553287505 with parameters: {'drop': 0.17138596403519962, 'lr': 0.0012224937752479577, 'optim_': <class 'torch.optim.rmsprop.RMSprop'>, 'momentum': 0.49612727071666957}. Best is trial#23 with value: 0.6586647778749466.\n",
      "[I 2020-07-01 14:34:23,237] Finished trial#49 with value: 0.45113636553287506 with parameters: {'drop': 0.13492645059739625, 'lr': 0.00011159307043395476, 'optim_': <class 'torch.optim.adam.Adam'>, 'momentum': 0.4556720295885841}. Best is trial#23 with value: 0.6586647778749466.\n",
      "[I 2020-07-01 14:36:45,466] Finished trial#50 with value: 0.4961647726595402 with parameters: {'drop': 0.14687812598559727, 'lr': 0.0005747877384913523, 'optim_': <class 'torch.optim.rmsprop.RMSprop'>, 'momentum': 0.47496965983913925}. Best is trial#23 with value: 0.6586647778749466.\n",
      "[I 2020-07-01 14:39:06,082] Finished trial#51 with value: 0.6082386374473572 with parameters: {'drop': 0.1543699206979594, 'lr': 1.0056109502142771e-05, 'optim_': <class 'torch.optim.rmsprop.RMSprop'>, 'momentum': 0.8747229261847073}. Best is trial#23 with value: 0.6586647778749466.\n",
      "[I 2020-07-01 14:41:26,926] Finished trial#52 with value: 0.6617897778749466 with parameters: {'drop': 0.18289678308126747, 'lr': 0.00026231596904839654, 'optim_': <class 'torch.optim.rmsprop.RMSprop'>, 'momentum': 0.9117238076647703}. Best is trial#52 with value: 0.6617897778749466.\n",
      "[I 2020-07-01 14:43:46,800] Finished trial#53 with value: 0.631534093618393 with parameters: {'drop': 0.18356447712284177, 'lr': 0.0002569786497802581, 'optim_': <class 'torch.optim.rmsprop.RMSprop'>, 'momentum': 0.8007470909353649}. Best is trial#52 with value: 0.6617897778749466.\n",
      "[I 2020-07-01 14:46:07,534] Finished trial#54 with value: 0.5906250014901161 with parameters: {'drop': 0.23488883521650478, 'lr': 0.00036621019881781313, 'optim_': <class 'torch.optim.rmsprop.RMSprop'>, 'momentum': 0.42465313016439904}. Best is trial#52 with value: 0.6617897778749466.\n",
      "[I 2020-07-01 14:49:22,335] Finished trial#55 with value: 0.5747159123420715 with parameters: {'drop': 0.18802541825409372, 'lr': 0.00011824549713313756, 'optim_': <class 'torch.optim.adam.Adam'>, 'momentum': 0.8563711400061006}. Best is trial#52 with value: 0.6617897778749466.\n",
      "[I 2020-07-01 14:51:59,041] Finished trial#56 with value: 0.564772729575634 with parameters: {'drop': 0.14426722874685396, 'lr': 0.000464716360526163, 'optim_': <class 'torch.optim.rmsprop.RMSprop'>, 'momentum': 0.9746428220603063}. Best is trial#52 with value: 0.6617897778749466.\n",
      "[I 2020-07-01 14:54:20,856] Finished trial#57 with value: 0.5789772763848304 with parameters: {'drop': 0.1734651967259202, 'lr': 0.00020129947595986446, 'optim_': <class 'torch.optim.adam.Adam'>, 'momentum': 0.9045950610879034}. Best is trial#52 with value: 0.6617897778749466.\n",
      "[I 2020-07-01 14:56:55,773] Finished trial#58 with value: 0.5376420468091965 with parameters: {'drop': 0.20178880719396405, 'lr': 0.0007468947630710536, 'optim_': <class 'torch.optim.rmsprop.RMSprop'>, 'momentum': 0.9458272293003804}. Best is trial#52 with value: 0.6617897778749466.\n",
      "[I 2020-07-01 15:00:04,163] Finished trial#59 with value: 0.6490056812763214 with parameters: {'drop': 0.2710818726414075, 'lr': 7.626819129724929e-05, 'optim_': <class 'torch.optim.rmsprop.RMSprop'>, 'momentum': 0.44240572844420867}. Best is trial#52 with value: 0.6617897778749466.\n",
      "[I 2020-07-01 15:02:50,592] Finished trial#60 with value: 0.5815340936183929 with parameters: {'drop': 0.30989669629704364, 'lr': 8.394345826702863e-05, 'optim_': <class 'torch.optim.rmsprop.RMSprop'>, 'momentum': 0.44004433671280446}. Best is trial#52 with value: 0.6617897778749466.\n",
      "[I 2020-07-01 15:05:09,808] Finished trial#61 with value: 0.6426136389374733 with parameters: {'drop': 0.27214539508318647, 'lr': 0.0002915541935200813, 'optim_': <class 'torch.optim.rmsprop.RMSprop'>, 'momentum': 0.4024558715442386}. Best is trial#52 with value: 0.6617897778749466.\n",
      "[I 2020-07-01 15:08:50,347] Finished trial#62 with value: 0.5765625044703484 with parameters: {'drop': 0.2840474437027115, 'lr': 0.00013942304238908542, 'optim_': <class 'torch.optim.rmsprop.RMSprop'>, 'momentum': 0.5172890055598011}. Best is trial#52 with value: 0.6617897778749466.\n",
      "[I 2020-07-01 15:13:09,768] Finished trial#63 with value: 0.6180397748947144 with parameters: {'drop': 0.2742286582801309, 'lr': 0.0003183462112749801, 'optim_': <class 'torch.optim.rmsprop.RMSprop'>, 'momentum': 0.40188034245537335}. Best is trial#52 with value: 0.6617897778749466.\n",
      "[I 2020-07-01 15:17:36,150] Finished trial#64 with value: 0.40071022808551787 with parameters: {'drop': 0.2956046207243159, 'lr': 0.06366832207118116, 'optim_': <class 'torch.optim.rmsprop.RMSprop'>, 'momentum': 0.5599409336547685}. Best is trial#52 with value: 0.6617897778749466.\n",
      "[I 2020-07-01 15:21:25,073] Finished trial#65 with value: 0.5583806827664375 with parameters: {'drop': 0.2608534379539266, 'lr': 7.061359351444986e-05, 'optim_': <class 'torch.optim.rmsprop.RMSprop'>, 'momentum': 0.4452224269428961}. Best is trial#52 with value: 0.6617897778749466.\n",
      "[I 2020-07-01 15:25:59,274] Finished trial#66 with value: 0.601846593618393 with parameters: {'drop': 0.32646875980834156, 'lr': 0.00018635709678481124, 'optim_': <class 'torch.optim.rmsprop.RMSprop'>, 'momentum': 0.4693539086940476}. Best is trial#52 with value: 0.6617897778749466.\n",
      "[I 2020-07-01 15:31:28,206] Finished trial#67 with value: 0.5431818209588528 with parameters: {'drop': 0.2165693929943908, 'lr': 0.00034506423902934505, 'optim_': <class 'torch.optim.rmsprop.RMSprop'>, 'momentum': 0.48280834003643613}. Best is trial#52 with value: 0.6617897778749466.\n",
      "[I 2020-07-01 15:35:44,166] Finished trial#68 with value: 0.5420454576611519 with parameters: {'drop': 0.2454043295272013, 'lr': 5.9477104637523215e-05, 'optim_': <class 'torch.optim.rmsprop.RMSprop'>, 'momentum': 0.4180061866068916}. Best is trial#52 with value: 0.6617897778749466.\n",
      "[I 2020-07-01 15:40:00,604] Finished trial#69 with value: 0.2546875 with parameters: {'drop': 0.25786053942766723, 'lr': 0.0010492198128651481, 'optim_': <class 'torch.optim.rmsprop.RMSprop'>, 'momentum': 0.4025773227165952}. Best is trial#52 with value: 0.6617897778749466.\n",
      "[I 2020-07-01 15:44:19,140] Finished trial#70 with value: 0.5190340921282768 with parameters: {'drop': 0.2652538126988884, 'lr': 0.0005294715298193271, 'optim_': <class 'torch.optim.rmsprop.RMSprop'>, 'momentum': 0.6116899683775143}. Best is trial#52 with value: 0.6617897778749466.\n",
      "[I 2020-07-01 15:48:35,100] Finished trial#71 with value: 0.6039772741496563 with parameters: {'drop': 0.16309529729042674, 'lr': 0.0002539914059418168, 'optim_': <class 'torch.optim.adam.Adam'>, 'momentum': 0.4500230412217612}. Best is trial#52 with value: 0.6617897778749466.\n",
      "[I 2020-07-01 15:52:49,086] Finished trial#72 with value: 0.6683238670229912 with parameters: {'drop': 0.22905994597218524, 'lr': 0.00010089304431275047, 'optim_': <class 'torch.optim.rmsprop.RMSprop'>, 'momentum': 0.6795149203389117}. Best is trial#72 with value: 0.6683238670229912.\n",
      "[I 2020-07-01 15:57:01,407] Finished trial#73 with value: 0.5978693217039108 with parameters: {'drop': 0.23771855331822214, 'lr': 3.541321633037175e-05, 'optim_': <class 'torch.optim.rmsprop.RMSprop'>, 'momentum': 0.6771496765744733}. Best is trial#72 with value: 0.6683238670229912.\n",
      "[I 2020-07-01 16:01:19,232] Finished trial#74 with value: 0.5839488655328751 with parameters: {'drop': 0.22471875805907732, 'lr': 9.619534830774907e-05, 'optim_': <class 'torch.optim.rmsprop.RMSprop'>, 'momentum': 0.9891229970664361}. Best is trial#72 with value: 0.6683238670229912.\n",
      "[I 2020-07-01 16:05:38,524] Finished trial#75 with value: 0.6407670482993126 with parameters: {'drop': 0.20636154715426885, 'lr': 0.00014743057935094367, 'optim_': <class 'torch.optim.rmsprop.RMSprop'>, 'momentum': 0.7691568508835763}. Best is trial#72 with value: 0.6683238670229912.\n",
      "[I 2020-07-01 16:09:53,671] Finished trial#76 with value: 0.5491477280855179 with parameters: {'drop': 0.2279235688378191, 'lr': 0.0006960596657262889, 'optim_': <class 'torch.optim.rmsprop.RMSprop'>, 'momentum': 0.8024891369249919}. Best is trial#72 with value: 0.6683238670229912.\n",
      "[I 2020-07-01 16:14:15,914] Finished trial#77 with value: 0.4921875 with parameters: {'drop': 0.2942423994072222, 'lr': 0.0001910359602726235, 'optim_': <class 'torch.optim.sgd.SGD'>, 'momentum': 0.7387197459004268}. Best is trial#72 with value: 0.6683238670229912.\n",
      "[I 2020-07-01 16:18:45,899] Finished trial#78 with value: 0.6542613677680492 with parameters: {'drop': 0.2507246319379388, 'lr': 0.00013208713288785192, 'optim_': <class 'torch.optim.rmsprop.RMSprop'>, 'momentum': 0.8554304075376329}. Best is trial#72 with value: 0.6683238670229912.\n",
      "[I 2020-07-01 16:23:45,223] Finished trial#79 with value: 0.6703125044703484 with parameters: {'drop': 0.24659718761907715, 'lr': 0.00011278801907235591, 'optim_': <class 'torch.optim.rmsprop.RMSprop'>, 'momentum': 0.848956458953429}. Best is trial#79 with value: 0.6703125044703484.\n",
      "[I 2020-07-01 16:28:08,231] Finished trial#80 with value: 0.5616477310657502 with parameters: {'drop': 0.24142713918080747, 'lr': 5.6044758974320865e-05, 'optim_': <class 'torch.optim.rmsprop.RMSprop'>, 'momentum': 0.8583562463046134}. Best is trial#79 with value: 0.6703125044703484.\n",
      "[I 2020-07-01 16:33:04,742] Finished trial#81 with value: 0.682386365532875 with parameters: {'drop': 0.24912652962139217, 'lr': 0.00012191227120259712, 'optim_': <class 'torch.optim.rmsprop.RMSprop'>, 'momentum': 0.9152939063265242}. Best is trial#81 with value: 0.682386365532875.\n",
      "[I 2020-07-01 16:38:15,336] Finished trial#82 with value: 0.6282670482993126 with parameters: {'drop': 0.21214301829988863, 'lr': 0.00012607455170572372, 'optim_': <class 'torch.optim.rmsprop.RMSprop'>, 'momentum': 0.908555874515081}. Best is trial#81 with value: 0.682386365532875.\n"
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "\n",
    "sampler = optuna.samplers.TPESampler()\n",
    "study = optuna.create_study(sampler=sampler, direction='maximize')\n",
    "study.optimize(func=objective, n_trials=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 521,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 522,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Depression</th>\n",
       "      <th>Alcohol</th>\n",
       "      <th>Suicide</th>\n",
       "      <th>Drugs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>304</th>\n",
       "      <td>Z9A6ACLK</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>305</th>\n",
       "      <td>ZDUOIGKN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>306</th>\n",
       "      <td>ZHQ60CCH</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>307</th>\n",
       "      <td>ZVIJMA4O</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>308</th>\n",
       "      <td>ZYIFAY98</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           ID  Depression  Alcohol  Suicide  Drugs\n",
       "304  Z9A6ACLK           0        0        0      0\n",
       "305  ZDUOIGKN           0        0        0      0\n",
       "306  ZHQ60CCH           0        0        0      0\n",
       "307  ZVIJMA4O           0        0        0      0\n",
       "308  ZYIFAY98           0        0        0      0"
      ]
     },
     "execution_count": 522,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 523,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>02V56KMO</td>\n",
       "      <td>how to overcome bad feelings and emotions</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>03BMGTOK</td>\n",
       "      <td>i feel like giving up in life</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>03LZVFM6</td>\n",
       "      <td>i was so depressed feel like got no strength t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0EPULUM5</td>\n",
       "      <td>i feel so low especially since i had no one to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0GM4C5GD</td>\n",
       "      <td>can i be successful when i am a drug addict ?</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         ID                                               text\n",
       "0  02V56KMO          how to overcome bad feelings and emotions\n",
       "1  03BMGTOK                      i feel like giving up in life\n",
       "2  03LZVFM6  i was so depressed feel like got no strength t...\n",
       "3  0EPULUM5  i feel so low especially since i had no one to...\n",
       "4  0GM4C5GD      can i be successful when i am a drug addict ?"
      ]
     },
     "execution_count": 523,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 524,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 524,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(test.ID == sample.ID).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 525,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['Depression', 'Alcohol', 'Suicide', 'Drugs'])"
      ]
     },
     "execution_count": 525,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LABELS.vocab.stoi.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 526,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Depression</th>\n",
       "      <th>Alcohol</th>\n",
       "      <th>Suicide</th>\n",
       "      <th>Drugs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.424338</td>\n",
       "      <td>0.094797</td>\n",
       "      <td>0.480179</td>\n",
       "      <td>0.000686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.993562</td>\n",
       "      <td>0.000686</td>\n",
       "      <td>0.005570</td>\n",
       "      <td>0.000181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.994707</td>\n",
       "      <td>0.000189</td>\n",
       "      <td>0.004724</td>\n",
       "      <td>0.000380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.996023</td>\n",
       "      <td>0.000205</td>\n",
       "      <td>0.003508</td>\n",
       "      <td>0.000265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.003675</td>\n",
       "      <td>0.028275</td>\n",
       "      <td>0.001293</td>\n",
       "      <td>0.966757</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Depression   Alcohol   Suicide     Drugs\n",
       "0    0.424338  0.094797  0.480179  0.000686\n",
       "1    0.993562  0.000686  0.005570  0.000181\n",
       "2    0.994707  0.000189  0.004724  0.000380\n",
       "3    0.996023  0.000205  0.003508  0.000265\n",
       "4    0.003675  0.028275  0.001293  0.966757"
      ]
     },
     "execution_count": 526,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = pd.DataFrame(y_pred, columns=LABELS.vocab.stoi.keys())\n",
    "predictions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 527,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pd.concat([sample[['ID']], predictions], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 528,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Depression</th>\n",
       "      <th>Alcohol</th>\n",
       "      <th>Suicide</th>\n",
       "      <th>Drugs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>02V56KMO</td>\n",
       "      <td>0.424338</td>\n",
       "      <td>0.094797</td>\n",
       "      <td>0.480179</td>\n",
       "      <td>0.000686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>03BMGTOK</td>\n",
       "      <td>0.993562</td>\n",
       "      <td>0.000686</td>\n",
       "      <td>0.005570</td>\n",
       "      <td>0.000181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>03LZVFM6</td>\n",
       "      <td>0.994707</td>\n",
       "      <td>0.000189</td>\n",
       "      <td>0.004724</td>\n",
       "      <td>0.000380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0EPULUM5</td>\n",
       "      <td>0.996023</td>\n",
       "      <td>0.000205</td>\n",
       "      <td>0.003508</td>\n",
       "      <td>0.000265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0GM4C5GD</td>\n",
       "      <td>0.003675</td>\n",
       "      <td>0.028275</td>\n",
       "      <td>0.001293</td>\n",
       "      <td>0.966757</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         ID  Depression   Alcohol   Suicide     Drugs\n",
       "0  02V56KMO    0.424338  0.094797  0.480179  0.000686\n",
       "1  03BMGTOK    0.993562  0.000686  0.005570  0.000181\n",
       "2  03LZVFM6    0.994707  0.000189  0.004724  0.000380\n",
       "3  0EPULUM5    0.996023  0.000205  0.003508  0.000265\n",
       "4  0GM4C5GD    0.003675  0.028275  0.001293  0.966757"
      ]
     },
     "execution_count": 528,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 529,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.to_csv('./Model_Bert_0.29_uncasted_split.csv',\n",
    "              index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 512,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 513,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 514,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(device)\n",
    "criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 515,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorical_accuracy(preds, y):\n",
    "    \"\"\"\n",
    "    Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8, NOT 8\n",
    "    \"\"\"\n",
    "#     ipdb.set_trace()\n",
    "    max_preds = preds.argmax(dim = 1, keepdim = True) # \n",
    "    correct = max_preds.squeeze(1).eq(y)\n",
    "    return correct.sum() / torch.FloatTensor([y.shape[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 516,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 517,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for batch in iterator:\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "                \n",
    "        predictions = model(batch.text).squeeze(1)\n",
    "        \n",
    "#         ipdb.set_trace()\n",
    "        \n",
    "#         label batch.label.long()\n",
    "        \n",
    "        loss = criterion(predictions, batch.label.long())\n",
    "        \n",
    "        acc = categorical_accuracy(predictions, batch.label)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 518,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, criterion):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for batch in iterator:\n",
    "\n",
    "            predictions = model(batch.text).squeeze(1)\n",
    "            \n",
    "            loss = criterion(predictions, batch.label.long())\n",
    "            \n",
    "#             ipdb.set_trace()\n",
    "            acc = categorical_accuracy(predictions, batch.label)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 519,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 520,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 1.184 | Train Acc: 52.18%\n",
      "\t Val. Loss: 1.231 |  Val. Acc: 52.14%\n",
      "Epoch: 02 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 1.112 | Train Acc: 56.81%\n",
      "\t Val. Loss: 1.226 |  Val. Acc: 52.66%\n",
      "Epoch: 03 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 1.116 | Train Acc: 57.78%\n",
      "\t Val. Loss: 1.208 |  Val. Acc: 53.70%\n",
      "Epoch: 04 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 1.111 | Train Acc: 59.38%\n",
      "\t Val. Loss: 1.213 |  Val. Acc: 53.70%\n",
      "Epoch: 05 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 1.094 | Train Acc: 58.73%\n",
      "\t Val. Loss: 1.232 |  Val. Acc: 53.18%\n"
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 5\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "\n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_loss, train_acc = train(model, train_iterator, optimizer, criterion)\n",
    "    valid_loss, valid_acc = evaluate(model, valid_iterator, criterion)\n",
    "    \n",
    "    end_time = time.time()\n",
    "\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'tut1-model.pt')\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 521,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pdb, traceback, sys\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     try:\n",
    "#         # put code here \n",
    "#         N_EPOCHS = 5\n",
    "\n",
    "#         best_valid_loss = float('inf')\n",
    "\n",
    "#         for epoch in range(N_EPOCHS):\n",
    "\n",
    "#             start_time = time.time()\n",
    "\n",
    "#             train_loss, train_acc = train(model, train_iterator, optimizer, criterion)\n",
    "#             valid_loss, valid_acc = evaluate(model, valid_iterator, criterion)\n",
    "\n",
    "#             end_time = time.time()\n",
    "\n",
    "#             epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "\n",
    "#             if valid_loss < best_valid_loss:\n",
    "#                 best_valid_loss = valid_loss\n",
    "#                 torch.save(model.state_dict(), 'tut1-model.pt')\n",
    "\n",
    "#             print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "#             print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
    "#             print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')\n",
    "        \n",
    "#     except:\n",
    "#         extype, value, tb = sys.exc_info()\n",
    "#         traceback.print_exc()\n",
    "#         pdb.post_mortem(tb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 522,
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer(model, iterator, criterion):\n",
    "    \n",
    "#     epoch_loss = 0\n",
    "#     epoch_acc = 0\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for batch in iterator:\n",
    "\n",
    "            predictions = model(batch.text).squeeze(1)\n",
    "            \n",
    "#             loss = criterion(predictions, batch.label.long())\n",
    "            \n",
    "#             ipdb.set_trace()\n",
    "#             acc = categorical_accuracy(predictions, batch.label)\n",
    "\n",
    "#             epoch_loss += loss.item()\n",
    "#             epoch_acc += acc.item()\n",
    "        \n",
    "    return predictions#epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# next(test_iterator.__iter__())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# infer(model, test_iterator, criterion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>596</th>\n",
       "      <td>C8VZAJWW</td>\n",
       "      <td>Low self esteem</td>\n",
       "      <td>Depression</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>597</th>\n",
       "      <td>IO9JHGW7</td>\n",
       "      <td>Is weed really going to give me the energy I n...</td>\n",
       "      <td>Drugs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>598</th>\n",
       "      <td>Z6F1A8PK</td>\n",
       "      <td>I never sought any assistance</td>\n",
       "      <td>Suicide</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>599</th>\n",
       "      <td>C2GBAI5D</td>\n",
       "      <td>How to stop alcohol intake</td>\n",
       "      <td>Alcohol</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>600</th>\n",
       "      <td>H35L8ZD9</td>\n",
       "      <td>how to deal with migraines</td>\n",
       "      <td>Depression</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>601</th>\n",
       "      <td>139DFUFL</td>\n",
       "      <td>Is suicide the best remedy?</td>\n",
       "      <td>Suicide</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>602</th>\n",
       "      <td>6SZ3EXJ3</td>\n",
       "      <td>I did not ask for any assistance at first but ...</td>\n",
       "      <td>Suicide</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>603</th>\n",
       "      <td>S6WK4NBL</td>\n",
       "      <td>I feel confused, how can  I overcome the problem?</td>\n",
       "      <td>Depression</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>604</th>\n",
       "      <td>ULID3I4V</td>\n",
       "      <td>How do I resist peer pressure?</td>\n",
       "      <td>Drugs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>605</th>\n",
       "      <td>MYHIEXZJ</td>\n",
       "      <td>I was just tired</td>\n",
       "      <td>Suicide</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>606</th>\n",
       "      <td>0OSLJ1HL</td>\n",
       "      <td>Where can i get money to be drinking daily?</td>\n",
       "      <td>Alcohol</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>607</th>\n",
       "      <td>XDD8ATSC</td>\n",
       "      <td>Why is it so hard to stop being a weed addict?</td>\n",
       "      <td>Drugs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>608</th>\n",
       "      <td>LGAPCAYO</td>\n",
       "      <td>I feel very low and at times</td>\n",
       "      <td>Depression</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>609</th>\n",
       "      <td>KFPOMS0P</td>\n",
       "      <td>Why am I alive?</td>\n",
       "      <td>Suicide</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>610</th>\n",
       "      <td>69V3L12G</td>\n",
       "      <td>Life is just hard</td>\n",
       "      <td>Depression</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>611</th>\n",
       "      <td>BOHSNXCN</td>\n",
       "      <td>What should I do to stop alcoholism?</td>\n",
       "      <td>Alcohol</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>612</th>\n",
       "      <td>GVDXRQPY</td>\n",
       "      <td>How to become my oldself again</td>\n",
       "      <td>Suicide</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>613</th>\n",
       "      <td>IO4JHIQS</td>\n",
       "      <td>How can someone stop it?</td>\n",
       "      <td>Alcohol</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>614</th>\n",
       "      <td>1DS3P1XO</td>\n",
       "      <td>I feel unworthy</td>\n",
       "      <td>Depression</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>615</th>\n",
       "      <td>ORF71PVQ</td>\n",
       "      <td>I feel so discouraged with life</td>\n",
       "      <td>Depression</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           ID                                               text       label\n",
       "596  C8VZAJWW                                    Low self esteem  Depression\n",
       "597  IO9JHGW7  Is weed really going to give me the energy I n...       Drugs\n",
       "598  Z6F1A8PK                      I never sought any assistance     Suicide\n",
       "599  C2GBAI5D                         How to stop alcohol intake     Alcohol\n",
       "600  H35L8ZD9                         how to deal with migraines  Depression\n",
       "601  139DFUFL                        Is suicide the best remedy?     Suicide\n",
       "602  6SZ3EXJ3  I did not ask for any assistance at first but ...     Suicide\n",
       "603  S6WK4NBL  I feel confused, how can  I overcome the problem?  Depression\n",
       "604  ULID3I4V                     How do I resist peer pressure?       Drugs\n",
       "605  MYHIEXZJ                                   I was just tired     Suicide\n",
       "606  0OSLJ1HL        Where can i get money to be drinking daily?     Alcohol\n",
       "607  XDD8ATSC     Why is it so hard to stop being a weed addict?       Drugs\n",
       "608  LGAPCAYO                      I feel very low and at times   Depression\n",
       "609  KFPOMS0P                                    Why am I alive?     Suicide\n",
       "610  69V3L12G                                  Life is just hard  Depression\n",
       "611  BOHSNXCN               What should I do to stop alcoholism?     Alcohol\n",
       "612  GVDXRQPY                     How to become my oldself again     Suicide\n",
       "613  IO4JHIQS                           How can someone stop it?     Alcohol\n",
       "614  1DS3P1XO                                   I feel unworthy   Depression\n",
       "615  ORF71PVQ                    I feel so discouraged with life  Depression"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.tail(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>611</th>\n",
       "      <td>BOHSNXCN</td>\n",
       "      <td>What should I do to stop alcoholism?</td>\n",
       "      <td>Alcohol</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>612</th>\n",
       "      <td>GVDXRQPY</td>\n",
       "      <td>How to become my oldself again</td>\n",
       "      <td>Suicide</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>613</th>\n",
       "      <td>IO4JHIQS</td>\n",
       "      <td>How can someone stop it?</td>\n",
       "      <td>Alcohol</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>614</th>\n",
       "      <td>1DS3P1XO</td>\n",
       "      <td>I feel unworthy</td>\n",
       "      <td>Depression</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>615</th>\n",
       "      <td>ORF71PVQ</td>\n",
       "      <td>I feel so discouraged with life</td>\n",
       "      <td>Depression</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           ID                                  text       label\n",
       "611  BOHSNXCN  What should I do to stop alcoholism?     Alcohol\n",
       "612  GVDXRQPY        How to become my oldself again     Suicide\n",
       "613  IO4JHIQS              How can someone stop it?     Alcohol\n",
       "614  1DS3P1XO                      I feel unworthy   Depression\n",
       "615  ORF71PVQ       I feel so discouraged with life  Depression"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>02V56KMO</td>\n",
       "      <td>How to overcome bad feelings and emotions</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>03BMGTOK</td>\n",
       "      <td>I feel like giving up in life</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>03LZVFM6</td>\n",
       "      <td>I was so depressed feel like got no strength t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0EPULUM5</td>\n",
       "      <td>I feel so low especially since I had no one to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0GM4C5GD</td>\n",
       "      <td>can i be successful when I am a drug addict?</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         ID                                               text\n",
       "0  02V56KMO          How to overcome bad feelings and emotions\n",
       "1  03BMGTOK                     I feel like giving up in life \n",
       "2  03LZVFM6  I was so depressed feel like got no strength t...\n",
       "3  0EPULUM5  I feel so low especially since I had no one to...\n",
       "4  0GM4C5GD       can i be successful when I am a drug addict?"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn import preprocessing\n",
    "\n",
    "# le = preprocessing.LabelEncoder()\n",
    "\n",
    "# le.fit(train.label)\n",
    "\n",
    "# labels = le.transform(train.label)\n",
    "# # le.inverse_transform(y_) == train.label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# le.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# labels[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi NB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# X_train, X_test, y_train, y_test = train_test_split(train['text'], train['label'], \n",
    "#                                                     random_state = 0, test_size=0.3)\n",
    "\n",
    "X_train, X_test, y_train = train['text'], test['text'], train['label']\n",
    "\n",
    "count_vect = CountVectorizer()\n",
    "\n",
    "count_vect_fit = count_vect.fit(X_train)\n",
    "X_train = count_vect_fit.transform(X_train)\n",
    "X_test = count_vect_fit.transform(X_test)\n",
    "\n",
    "clf = MultinomialNB().fit(X_train, y_train)\n",
    "\n",
    "# tfidf_transformer = TfidfTransformer()\n",
    "# tfidf_transformer_fit = tfidf_transformer.fit(X_train_counts)\n",
    "\n",
    "# X_train_tfidf = tfidf_transformer_fit.transform(X_train_counts)\n",
    "# X_test_tfidf = tfidf_transformer_fit.transform(X_test_counts)\n",
    "\n",
    "# clf = MultinomialNB().fit(X_train_tfidf, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score\n",
    "\n",
    "y_pred = clf.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Alcohol', 'Depression', 'Drugs', 'Suicide'], dtype='<U10')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.16721233e-01, 8.64846895e-01, 1.10706874e-03, 1.73248034e-02],\n",
       "       [4.38782846e-06, 9.99887086e-01, 3.20866305e-06, 1.05317758e-04],\n",
       "       [1.33509995e-07, 9.99976087e-01, 5.91848765e-07, 2.31874675e-05],\n",
       "       [1.74333946e-06, 9.98397491e-01, 1.45104754e-05, 1.58625512e-03],\n",
       "       [2.62991326e-01, 6.05087194e-01, 1.07147604e-01, 2.47738763e-02],\n",
       "       [7.02054814e-02, 8.67282103e-01, 1.92979458e-02, 4.32144700e-02],\n",
       "       [8.26158754e-02, 7.82337575e-01, 6.82903193e-02, 6.67562301e-02],\n",
       "       [6.39935327e-04, 9.97073863e-01, 4.39760435e-04, 1.84644143e-03],\n",
       "       [9.96249767e-01, 1.07473325e-03, 2.62370035e-03, 5.17998029e-05],\n",
       "       [8.90928646e-01, 6.77340810e-02, 2.19634350e-02, 1.93738379e-02],\n",
       "       [4.73915086e-03, 9.88832085e-01, 4.26188235e-04, 6.00257635e-03],\n",
       "       [9.63736007e-01, 1.53928524e-02, 2.90025868e-03, 1.79708816e-02],\n",
       "       [1.56222028e-05, 9.98921648e-01, 2.88554596e-06, 1.05984375e-03],\n",
       "       [3.56073285e-02, 9.43319145e-01, 1.30813482e-02, 7.99217867e-03],\n",
       "       [9.99998680e-01, 3.10477996e-08, 1.23043412e-06, 5.89297239e-08],\n",
       "       [2.39820153e-03, 9.92790786e-01, 6.59212956e-04, 4.15179953e-03],\n",
       "       [5.21321932e-04, 9.90071478e-01, 1.32794434e-03, 8.07925564e-03],\n",
       "       [1.12672078e-01, 2.40407727e-02, 8.62570391e-01, 7.16757503e-04],\n",
       "       [1.43987103e-02, 9.72586028e-01, 5.14137604e-03, 7.87388574e-03],\n",
       "       [9.58104611e-03, 7.32661104e-01, 2.70715927e-02, 2.30686258e-01]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(309, 4)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average accuracy model RandomForestClassifier, is 0.5876669272076115\n",
      "Average accuracy model LinearSVC, is 0.8554426521864373\n",
      "Average accuracy model MultinomialNB, is 0.7760512397191661\n",
      "Average accuracy model LogisticRegression, is 0.8522033130833689\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aims/anaconda3/envs/aims/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/home/aims/anaconda3/envs/aims/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "/home/aims/anaconda3/envs/aims/lib/python3.7/site-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/home/aims/anaconda3/envs/aims/lib/python3.7/site-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average accuracy model SVC, is 0.709313229546891\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aims/anaconda3/envs/aims/lib/python3.7/site-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/home/aims/anaconda3/envs/aims/lib/python3.7/site-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/home/aims/anaconda3/envs/aims/lib/python3.7/site-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "models = [\n",
    "    RandomForestClassifier(n_estimators=200, max_depth=3, random_state=0),\n",
    "    LinearSVC(),\n",
    "    MultinomialNB(),\n",
    "    LogisticRegression(random_state=0),\n",
    "    SVC(kernel='rbf', C=9, degree=5), #'linear', 'poly', 'rbf', 'sigmoid'\n",
    "]\n",
    "\n",
    "CV = 5\n",
    "\n",
    "cv_df = pd.DataFrame(index=range(CV * len(models)))\n",
    "\n",
    "entries = []\n",
    "\n",
    "# X_train = features\n",
    "# y_train = labels\n",
    "\n",
    "for model in models:\n",
    "    acc = []\n",
    "    model_name = model.__class__.__name__\n",
    "    accuracies = cross_val_score(model, X_train, y_train, scoring='accuracy', cv=CV)\n",
    "    for fold_idx, accuracy in enumerate(accuracies):\n",
    "#         ipdb.set_trace()\n",
    "        entries.append((model_name, fold_idx, accuracy))\n",
    "        acc.append(accuracy)\n",
    "    print(f'Average accuracy model {model_name}, is {np.mean(acc)}')\n",
    "\n",
    "        \n",
    "    \n",
    "# cv_df = pd.DataFrame(entries, columns=['model_name', 'fold_idx', 'accuracy'])\n",
    "# import seaborn as sns\n",
    "# sns.boxplot(x='model_name', y='accuracy', data=cv_df)\n",
    "# sns.stripplot(x='model_name', y='accuracy', data=cv_df, \n",
    "#               size=8, jitter=True, edgecolor=\"gray\", linewidth=2)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model selection and cross validation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(616, 834)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# tfidf = TfidfVectorizer(sublinear_tf=True, min_df=0, max_df=50, norm='l2', lowercase=True, \n",
    "#                         encoding='latin-1', ngram_range=(1, 2), stop_words='english')\n",
    "\n",
    "# tfidf = TfidfVectorizer(sublinear_tf=True, min_df=5, norm='l2', \n",
    "#                         encoding='latin-1', ngram_range=(1, 2), stop_words='english')\n",
    "\n",
    "tfidf = TfidfVectorizer()\n",
    "\n",
    "features_transform = tfidf.fit(train.text)\n",
    "\n",
    "X_train = features_transform.transform(train.text).toarray()\n",
    "X_test = features_transform.transform(test.text).toarray()\n",
    "\n",
    "\n",
    "y_train = train.label\n",
    "\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average accuracy model RandomForestClassifier, is 0.5989720567686904\n",
      "Average accuracy model LinearSVC, is 0.8635213458702541\n",
      "Average accuracy model MultinomialNB, is 0.7319875748625282\n",
      "Average accuracy model LogisticRegression, is 0.7726801098915274\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aims/anaconda3/envs/aims/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/home/aims/anaconda3/envs/aims/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "/home/aims/anaconda3/envs/aims/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/home/aims/anaconda3/envs/aims/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "/home/aims/anaconda3/envs/aims/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/home/aims/anaconda3/envs/aims/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "/home/aims/anaconda3/envs/aims/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/home/aims/anaconda3/envs/aims/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "/home/aims/anaconda3/envs/aims/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/home/aims/anaconda3/envs/aims/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "/home/aims/anaconda3/envs/aims/lib/python3.7/site-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/home/aims/anaconda3/envs/aims/lib/python3.7/site-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/home/aims/anaconda3/envs/aims/lib/python3.7/site-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/home/aims/anaconda3/envs/aims/lib/python3.7/site-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/home/aims/anaconda3/envs/aims/lib/python3.7/site-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average accuracy model SVC, is 0.5714454639649517\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "models = [\n",
    "    RandomForestClassifier(n_estimators=200, max_depth=3, random_state=0),\n",
    "    LinearSVC(),\n",
    "    MultinomialNB(),\n",
    "    LogisticRegression(random_state=0),\n",
    "    SVC(kernel='rbf', C=9, degree=5), #'linear', 'poly', 'rbf', 'sigmoid'\n",
    "]\n",
    "\n",
    "CV = 5\n",
    "\n",
    "cv_df = pd.DataFrame(index=range(CV * len(models)))\n",
    "\n",
    "entries = []\n",
    "\n",
    "# X_train = features\n",
    "# y_train = labels\n",
    "\n",
    "for model in models:\n",
    "    acc = []\n",
    "    model_name = model.__class__.__name__\n",
    "    accuracies = cross_val_score(model, X_train, y_train, scoring='accuracy', cv=CV)\n",
    "    for fold_idx, accuracy in enumerate(accuracies):\n",
    "#         ipdb.set_trace()\n",
    "        entries.append((model_name, fold_idx, accuracy))\n",
    "        acc.append(accuracy)\n",
    "    print(f'Average accuracy model {model_name}, is {np.mean(acc)}')\n",
    "\n",
    "        \n",
    "    \n",
    "# cv_df = pd.DataFrame(entries, columns=['model_name', 'fold_idx', 'accuracy'])\n",
    "# import seaborn as sns\n",
    "# sns.boxplot(x='model_name', y='accuracy', data=cv_df)\n",
    "# sns.stripplot(x='model_name', y='accuracy', data=cv_df, \n",
    "#               size=8, jitter=True, edgecolor=\"gray\", linewidth=2)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.preprocessing import Binarizer\n",
    "\n",
    "# freq   = CountVectorizer()\n",
    "# corpus = freq.fit_transform(train.text)\n",
    "\n",
    "# onehot = Binarizer()\n",
    "# features = onehot.fit_transform(corpus.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "models = [\n",
    "    RandomForestClassifier(n_estimators=500, max_depth=500, random_state=0),\n",
    "    LinearSVC(),\n",
    "    MultinomialNB(),\n",
    "    LogisticRegression(random_state=0),\n",
    "]\n",
    "\n",
    "X_train, X_test, y_train = train['text'], test['text'], train['label']\n",
    "\n",
    "\n",
    "count_vect = CountVectorizer()\n",
    "\n",
    "count_vect_fit = count_vect.fit(X_train)\n",
    "X_train = count_vect_fit.transform(X_train)\n",
    "X_test = count_vect_fit.transform(X_test)\n",
    "\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "tfidf_transformer_fit = tfidf_transformer.fit(X_train)\n",
    "\n",
    "X_train = tfidf_transformer_fit.transform(X_train)\n",
    "X_test = tfidf_transformer_fit.transform(X_test)\n",
    "\n",
    "clf = models[2].fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Alcohol', 'Depression', 'Drugs', 'Suicide'], dtype='<U10')"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.22702267, 0.68265981, 0.03252599, 0.05779153],\n",
       "       [0.01224588, 0.97168691, 0.00496247, 0.01110475],\n",
       "       [0.01856802, 0.94967283, 0.01111034, 0.0206488 ],\n",
       "       [0.04018702, 0.87841595, 0.02666824, 0.05472879],\n",
       "       [0.24381393, 0.58726743, 0.10460836, 0.06431028],\n",
       "       [0.16446563, 0.69413474, 0.05730467, 0.08409496],\n",
       "       [0.14913113, 0.68432158, 0.07920022, 0.08734707],\n",
       "       [0.0473437 , 0.90354769, 0.02192313, 0.02718548],\n",
       "       [0.68335574, 0.24151022, 0.05484979, 0.02028425],\n",
       "       [0.49117261, 0.37243102, 0.06280318, 0.07359319],\n",
       "       [0.10714126, 0.810985  , 0.0282685 , 0.05360524],\n",
       "       [0.42470458, 0.42042303, 0.06070478, 0.09416761],\n",
       "       [0.02954996, 0.92205386, 0.01139442, 0.03700176],\n",
       "       [0.0804813 , 0.8607125 , 0.02906664, 0.02973956],\n",
       "       [0.82282801, 0.11359654, 0.03898174, 0.0245937 ],\n",
       "       [0.06148122, 0.87143413, 0.02632888, 0.04075577],\n",
       "       [0.10844732, 0.76136843, 0.05681244, 0.07337181],\n",
       "       [0.29598195, 0.38843644, 0.2865736 , 0.029008  ],\n",
       "       [0.13508557, 0.74821625, 0.05298155, 0.06371662],\n",
       "       [0.10291825, 0.74666733, 0.05365703, 0.09675738]])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/aims/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/aims/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "from nltk.stem import PorterStemmer\n",
    "from keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 630,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Not really used \n",
    "def clean_doc(text_1):\n",
    "    ps = PorterStemmer()\n",
    "    text_clean = []\n",
    "    #   ipdb.set_trace()\n",
    "    for text in text_1:    \n",
    "        # split into tokens by white space\n",
    "        tokens = word_tokenize(text)\n",
    "\n",
    "        # convert to lower case\n",
    "        tokens = [w.lower() for w in tokens]\n",
    "\n",
    "#         # remove punctuation from each token\n",
    "#         table = str.maketrans('', '', string.punctuation)\n",
    "#         stripped = [w.translate(table) for w in tokens]\n",
    "\n",
    "#         # remove remaining tokens that are not alphabetic\n",
    "#         tokens = [word for word in stripped if word.isalpha()]\n",
    "\n",
    "#         # filter out stop words\n",
    "#         stop_words = set(stopwords.words('english'))\n",
    "#         tokens = [w for w in tokens if not w in stop_words]\n",
    "\n",
    "        # filter out short tokens\n",
    "#         tokens = [word for word in tokens if len(word) > 2]\n",
    "\n",
    "        tokens = [ps.stem(w) for w in tokens]\n",
    "\n",
    "        text_clean.append(tokens)\n",
    "    \n",
    "    return text_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 631,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean_doc(train.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 749,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 734 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "# texts_tr = train.text\n",
    "# texts_tst = test.text\n",
    "\n",
    "texts_tr = clean_doc(train.text)\n",
    "texts_tst = clean_doc(test.text)\n",
    "\n",
    "# Training 92916\n",
    "# Considers only the top \n",
    "# 20,000 words in the dataset\n",
    "max_words = 734\n",
    "\n",
    "tokenizer_tr = Tokenizer(num_words=max_words)\n",
    "# tokenizer_tr = Tokenizer()\n",
    "tokenizer_tr.fit_on_texts(texts_tr)\n",
    "data_tr = tokenizer_tr.texts_to_sequences(texts_tr)\n",
    "# \"count\", \"tfidf\", \"freq\"\n",
    "tfidf_train = tokenizer_tr.texts_to_matrix(texts_tr, mode='binary') \n",
    "# bin_tr = tokenizer_tr.texts_to_matrix(texts_tr, mode='binary')\n",
    "\n",
    "\n",
    "# Testing \n",
    "tokenizer_tst = Tokenizer(num_words=max_words)\n",
    "# tokenizer_tst = Tokenizer()\n",
    "tokenizer_tst.fit_on_texts(texts_tst)\n",
    "data_tst = tokenizer_tst.texts_to_sequences(texts_tst)\n",
    "tfidf_tst = tokenizer_tr.texts_to_matrix(texts_tst, mode='binary')\n",
    "# bin_tst = tokenizer_tr.texts_to_matrix(texts_tst, mode='binary')\n",
    "\n",
    "\n",
    "word_index = tokenizer_tr.word_index\n",
    "print('Found %s unique tokens.' % len(set(word_index))) #88582 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 750,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(616, 734)"
      ]
     },
     "execution_count": 750,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 751,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(309, 734)"
      ]
     },
     "execution_count": 751,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_tst.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 752,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = tfidf_train\n",
    "X_test = tfidf_tst\n",
    "\n",
    "y_train = train['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 753,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SUAVK39Z</td>\n",
       "      <td>I feel that it was better I dieAm happy</td>\n",
       "      <td>Depression</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9JDAGUV3</td>\n",
       "      <td>Why do I get hallucinations?</td>\n",
       "      <td>Drugs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>419WR1LQ</td>\n",
       "      <td>I am stresseed due to lack of financial suppor...</td>\n",
       "      <td>Depression</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6UY7DX6Q</td>\n",
       "      <td>Why is life important?</td>\n",
       "      <td>Suicide</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>FYC0FTFB</td>\n",
       "      <td>How could I be helped to go through the depres...</td>\n",
       "      <td>Depression</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         ID                                               text       label\n",
       "0  SUAVK39Z            I feel that it was better I dieAm happy  Depression\n",
       "1  9JDAGUV3                       Why do I get hallucinations?       Drugs\n",
       "2  419WR1LQ  I am stresseed due to lack of financial suppor...  Depression\n",
       "3  6UY7DX6Q                             Why is life important?     Suicide\n",
       "4  FYC0FTFB  How could I be helped to go through the depres...  Depression"
      ]
     },
     "execution_count": 753,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 758,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0.])"
      ]
     },
     "execution_count": 758,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 759,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aims/anaconda3/envs/aims/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/home/aims/anaconda3/envs/aims/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# X_train, X_test, y_train = train['text'], test['text'], train['label']\n",
    "\n",
    "# count_vect = CountVectorizer()\n",
    "\n",
    "# count_vect_fit = count_vect.fit(X_train)\n",
    "# X_train_counts = count_vect_fit.transform(X_train)\n",
    "# X_test_counts = count_vect_fit.transform(X_test)\n",
    "\n",
    "models = [\n",
    "    RandomForestClassifier(n_estimators=200, max_depth=3, random_state=0),\n",
    "    LinearSVC(),\n",
    "    MultinomialNB(),\n",
    "    LogisticRegression(random_state=0),\n",
    "]\n",
    "\n",
    "clf = models[3].fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 760,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "y_pred = clf.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 761,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Alcohol', 'Depression', 'Drugs', 'Suicide'], dtype=object)"
      ]
     },
     "execution_count": 761,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 762,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[6.22712098e-02, 8.56397621e-01, 6.97693899e-03, 7.43542305e-02],\n",
       "       [4.95704784e-03, 9.65822129e-01, 5.59737199e-03, 2.36234517e-02],\n",
       "       [7.21396079e-04, 9.90858966e-01, 1.47969798e-03, 6.93993961e-03],\n",
       "       [2.90456002e-03, 9.23592656e-01, 1.16602050e-02, 6.18425793e-02],\n",
       "       [1.58712921e-01, 4.11620907e-01, 4.21029428e-01, 8.63674431e-03],\n",
       "       [2.97787001e-02, 9.41299212e-01, 1.29276661e-02, 1.59944217e-02],\n",
       "       [8.75368079e-02, 7.90884161e-01, 3.88977093e-02, 8.26813218e-02],\n",
       "       [1.32509995e-02, 9.60468694e-01, 8.30094651e-03, 1.79793598e-02],\n",
       "       [9.78010873e-01, 7.98399085e-03, 1.03038264e-02, 3.70130990e-03],\n",
       "       [4.86962538e-01, 4.16802027e-01, 6.10199656e-02, 3.52154689e-02],\n",
       "       [1.70721732e-02, 9.11887957e-01, 5.34118350e-03, 6.56986863e-02],\n",
       "       [8.53069341e-01, 1.23368833e-01, 3.19617012e-03, 2.03656561e-02],\n",
       "       [1.49870835e-02, 7.54770666e-01, 6.26663046e-03, 2.23975620e-01],\n",
       "       [1.16794938e-01, 8.15643817e-01, 2.67481853e-02, 4.08130591e-02],\n",
       "       [9.63361512e-01, 4.77497964e-03, 2.45821470e-02, 7.28136091e-03],\n",
       "       [3.61098408e-02, 8.53505030e-01, 1.70662353e-02, 9.33188937e-02],\n",
       "       [4.09300408e-02, 8.62214582e-01, 1.32049726e-02, 8.36504050e-02],\n",
       "       [7.31730611e-02, 6.44318703e-02, 8.37058108e-01, 2.53369604e-02],\n",
       "       [1.41316969e-02, 9.59898550e-01, 5.98964376e-03, 1.99801091e-02],\n",
       "       [2.42277798e-02, 7.37609325e-01, 1.07540302e-01, 1.30622593e-01]])"
      ]
     },
     "execution_count": 762,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 763,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import cross_val_score\n",
    "models = [\n",
    "    RandomForestClassifier(n_estimators=200, max_depth=3, random_state=0),\n",
    "    LinearSVC(),\n",
    "    MultinomialNB(),\n",
    "    LogisticRegression(random_state=0),\n",
    "    SVC(kernel='rbf', C=9, degree=5), #'linear', 'poly', 'rbf', 'sigmoid'\n",
    "]\n",
    "\n",
    "CV = 5\n",
    "\n",
    "cv_df = pd.DataFrame(index=range(CV * len(models)))\n",
    "\n",
    "entries = []\n",
    "\n",
    "# X_train = features\n",
    "# y_train = labels\n",
    "\n",
    "for model in models:\n",
    "    acc = []\n",
    "    model_name = model.__class__.__name__\n",
    "    accuracies = cross_val_score(model, X_train, y_train, scoring='accuracy', cv=CV)\n",
    "    for fold_idx, accuracy in enumerate(accuracies):\n",
    "#         ipdb.set_trace()\n",
    "        entries.append((model_name, fold_idx, accuracy))\n",
    "        acc.append(accuracy)\n",
    "    print(f'Average accuracy model {model_name}, is {np.mean(acc)}')\n",
    "\n",
    "        \n",
    "    \n",
    "# cv_df = pd.DataFrame(entries, columns=['model_name', 'fold_idx', 'accuracy'])\n",
    "# import seaborn as sns\n",
    "# sns.boxplot(x='model_name', y='accuracy', data=cv_df)\n",
    "# sns.stripplot(x='model_name', y='accuracy', data=cv_df, \n",
    "#               size=8, jitter=True, edgecolor=\"gray\", linewidth=2)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "def objective_sgd(trial):\n",
    "    \n",
    "#     q  = trial.suggest_loguniform('q', 1e-5, 3e+0)\n",
    "    \n",
    "#     sigma  = trial.suggest_loguniform('sigma', 4e-0, 5e+0) # trial.suggest_float('sigma', 1e-5, 1e-3, log=True)\n",
    "    \n",
    "    kernel = trial.suggest_categorical('kernel', ['linear', 'poly', 'rbf', 'sigmoid']) \n",
    "    \n",
    "#     lam = trial.suggest_loguniform('lam', 1e-15, 1e-5)\n",
    "    \n",
    "#     d = trial.suggest_int('d', 2, 15)\n",
    "    \n",
    "    c = trial.suggest_int('c', 1, 10)\n",
    "    \n",
    "#     n = trial.suggest_int('n', 1, 5)\n",
    "    \n",
    "    degree = trial.suggest_int('degree', 2, 10)\n",
    "    \n",
    "    models = SVC(kernel=kernel, C=c, degree=degree), #'linear', 'poly', 'rbf', 'sigmoid'\n",
    "#     ]\n",
    "\n",
    "    CV = 5\n",
    "\n",
    "    cv_df = pd.DataFrame(index=range(CV * len(models)))\n",
    "\n",
    "    entries = []\n",
    "\n",
    "    # X_train = features\n",
    "    # y_train = labels\n",
    "\n",
    "#     for model in models:\n",
    "    acc = []\n",
    "    model_name = model.__class__.__name__\n",
    "    accuracies = cross_val_score(model, X_train, y_train, scoring='accuracy', cv=CV)\n",
    "    for fold_idx, accuracy in enumerate(accuracies):\n",
    "#         ipdb.set_trace()\n",
    "        entries.append((model_name, fold_idx, accuracy))\n",
    "        acc.append(accuracy)\n",
    "    print(f'Average accuracy model {model_name}, is {np.mean(acc)}')\n",
    "    \n",
    "    \n",
    "     \n",
    "    return np.mean(acc)\n",
    "\n",
    "sampler = optuna.samplers.TPESampler()\n",
    "\n",
    "study = optuna.create_study(sampler=sampler, direction='maximize')\n",
    "study.optimize(func=objective_sgd, n_trials=1500, show_progress_bar=True)\n",
    "\n",
    "\n",
    "trial = study.best_trial\n",
    "\n",
    "print('Accuracy: {}'.format(trial.value))\n",
    "print(\"Best hyperparameters: {}\".format(trial.params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Depression</th>\n",
       "      <th>Alcohol</th>\n",
       "      <th>Suicide</th>\n",
       "      <th>Drugs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>304</th>\n",
       "      <td>Z9A6ACLK</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>305</th>\n",
       "      <td>ZDUOIGKN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>306</th>\n",
       "      <td>ZHQ60CCH</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>307</th>\n",
       "      <td>ZVIJMA4O</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>308</th>\n",
       "      <td>ZYIFAY98</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           ID  Depression  Alcohol  Suicide  Drugs\n",
       "304  Z9A6ACLK           0        0        0      0\n",
       "305  ZDUOIGKN           0        0        0      0\n",
       "306  ZHQ60CCH           0        0        0      0\n",
       "307  ZVIJMA4O           0        0        0      0\n",
       "308  ZYIFAY98           0        0        0      0"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(test.ID == sample.ID).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Alcohol</th>\n",
       "      <th>Depression</th>\n",
       "      <th>Drugs</th>\n",
       "      <th>Suicide</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.227023</td>\n",
       "      <td>0.682660</td>\n",
       "      <td>0.032526</td>\n",
       "      <td>0.057792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.012246</td>\n",
       "      <td>0.971687</td>\n",
       "      <td>0.004962</td>\n",
       "      <td>0.011105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.018568</td>\n",
       "      <td>0.949673</td>\n",
       "      <td>0.011110</td>\n",
       "      <td>0.020649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.040187</td>\n",
       "      <td>0.878416</td>\n",
       "      <td>0.026668</td>\n",
       "      <td>0.054729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.243814</td>\n",
       "      <td>0.587267</td>\n",
       "      <td>0.104608</td>\n",
       "      <td>0.064310</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Alcohol  Depression     Drugs   Suicide\n",
       "0  0.227023    0.682660  0.032526  0.057792\n",
       "1  0.012246    0.971687  0.004962  0.011105\n",
       "2  0.018568    0.949673  0.011110  0.020649\n",
       "3  0.040187    0.878416  0.026668  0.054729\n",
       "4  0.243814    0.587267  0.104608  0.064310"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = pd.DataFrame(y_pred, columns=clf.classes_)\n",
    "predictions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pd.concat([sample[['ID']], predictions], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Alcohol</th>\n",
       "      <th>Depression</th>\n",
       "      <th>Drugs</th>\n",
       "      <th>Suicide</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>02V56KMO</td>\n",
       "      <td>0.394277</td>\n",
       "      <td>0.421439</td>\n",
       "      <td>0.002456</td>\n",
       "      <td>0.181828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>03BMGTOK</td>\n",
       "      <td>0.001315</td>\n",
       "      <td>0.991308</td>\n",
       "      <td>0.001106</td>\n",
       "      <td>0.006272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>03LZVFM6</td>\n",
       "      <td>0.000031</td>\n",
       "      <td>0.997877</td>\n",
       "      <td>0.000369</td>\n",
       "      <td>0.001723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0EPULUM5</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.971215</td>\n",
       "      <td>0.002083</td>\n",
       "      <td>0.026697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0GM4C5GD</td>\n",
       "      <td>0.189575</td>\n",
       "      <td>0.324375</td>\n",
       "      <td>0.483044</td>\n",
       "      <td>0.003007</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         ID   Alcohol  Depression     Drugs   Suicide\n",
       "0  02V56KMO  0.394277    0.421439  0.002456  0.181828\n",
       "1  03BMGTOK  0.001315    0.991308  0.001106  0.006272\n",
       "2  03LZVFM6  0.000031    0.997877  0.000369  0.001723\n",
       "3  0EPULUM5  0.000005    0.971215  0.002083  0.026697\n",
       "4  0GM4C5GD  0.189575    0.324375  0.483044  0.003007"
      ]
     },
     "execution_count": 351,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.to_csv('./LogisticRegression_random_state=0_train.text.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
